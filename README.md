# AnÃ¡lisis y EvaluaciÃ³n Comparativa de Pipelines RAG

Este proyecto ofrece un framework completo para implementar, evaluar y comparar el rendimiento de diferentes pipelines de Retrieval-Augmented Generation (RAG). Se utilizan varios modelos de lenguaje (LLMs) y estrategias de chunking para analizar cÃ³mo cada componente afecta la calidad de las respuestas generadas.

## âœ¨ CaracterÃ­sticas Principales

- **EvaluaciÃ³n de MÃºltiples LLMs**: Compara el rendimiento de modelos como Gemini, llama y Qwen.
- **Estrategias de RAG Avanzadas**: ImplementaciÃ³n desde un sistema RAG simple hasta configuraciones mÃ¡s complejas y optimizadas.
- **Vector Store con ChromaDB**: Utiliza ChromaDB para la gestiÃ³n eficiente de embeddings vectoriales.
- **Notebooks Detallados**: El proceso de evaluaciÃ³n estÃ¡ documentado paso a paso en Jupyter Notebooks.
- **Resultados Cuantificables**: Genera datasets de evaluaciÃ³n y reportes en CSV para un anÃ¡lisis objetivo.

## ğŸ“Š VisualizaciÃ³n de Resultados

A continuaciÃ³n, se muestra un heatmap que resume el rendimiento comparativo de los modelos evaluados en las distintas configuraciones de RAG.

![Heatmap de Resultados](src/heatmap_rag_models.png)

## ğŸ“‚ Estructura del Proyecto

```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Documentos fuente para el proceso RAG (PDFs, etc.).
â”œâ”€â”€ nuevo_rag/
â”‚   â””â”€â”€ Entorno virtual de Python con las dependencias.
â”œâ”€â”€ resultados/
â”‚   â”œâ”€â”€ datasets de evaluaciÃ³n/
â”‚   â”‚   â””â”€â”€ Archivos JSON con los resultados de la evaluaciÃ³n.
â”‚   â””â”€â”€ reportes_csv/
â”‚       â””â”€â”€ Reportes CSV detallados por cada modelo y configuraciÃ³n.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ *.ipynb (Notebooks de evaluaciÃ³n: simple, intermedio, avanzado).
â”‚   â””â”€â”€ *.json (Resultados agregados de las evaluaciones).
â”œâ”€â”€ vector_db/
â”‚   â””â”€â”€ Bases de datos vectoriales de ChromaDB para cada estrategia.
â”œâ”€â”€ .env
â”‚   â””â”€â”€ Archivo para variables de entorno (API Keys).
â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dependencias de Python.
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n y Puesta en Marcha

Sigue estos pasos para configurar el entorno y ejecutar el proyecto.

### 1. Clonar el Repositorio

```bash
git clone <URL-DEL-REPOSITORIO>
cd <NOMBRE-DEL-DIRECTORIO>
```

### 2. Crear y Activar el Entorno Virtual

Se recomienda utilizar un entorno virtual para gestionar las dependencias. El proyecto ya incluye uno en `nuevo_rag/`, pero si necesitas crearlo desde cero:

```bash
# Crear entorno virtual
python -m venv nuevo_rag

# Activar en Windows
.\nuevo_rag\Scripts\activate

# Activar en macOS/Linux
source nuevo_rag/bin/activate
```

### 3. Instalar Dependencias

Instala todas las librerÃ­as necesarias desde el archivo `requirements.txt`.

```bash
pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

Crea una copia del archivo `.env.example` (o crea un archivo `.env` desde cero) y aÃ±ade tus API keys para los servicios de LLM que vayas a utilizar.

```ini
# .env
GOOGLE_API_KEY="tu-api-key-de-google"
GROQ_API_KEY="tu-api-key-de-groq"
# AÃ±ade otras claves si son necesarias
```

## ğŸ› ï¸ Uso y EjecuciÃ³n

El nÃºcleo de este proyecto son los Jupyter Notebooks ubicados en el directorio `src/`.

1.  **Inicia Jupyter Lab o Jupyter Notebook:**

    ```bash
jupyter lab
```

2.  **Abre los Notebooks de EvaluaciÃ³n:**

    -   `simple_rag_evaluation.ipynb`: Implementa y evalÃºa un pipeline de RAG bÃ¡sico.
    -   `intermedia_rag_evaluation.ipynb`: Introduce optimizaciones en el proceso de chunking y retrieval.
    -   `advanced_rag_evaluation.ipynb`: Utiliza tÃ©cnicas avanzadas para mejorar aÃºn mÃ¡s la calidad de las respuestas.

Ejecuta las celdas de cada notebook para replicar el proceso de evaluaciÃ³n.

## ğŸ“ˆ Resultados

Todos los resultados generados se almacenan en el directorio `resultados/`.
-   Los **datasets de evaluaciÃ³n** en formato JSON se encuentran en `resultados/datasets de evaluaciÃ³n/`. Estos archivos son ideales para un anÃ¡lisis programÃ¡tico posterior.
-   Los **reportes en CSV** en `resultados/reportes_csv/` ofrecen una vista tabular fÃ¡cil de interpretar de las mÃ©tricas de rendimiento.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Si tienes ideas para mejorar el framework o aÃ±adir nuevas evaluaciones, por favor abre un issue o envÃ­a un pull request.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
