{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd02c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class AdvancedRAGEvaluationGenerator:\n",
    "    \"\"\"\n",
    "    Clase para generar respuestas y dataset RAGAS usando AdvancedRAGPipeline.\n",
    "    Maneja indexaci√≥n de documentos y procesamiento en lotes.\n",
    "    \"\"\"\n",
    "    def __init__(self, rag_pipeline, clean_dataset, papers_dir=None, batch_size=3, delay=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rag_pipeline: Instancia de AdvancedRAGPipeline.\n",
    "            clean_dataset: Dataset limpio (qa_clean_dataset).\n",
    "            papers_dir: Directorio con PDFs (None si no hay PDFs).\n",
    "            batch_size: Tama√±o del lote para procesamiento.\n",
    "            delay: Segundos de espera entre lotes.\n",
    "        \"\"\"\n",
    "        self.rag = rag_pipeline\n",
    "        self.dataset = clean_dataset\n",
    "        self.papers_dir = papers_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.delay = delay\n",
    "        self.results_file = \"advanced_rag_evaluation_results.json\"\n",
    "        self.results = self.load_previous_results()\n",
    "        print(f\"‚úÖ Inicializado AdvancedRAGEvaluationGenerator con {len(clean_dataset)} ejemplos\")\n",
    "\n",
    "    def load_previous_results(self):\n",
    "        \"\"\"Cargar resultados previos para reanudar.\"\"\"\n",
    "        if os.path.exists(self.results_file):\n",
    "            with open(self.results_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return []\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Guardar resultados incrementalmente.\"\"\"\n",
    "        with open(self.results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def index_documents(self):\n",
    "        \"\"\"Indexar documentos referenciados por paper_id en Qdrant.\"\"\"\n",
    "        print(\"Indexando documentos...\")\n",
    "        papers_dataset = load_dataset(\"UKPLab/PeerQA\", \"papers\")[\"test\"]\n",
    "        indexed, failed = self.rag.setup_peerqa_retriever(papers_dataset=papers_dataset)\n",
    "        print(f\"Indexaci√≥n completada: {indexed} exitosos, {failed} fallidos\")\n",
    "        return indexed, failed\n",
    "\n",
    "    def generate_responses_batch(self, model_type='openrouter', start_idx=None, max_samples=30):\n",
    "        \"\"\"\n",
    "        Generar respuestas en lotes.\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'openrouter' o 'groq'.\n",
    "            start_idx: √çndice inicial (None para continuar desde resultados previos).\n",
    "            max_samples: M√°ximo n√∫mero de muestras a procesar.\n",
    "        \"\"\"\n",
    "        total_samples = len(self.dataset)\n",
    "        if start_idx is None:\n",
    "            start_idx = len(self.results)\n",
    "\n",
    "        end_idx = min(start_idx + max_samples, total_samples) if max_samples else total_samples\n",
    "\n",
    "        print(f\"Generando respuestas con {model_type}\")\n",
    "        print(f\"Procesando desde √≠ndice {start_idx} hasta {end_idx}\")\n",
    "        print(f\"Lotes de {self.batch_size} con {self.delay}s de espera\")\n",
    "        print(f\"Ya procesados: {len(self.results)} ejemplos\")\n",
    "\n",
    "        for i in range(start_idx, end_idx, self.batch_size):\n",
    "            batch_end = min(i + self.batch_size, end_idx)\n",
    "            current_batch = range(i, batch_end)\n",
    "\n",
    "            print(f\"\\nüîÑ Procesando lote {i//self.batch_size + 1}: √≠ndices {i}-{batch_end-1}\")\n",
    "\n",
    "            for idx in current_batch:\n",
    "                try:\n",
    "                    example = self.dataset[idx]\n",
    "                    question = example['question']\n",
    "                    print(f\"[{idx+1}/{end_idx}] Procesando pregunta: {question[:50]}...\")\n",
    "\n",
    "                    # Generar respuesta con AdvancedRAGPipeline\n",
    "                    rag_result = self.rag.query_rag(question, model=model_type, top_k=5)\n",
    "\n",
    "                    # Preparar resultado para RAGAS\n",
    "                    result = {\n",
    "                        'question': question,\n",
    "                        'answer': rag_result['answer'],\n",
    "                        'contexts': rag_result['contexts'],\n",
    "                        'ground_truth': example['ground_truth'],\n",
    "                        'paper_id': example['paper_id'],\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'processed_idx': idx\n",
    "                    }\n",
    "                    self.results.append(result)\n",
    "                    print(f\"  ‚úÖ Completado!\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en √≠ndice {idx}: {str(e)}\")\n",
    "                    error_result = {\n",
    "                        'question': self.dataset[idx]['question'] if idx < len(self.dataset) else \"Error loading\",\n",
    "                        'answer': f\"ERROR: {str(e)}\",\n",
    "                        'contexts': [],\n",
    "                        'ground_truth': self.dataset[idx].get('ground_truth', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'paper_id': self.dataset[idx].get('paper_id', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'processed_idx': idx,\n",
    "                        'error': True\n",
    "                    }\n",
    "                    self.results.append(error_result)\n",
    "\n",
    "            # Guardar progreso despu√©s de cada lote\n",
    "            self.save_results()\n",
    "            print(f\"üíæ Progreso guardado: {len(self.results)} ejemplos completados\")\n",
    "\n",
    "            if batch_end < end_idx:\n",
    "                print(f\"‚è≥ Esperando {self.delay} segundos...\")\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        print(f\"\\nüéâ ¬°Proceso completado! Total: {len(self.results)} ejemplos\")\n",
    "        return self.results\n",
    "\n",
    "    def get_ragas_dataset(self, filter_errors=True):\n",
    "        \"\"\"\n",
    "        Crear dataset para RAGAS.\n",
    "        \n",
    "        Args:\n",
    "            filter_errors: Excluir ejemplos con errores.\n",
    "        \"\"\"\n",
    "        clean_results = [r for r in self.results if not r.get('error', False)] if filter_errors else self.results\n",
    "\n",
    "        if not clean_results:\n",
    "            print(\"No hay resultados v√°lidos para crear dataset\")\n",
    "            return None\n",
    "\n",
    "        ragas_data = {\n",
    "            'question': [r['question'] for r in clean_results],\n",
    "            'answer': [r['answer'] for r in clean_results],\n",
    "            'contexts': [r['contexts'] for r in clean_results],\n",
    "            'ground_truth': [r['ground_truth'] for r in clean_results]\n",
    "        }\n",
    "\n",
    "        dataset = Dataset.from_dict(ragas_data)\n",
    "        print(f\"üìä Dataset RAGAS creado: {len(dataset)} ejemplos\")\n",
    "        return dataset\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Resumen del progreso.\"\"\"\n",
    "        if not self.results:\n",
    "            return \"No hay resultados a√∫n\"\n",
    "\n",
    "        total = len(self.results)\n",
    "        errors = sum(1 for r in self.results if r.get('error', False))\n",
    "        successful = total - errors\n",
    "        models_used = {r.get('model_used', 'unknown'): 0 for r in self.results}\n",
    "        for r in self.results:\n",
    "            model = r.get('model_used', 'unknown')\n",
    "            models_used[model] = models_used.get(model, 0) + 1\n",
    "\n",
    "        return f\"\"\"\n",
    "üìä RESUMEN DEL PROGRESO:\n",
    "- Total procesado: {total}\n",
    "- Exitosos: {successful}\n",
    "- Con errores: {errors}\n",
    "- Modelos usados: {dict(models_used)}\n",
    "- Dataset original: {len(self.dataset)} ejemplos\n",
    "- Progreso: {(total/len(self.dataset)*100):.1f}%\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd921d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 579 ejemplos...\n",
      "Procesando ejemplo 0/579\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "\n",
      "N√∫mero final de ejemplos v√°lidos: 267\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset\n",
    "qa = load_dataset(\"UKPLab/PeerQA\", \"qa\")[\"test\"]\n",
    "\n",
    "# Funci√≥n mejorada para validar el ground truth\n",
    "def is_valid_text(val):\n",
    "    # Verificar si es None\n",
    "    if val is None:\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es NaN float\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es pandas NaN\n",
    "    if pd.isna(val):\n",
    "        return False\n",
    "    \n",
    "    # Convertir a string y verificar\n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "    # Verificar si es string vac√≠o o variaciones de \"nan\"\n",
    "    if val_str == \"\" or val_str in [\"nan\", \"none\", \"null\", \"<na>\", \"n/a\"]:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Extraer ground truth con prioridad y validar\n",
    "def get_ground_truth(example):\n",
    "    for key in [\"answer_free_form_augmented\", \"answer_free_form\"]:\n",
    "        val = example.get(key)\n",
    "        if is_valid_text(val):\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "# Funci√≥n para validar contextos\n",
    "def is_valid_context(context):\n",
    "    if not context:\n",
    "        return False\n",
    "    if isinstance(context, list):\n",
    "        # Verificar que la lista no est√© vac√≠a y tenga elementos v√°lidos\n",
    "        return len(context) > 0 and all(is_valid_text(item) for item in context)\n",
    "    return is_valid_text(context)\n",
    "\n",
    "# Construir dataset filtrado con m√°s debugging\n",
    "examples = []\n",
    "total_examples = len(qa)\n",
    "filtered_stats = {\n",
    "    \"not_answerable\": 0,\n",
    "    \"invalid_paper_id\": 0,\n",
    "    \"no_ground_truth\": 0,\n",
    "    \"invalid_context\": 0,\n",
    "    \"valid\": 0\n",
    "}\n",
    "\n",
    "print(f\"Procesando {total_examples} ejemplos...\")\n",
    "\n",
    "for i, ex in enumerate(qa):\n",
    "    # Debug cada 1000 ejemplos\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Procesando ejemplo {i}/{total_examples}\")\n",
    "    \n",
    "    # Verificar si es answerable\n",
    "    if not ex[\"answerable\"]:\n",
    "        filtered_stats[\"not_answerable\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Verificar paper_id\n",
    "    if not is_valid_text(ex[\"paper_id\"]):\n",
    "        filtered_stats[\"invalid_paper_id\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Obtener ground truth\n",
    "    ground_truth = get_ground_truth(ex)\n",
    "    if not ground_truth:\n",
    "        filtered_stats[\"no_ground_truth\"] += 1\n",
    "        # Debug: imprimir algunos casos problem√°ticos\n",
    "        if filtered_stats[\"no_ground_truth\"] <= 5:\n",
    "            print(f\"Ejemplo sin ground_truth v√°lido:\")\n",
    "            print(f\"  answer_free_form_augmented: {repr(ex.get('answer_free_form_augmented'))}\")\n",
    "            print(f\"  answer_free_form: {repr(ex.get('answer_free_form'))}\")\n",
    "        continue\n",
    "    \n",
    "    # Verificar contexto\n",
    "    context = ex[\"answer_evidence_sent\"] or ex[\"raw_answer_evidence\"]\n",
    "    if not is_valid_context(context):\n",
    "        filtered_stats[\"invalid_context\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Si llegamos aqu√≠, el ejemplo es v√°lido\n",
    "    filtered_stats[\"valid\"] += 1\n",
    "    examples.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"contexts\": context,\n",
    "        \"paper_id\": ex[\"paper_id\"]\n",
    "    })\n",
    "\n",
    "# Crear dataset limpio\n",
    "qa_clean_dataset = Dataset.from_list(examples)\n",
    "\n",
    "print(f\"\\nN√∫mero final de ejemplos v√°lidos: {len(qa_clean_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17571953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "from docling.document_converter import DocumentConverter\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import groq\n",
    "from openai import OpenAI\n",
    "from datasets import Dataset, load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except ImportError:\n",
    "    spacy = None\n",
    "\n",
    "# Descargar recursos NLTK\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "load_dotenv(\"./.env\", override=True)\n",
    "\n",
    "class OptimizedRAGPipeline:\n",
    "    \"\"\"\n",
    "    RAG Optimizado:\n",
    "    - Cross-encoder re-ranking\n",
    "    - B√∫squeda h√≠brida (Dense + BM25)\n",
    "    - Context expansion\n",
    "    - Query expansion\n",
    "    - Prompt engineering avanzado\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 qdrant_host=\"localhost\", \n",
    "                 qdrant_port=6333,\n",
    "                 collection_name=\"advanced_rag_docs\",\n",
    "                 device=\"auto\"):\n",
    "        \n",
    "        print(\"Inicializando RAG Optimizado...\")\n",
    "        \n",
    "        # Configurar dispositivo\n",
    "        if device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        print(f\"Dispositivo: {self.device}\")\n",
    "        \n",
    "        # 1. Docling (mantenido)\n",
    "        self.doc_converter = DocumentConverter()\n",
    "        \n",
    "        # 2. BGE-M3 embeddings (mantenido)\n",
    "        print(\"Cargando BGE-M3...\")\n",
    "        self.embedding_model = SentenceTransformer(\n",
    "            'BAAI/bge-m3',\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.embedding_dim = 1024\n",
    "        \n",
    "        # 3. NUEVO: Cross-encoder para re-ranking\n",
    "        print(\"üéØ Cargando Cross-encoder...\")\n",
    "        try:\n",
    "            self.cross_encoder = CrossEncoder(\n",
    "                'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "                device=self.device\n",
    "            )\n",
    "            print(\"Cross-encoder cargado\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando cross-encoder: {e}\")\n",
    "            self.cross_encoder = None\n",
    "        \n",
    "        # 4. Qdrant (mantenido)\n",
    "        print(\"üóÑÔ∏è Conectando Qdrant...\")\n",
    "        self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)\n",
    "        self.collection_name = collection_name\n",
    "        self._setup_collection()\n",
    "     \n",
    "        # 5. NUEVO: BM25 Index (se construye din√°micamente)\n",
    "        self.bm25_index = None\n",
    "        self.bm25_docs = []\n",
    "        self.bm25_metadata = []\n",
    "        \n",
    "        # 6. spaCy para chunks sem√°nticos (mantenido)\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "        except:\n",
    "            print(\"Modelo espa√±ol no encontrado, usando ingl√©s\")\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except:\n",
    "                print(\"Modelo ingl√©s no encontrado, usando b√°sico\")\n",
    "                self.nlp = spacy.blank(\"es\")\n",
    "        \n",
    "        # 7. Clientes LLM (corregido)\n",
    "        self.gemini_client = None\n",
    "        gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if gemini_key:\n",
    "            genai.configure(api_key=gemini_key)\n",
    "            self.gemini_client = genai.GenerativeModel('gemini-2.0-flash')\n",
    "        \n",
    "        self.groq_client = None\n",
    "        self.openrouter_client = None\n",
    "        \n",
    "        # Inicializar Groq si hay API key\n",
    "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if groq_key:\n",
    "            self.groq_client = groq.Groq(api_key=groq_key)\n",
    "        \n",
    "        # Inicializar OpenRouter si hay API key  \n",
    "        openrouter_key = os.getenv(\"OPENROUTE_API_KEY\")\n",
    "        if openrouter_key:\n",
    "            self.openrouter_client = OpenAI(\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                api_key=openrouter_key\n",
    "            )\n",
    "        \n",
    "        print(\"RAG Optimizado inicializado!\")\n",
    "    \n",
    "    def _setup_collection(self):\n",
    "        \"\"\"Crear colecci√≥n Qdrant (mantenido)\"\"\"\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            if not any(c.name == self.collection_name for c in collections):\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config=VectorParams(size=self.embedding_dim, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"Colecci√≥n '{self.collection_name}' creada\")\n",
    "            else:\n",
    "                print(f\"Colecci√≥n '{self.collection_name}' ya existe\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al configurar colecci√≥n: {e}\")\n",
    "\n",
    "    def _build_bm25_index(self, force_rebuild=False):\n",
    "        \"\"\"NUEVO: Construir √≠ndice BM25 a partir de documentos en Qdrant\"\"\"\n",
    "        if self.bm25_index is not None and not force_rebuild:\n",
    "            return\n",
    "            \n",
    "        print(\"üîç Construyendo √≠ndice BM25...\")\n",
    "        \n",
    "        try:\n",
    "            # Recuperar todos los documentos de Qdrant\n",
    "            all_docs = []\n",
    "            offset = None\n",
    "            \n",
    "            while True:\n",
    "                results = self.qdrant_client.scroll(\n",
    "                    collection_name=self.collection_name,\n",
    "                    limit=1000,\n",
    "                    offset=offset,\n",
    "                    with_payload=True\n",
    "                )\n",
    "                \n",
    "                points, next_offset = results\n",
    "                if not points:\n",
    "                    break\n",
    "                    \n",
    "                all_docs.extend(points)\n",
    "                offset = next_offset\n",
    "                \n",
    "                if next_offset is None:\n",
    "                    break\n",
    "            \n",
    "            print(f\"Recuperados {len(all_docs)} documentos para BM25\")\n",
    "            \n",
    "            # Preparar textos y metadatos\n",
    "            self.bm25_docs = []\n",
    "            self.bm25_metadata = []\n",
    "            \n",
    "            for doc in all_docs:\n",
    "                text = doc.payload.get('text', '')\n",
    "                if text.strip():\n",
    "                    # Tokenizar para BM25\n",
    "                    tokens = self._tokenize_for_bm25(text)\n",
    "                    self.bm25_docs.append(tokens)\n",
    "                    self.bm25_metadata.append({\n",
    "                        'id': doc.id,\n",
    "                        'doc_id': doc.payload.get('doc_id'),\n",
    "                        'chunk_id': doc.payload.get('chunk_id'),\n",
    "                        'text': text,\n",
    "                        'tokens': doc.payload.get('tokens', len(tokens))\n",
    "                    })\n",
    "            \n",
    "            # Construir √≠ndice BM25\n",
    "            if self.bm25_docs:\n",
    "                self.bm25_index = BM25Okapi(self.bm25_docs)\n",
    "                print(f\"√çndice BM25 construido con {len(self.bm25_docs)} documentos\")\n",
    "            else:\n",
    "                print(\"No se encontraron documentos para BM25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error construyendo √≠ndice BM25: {e}\")\n",
    "            self.bm25_index = None\n",
    "    \n",
    "    def _tokenize_for_bm25(self, text: str) -> List[str]:\n",
    "        \"\"\"NUEVO: Tokenizaci√≥n para BM25\"\"\"\n",
    "        # Limpiar y tokenizar\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        tokens = [token for token in text.split() if len(token) > 2]\n",
    "        return tokens\n",
    "    \n",
    "    def _expand_query(self, query: str) -> List[str]:\n",
    "        \"\"\"NUEVO: Expandir query con sin√≥nimos y variaciones\"\"\"\n",
    "        queries = [query]\n",
    "        \n",
    "        # Variaciones b√°sicas\n",
    "        query_lower = query.lower()\n",
    "        if query != query_lower:\n",
    "            queries.append(query_lower)\n",
    "        \n",
    "        # Agregar variaciones sin stopwords\n",
    "        stopwords = {'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'en', 'con', 'por', 'para', 'que', 'se', 'es', 'son', 'fue', 'fueron'}\n",
    "        words = [w for w in query.split() if w.lower() not in stopwords]\n",
    "        if len(words) > 1:\n",
    "            queries.append(' '.join(words))\n",
    "        \n",
    "        # Variaciones con palabras clave\n",
    "        if 'modelo' in query_lower:\n",
    "            queries.append(query.replace('modelo', 'model').replace('Modelo', 'Model'))\n",
    "        if 'datos' in query_lower:\n",
    "            queries.append(query.replace('datos', 'data').replace('Datos', 'Data'))\n",
    "        if 'precisi√≥n' in query_lower or 'accuracy' in query_lower:\n",
    "            queries.append(query.replace('precisi√≥n', 'accuracy').replace('Precisi√≥n', 'Accuracy'))\n",
    "        \n",
    "        return queries[:3]  # M√°ximo 3 variaciones\n",
    "    \n",
    "    def _hybrid_search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"NUEVO: B√∫squeda h√≠brida (Dense + BM25)\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # 1. B√∫squeda densa (sem√°ntica) - ORIGINAL\n",
    "        try:\n",
    "            query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "            dense_results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_emb.tolist(),\n",
    "                limit=min(top_k * 3, 100),\n",
    "                with_payload=True\n",
    "            )\n",
    "            \n",
    "            for result in dense_results:\n",
    "                results.append({\n",
    "                    'id': result.id,\n",
    "                    'text': result.payload['text'],\n",
    "                    'doc_id': result.payload['doc_id'],\n",
    "                    'chunk_id': result.payload['chunk_id'],\n",
    "                    'dense_score': result.score,\n",
    "                    'bm25_score': 0.0,\n",
    "                    'source': 'dense'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en b√∫squeda densa: {e}\")\n",
    "        \n",
    "        # 2. B√∫squeda BM25 (l√©xica) - NUEVO\n",
    "        if self.bm25_index is None:\n",
    "            self._build_bm25_index()\n",
    "        \n",
    "        if self.bm25_index is not None:\n",
    "            try:\n",
    "                # Expandir query\n",
    "                expanded_queries = self._expand_query(query)\n",
    "                bm25_scores_combined = defaultdict(float)\n",
    "                \n",
    "                for q in expanded_queries:\n",
    "                    query_tokens = self._tokenize_for_bm25(q)\n",
    "                    scores = self.bm25_index.get_scores(query_tokens)\n",
    "                    \n",
    "                    for i, score in enumerate(scores):\n",
    "                        if score > 0:\n",
    "                            bm25_scores_combined[i] += score / len(expanded_queries)\n",
    "                \n",
    "                # Agregar mejores resultados BM25\n",
    "                for idx, score in sorted(bm25_scores_combined.items(), key=lambda x: x[1], reverse=True)[:top_k]:\n",
    "                    if idx < len(self.bm25_metadata):\n",
    "                        meta = self.bm25_metadata[idx]\n",
    "                        # Verificar si ya existe (por dense search)\n",
    "                        existing = next((r for r in results if r['id'] == meta['id']), None)\n",
    "                        if existing:\n",
    "                            existing['bm25_score'] = score\n",
    "                            existing['source'] = 'hybrid'\n",
    "                        else:\n",
    "                            results.append({\n",
    "                                'id': meta['id'],\n",
    "                                'text': meta['text'],\n",
    "                                'doc_id': meta['doc_id'],\n",
    "                                'chunk_id': meta['chunk_id'],\n",
    "                                'dense_score': 0.0,\n",
    "                                'bm25_score': score,\n",
    "                                'source': 'bm25'\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error en b√∫squeda BM25: {e}\")\n",
    "        \n",
    "        # 3. Combinar scores h√≠bridos\n",
    "        for result in results:\n",
    "            # Normalizar scores (0-1)\n",
    "            dense_norm = result['dense_score']  # Ya est√° normalizado por cosine\n",
    "            bm25_norm = min(result['bm25_score'] / 10.0, 1.0) if result['bm25_score'] > 0 else 0.0\n",
    "            \n",
    "            # Score h√≠brido\n",
    "            result['hybrid_score'] = (dense_norm * 0.6) + (bm25_norm * 0.4)\n",
    "        \n",
    "        # Ordenar por score h√≠brido\n",
    "        results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _cross_encoder_rerank(self, query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"NUEVO: Re-ranking con cross-encoder\"\"\"\n",
    "        if not results or self.cross_encoder is None:\n",
    "            return results[:top_k]\n",
    "        \n",
    "        try:\n",
    "            # Preparar pares query-documento\n",
    "            pairs = [[query, result['text']] for result in results]\n",
    "            \n",
    "            # Obtener scores del cross-encoder\n",
    "            ce_scores = self.cross_encoder.predict(pairs)\n",
    "            \n",
    "            # Actualizar results con scores del cross-encoder\n",
    "            for result, ce_score in zip(results, ce_scores):\n",
    "                result['ce_score'] = float(ce_score)\n",
    "                # Score final combina h√≠brido y cross-encoder\n",
    "                result['final_score'] = (result['hybrid_score'] * 0.4) + (ce_score * 0.6)\n",
    "            \n",
    "            # Re-ordenar por score final\n",
    "            results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "            print(f\"Cross-encoder reranking aplicado a {len(results)} resultados\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en cross-encoder: {e}\")\n",
    "            # Fallback: usar solo score h√≠brido\n",
    "            for result in results:\n",
    "                result['final_score'] = result['hybrid_score']\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _expand_context(self, results: List[Dict], expansion_size: int = 1) -> List[Dict]:\n",
    "        \"\"\"NUEVO: Expandir contexto con chunks adyacentes\"\"\"\n",
    "        expanded_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            doc_id = result['doc_id']\n",
    "            chunk_id = result['chunk_id']\n",
    "            \n",
    "            # Buscar chunks adyacentes\n",
    "            adjacent_chunks = []\n",
    "            for offset in range(-expansion_size, expansion_size + 1):\n",
    "                if offset == 0:\n",
    "                    continue  # Skip el chunk actual\n",
    "                \n",
    "                target_chunk_id = chunk_id + offset\n",
    "                try:\n",
    "                    # Buscar chunk adyacente\n",
    "                    adjacent_results = self.qdrant_client.scroll(\n",
    "                        collection_name=self.collection_name,\n",
    "                        scroll_filter={\n",
    "                            \"must\": [\n",
    "                                {\"key\": \"doc_id\", \"match\": {\"value\": doc_id}},\n",
    "                                {\"key\": \"chunk_id\", \"match\": {\"value\": target_chunk_id}}\n",
    "                            ]\n",
    "                        },\n",
    "                        limit=1,\n",
    "                        with_payload=True\n",
    "                    )\n",
    "                    \n",
    "                    if adjacent_results[0]:\n",
    "                        adjacent_chunks.append((offset, adjacent_results[0][0].payload['text']))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Construir texto expandido\n",
    "            expanded_text = result['text']\n",
    "            \n",
    "            # Agregar chunks anteriores\n",
    "            before_chunks = [text for offset, text in sorted(adjacent_chunks) if offset < 0]\n",
    "            if before_chunks:\n",
    "                expanded_text = \" [...] \" + \" \".join(before_chunks) + \" \" + expanded_text\n",
    "            \n",
    "            # Agregar chunks posteriores  \n",
    "            after_chunks = [text for offset, text in sorted(adjacent_chunks) if offset > 0]\n",
    "            if after_chunks:\n",
    "                expanded_text = expanded_text + \" \" + \" \".join(after_chunks) + \" [...]\"\n",
    "            \n",
    "            # Crear resultado expandido\n",
    "            expanded_result = result.copy()\n",
    "            expanded_result['text'] = expanded_text\n",
    "            expanded_result['expanded'] = len(adjacent_chunks) > 0\n",
    "            expanded_results.append(expanded_result)\n",
    "        \n",
    "        return expanded_results\n",
    "    \n",
    "    def optimized_search(self, query: str, top_k: int = 5, expand_context: bool = True) -> List[Dict]:\n",
    "        \"\"\"NUEVO: B√∫squeda optimizada completa\"\"\"\n",
    "        print(f\"üîç B√∫squeda optimizada: {query[:50]}...\")\n",
    "        \n",
    "        # 1. B√∫squeda h√≠brida\n",
    "        hybrid_results = self._hybrid_search(query, top_k * 3)\n",
    "        print(f\"Resultados h√≠bridos: {len(hybrid_results)}\")\n",
    "        \n",
    "        # 2. Re-ranking con cross-encoder\n",
    "        reranked_results = self._cross_encoder_rerank(query, hybrid_results, top_k * 2)\n",
    "        print(f\"Despu√©s de re-ranking: {len(reranked_results)}\")\n",
    "        \n",
    "        # 3. Expansi√≥n de contexto\n",
    "        if expand_context:\n",
    "            expanded_results = self._expand_context(reranked_results, expansion_size=0)\n",
    "            print(f\"Contextos expandidos: {sum(1 for r in expanded_results if r.get('expanded', False))}\")\n",
    "        else:\n",
    "            expanded_results = reranked_results\n",
    "        \n",
    "        return expanded_results[:top_k]\n",
    "    \n",
    "    def advanced_generate_answer(self, query: str, contexts: List[str], model: str = \"groq\") -> str:\n",
    "        \"\"\"NUEVO: Generaci√≥n avanzada con mejor prompt engineering\"\"\"\n",
    "        if not contexts:\n",
    "            return \"No se encontr√≥ informaci√≥n relevante en los documentos indexados.\"\n",
    "        \n",
    "        # Preparar contextos numerados y mejorados\n",
    "        context_text = \"\"\n",
    "        for i, ctx in enumerate(contexts[:5], 1):  # M√°ximo 5 contextos\n",
    "            context_text += f\"\\n[CONTEXTO {i}]\\n{ctx}\\n\"\n",
    "        \n",
    "        # Prompt mejorado con t√©cnicas avanzadas\n",
    "        prompt = f\"\"\"Eres un asistente experto en an√°lisis de documentos acad√©micos y t√©cnicos. Tu tarea es responder preguntas bas√°ndote en los contextos proporcionados.\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Analiza cuidadosamente todos los contextos proporcionados\n",
    "2. Si la informaci√≥n est√° presente, proporciona una respuesta completa y detallada\n",
    "3. Si la informaci√≥n est√° parcialmente presente, indica qu√© partes puedes responder y cu√°les no\n",
    "4. no utilices un lenguaje maquinal o robotizado, el tono de las respuestas debe ser natural y profesional como si fueras un asistente\n",
    "5. Si necesitas hacer inferencias l√≥gicas basadas en la informaci√≥n disponible, hazlo expl√≠citamente evitando alucinaciones\n",
    "6. Solo indica \"no tengo suficiente informaci√≥n\" si realmente no hay nada relevante en los contextos\n",
    "\n",
    "CONTEXTOS DISPONIBLES:{context_text}\n",
    "\n",
    "PREGUNTA: {query}\n",
    "\n",
    "AN√ÅLISIS Y RESPUESTA:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if model == \"groq\" and self.groq_client:\n",
    "                response = self.groq_client.chat.completions.create(\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.5,  # Ligeramente m√°s alta para flexibilidad\n",
    "                    max_tokens=1024   # M√°s tokens para respuestas completas\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "            elif model == \"openrouter\" and self.openrouter_client:\n",
    "                response = self.openrouter_client.chat.completions.create(\n",
    "                    model=\"qwen/qwen3-32b\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.5,\n",
    "                    max_tokens=1024\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            elif model == \"gemini\" and self.gemini_client:  # AGREGAR ESTO\n",
    "                response = self.gemini_client.generate_content(prompt)\n",
    "                return response.text\n",
    "            else:\n",
    "                return f\"Modelo '{model}' no disponible. Verifica las API keys.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error generando respuesta: {str(e)}\"\n",
    "    \n",
    "    def query_rag_optimized(self, question: str, model: str = \"groq\", top_k: int = 5) -> Dict:\n",
    "        print(f\"Query optimizada: {question[:50]}...\")\n",
    "        \n",
    "        # B√∫squeda optimizada\n",
    "        results = self.optimized_search(question, top_k, expand_context=True)\n",
    "        contexts = [r['text'] for r in results]\n",
    "        \n",
    "        if not contexts:\n",
    "            answer = \"No se encontraron documentos relevantes para responder la pregunta.\"\n",
    "        else:\n",
    "            # Generaci√≥n avanzada\n",
    "            answer = self.advanced_generate_answer(question, contexts, model)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'contexts': contexts,\n",
    "            'sources': [r['doc_id'] for r in results],\n",
    "            'search_details': {\n",
    "                'total_results': len(results),\n",
    "                'expanded_contexts': sum(1 for r in results if r.get('expanded', False)),\n",
    "                'avg_hybrid_score': np.mean([r['hybrid_score'] for r in results]) if results else 0,\n",
    "                'avg_final_score': np.mean([r.get('final_score', 0) for r in results]) if results else 0\n",
    "            },\n",
    "            'model': model,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    \n",
    "    def document_exists(self, doc_id: str) -> bool:\n",
    "        \"\"\"Verificar si el documento ya est√° indexado (mantenido)\"\"\"\n",
    "        try:\n",
    "            results = self.qdrant_client.scroll(\n",
    "                collection_name=self.collection_name,\n",
    "                scroll_filter={\"must\": [{\"key\": \"doc_id\", \"match\": {\"value\": doc_id}}]},\n",
    "                limit=1\n",
    "            )\n",
    "            return len(results[0]) > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_with_docling(self, file_path: str) -> str:\n",
    "        \"\"\"Extraer texto con Docling (mantenido)\"\"\"\n",
    "        try:\n",
    "            result = self.doc_converter.convert(file_path)\n",
    "            \n",
    "            if hasattr(result.document, 'export_to_markdown'):\n",
    "                text = result.document.export_to_markdown()\n",
    "            elif hasattr(result.document, 'export_to_text'):\n",
    "                text = result.document.export_to_text()\n",
    "            elif hasattr(result.document, 'body'):\n",
    "                text = str(result.document.body)\n",
    "            else:\n",
    "                text = str(result.document)\n",
    "                \n",
    "            print(f\"Texto extra√≠do: {len(text)} caracteres\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error Docling: {e}\")\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()\n",
    "            except:\n",
    "                return \"\"\n",
    "    \n",
    "    def create_semantic_chunks(self, text: str, max_tokens: int = 400) -> List[Dict]:\n",
    "        \"\"\"Crear chunks sem√°nticos (mantenido)\"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            doc = self.nlp(text)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        except:\n",
    "            try:\n",
    "                sentences = sent_tokenize(text)\n",
    "            except:\n",
    "                sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            sentences = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            \n",
    "            if current_tokens + tokens > max_tokens and current_chunk:\n",
    "                chunks.append({\n",
    "                    'text': ' '.join(current_chunk),\n",
    "                    'tokens': current_tokens,\n",
    "                    'id': len(chunks)\n",
    "                })\n",
    "                current_chunk = [sentence]\n",
    "                current_tokens = tokens\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                'text': ' '.join(current_chunk),\n",
    "                'tokens': current_tokens,\n",
    "                'id': len(chunks)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def index_document(self, file_path: str, doc_id: str = None) -> bool:\n",
    "        print(f\"Indexando: {file_path}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Archivo no encontrado: {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        doc_id = doc_id or Path(file_path).stem\n",
    "        if self.document_exists(doc_id):\n",
    "            print(f\"Documento '{doc_id}' ya existe - saltando indexaci√≥n\")\n",
    "            return True\n",
    "        \n",
    "        text = self.extract_with_docling(file_path)\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            print(f\"No se pudo extraer texto v√°lido del archivo\")\n",
    "            return False\n",
    "        \n",
    "        chunks = self.create_semantic_chunks(text)\n",
    "        if not chunks:\n",
    "            print(f\"No se pudieron crear chunks\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Chunks creados: {len(chunks)}\")\n",
    "        \n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        try:\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                texts, \n",
    "                batch_size=8,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generando embeddings: {e}\")\n",
    "            return False\n",
    "        \n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            points.append(PointStruct(\n",
    "                id=hash(f\"{doc_id}_{i}\") & 0x7FFFFFFF,\n",
    "                vector=embedding.tolist(),\n",
    "                payload={\n",
    "                    'doc_id': doc_id,\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk['text'],\n",
    "                    'tokens': chunk['tokens'],\n",
    "                    'file_path': file_path,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=points\n",
    "            )\n",
    "            print(f\"Indexado: {len(points)} chunks\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error insertando en Qdrant: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_peerqa_retriever(self, papers_dataset=None):\n",
    "        \"\"\"Configura el retriever para PeerQA (mantenido)\"\"\"\n",
    "        print(\"üöÄ Configurando retriever para PeerQA...\")\n",
    "        \n",
    "        if papers_dataset is None:\n",
    "            papers_dataset = load_dataset(\"UKPLab/PeerQA\", \"papers\")[\"test\"]\n",
    "        \n",
    "        papers_dict = {}\n",
    "        for paper in papers_dataset:\n",
    "            paper_id = paper['paper_id']\n",
    "            content = paper['content']\n",
    "            if paper_id not in papers_dict:\n",
    "                papers_dict[paper_id] = []\n",
    "            if content.strip():\n",
    "                papers_dict[paper_id].append(content)\n",
    "        \n",
    "        indexed = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for paper_id, contents in papers_dict.items():\n",
    "            try:\n",
    "                if self.document_exists(paper_id):\n",
    "                    print(f\"Documento '{paper_id}' ya indexado\")\n",
    "                    indexed += 1\n",
    "                    continue\n",
    "                \n",
    "                text = \" \".join(contents)\n",
    "                if not text.strip() or len(text) < 50:\n",
    "                    print(f\"Texto vac√≠o o muy corto para paper_id: {paper_id}\")\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                \n",
    "                chunks = self.create_semantic_chunks(text)\n",
    "                if not chunks:\n",
    "                    print(f\"No se pudieron crear chunks para {paper_id}\")\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                \n",
    "                texts = [chunk['text'] for chunk in chunks]\n",
    "                embeddings = self.embedding_model.encode(\n",
    "                    texts,\n",
    "                    batch_size=8,\n",
    "                    normalize_embeddings=True,\n",
    "                    show_progress_bar=True\n",
    "                )\n",
    "                \n",
    "                points = [\n",
    "                    PointStruct(\n",
    "                        id=hash(f\"{paper_id}_{i}\") & 0x7FFFFFFF,\n",
    "                        vector=embedding.tolist(),\n",
    "                        payload={\n",
    "                            'doc_id': paper_id,\n",
    "                            'chunk_id': i,\n",
    "                            'text': chunk['text'],\n",
    "                            'tokens': chunk['tokens'],\n",
    "                            'timestamp': datetime.now().isoformat()\n",
    "                        }\n",
    "                    ) for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
    "                ]\n",
    "                \n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=points\n",
    "                )\n",
    "                print(f\"Indexado: {paper_id} ({len(points)} chunks)\")\n",
    "                indexed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error indexando {paper_id}: {str(e)}\")\n",
    "                failed += 1\n",
    "        \n",
    "        print(f\"Indexaci√≥n completada: {indexed} exitosos, {failed} fallidos\")\n",
    "        \n",
    "        # IMPORTANTE: Rebuild BM25 index despu√©s de indexar nuevos documentos\n",
    "        if indexed > 0:\n",
    "            print(\"Reconstruyendo √≠ndice BM25...\")\n",
    "            self._build_bm25_index(force_rebuild=True)\n",
    "        \n",
    "        return indexed, failed\n",
    "\n",
    "    def get_collection_info(self) -> Dict:\n",
    "        \"\"\"Informaci√≥n de la colecci√≥n (mantenido)\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.collection_name)\n",
    "            return {\n",
    "                'name': self.collection_name,\n",
    "                'points_count': info.points_count,\n",
    "                'vectors_count': info.vectors_count,\n",
    "                'indexed': info.indexed_vectors_count,\n",
    "                'bm25_ready': self.bm25_index is not None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    # M√âTODOS DE COMPATIBILIDAD (wrappers para m√©todos originales)\n",
    "    \n",
    "    def agent_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Wrapper para compatibilidad - usa b√∫squeda optimizada\"\"\"\n",
    "        results = self.optimized_search(query, top_k, expand_context=False)\n",
    "        \n",
    "        # Convertir formato para compatibilidad\n",
    "        compatible_results = []\n",
    "        for r in results:\n",
    "            compatible_results.append({\n",
    "                'text': r['text'],\n",
    "                'doc_id': r['doc_id'], \n",
    "                'score': r.get('final_score', r['hybrid_score']),\n",
    "                'chunk_id': r['chunk_id']\n",
    "            })\n",
    "        \n",
    "        return compatible_results\n",
    "    \n",
    "    def generate_answer(self, query: str, contexts: List[str], model: str = \"groq\") -> str:\n",
    "        \"\"\"Wrapper para compatibilidad - usa generaci√≥n avanzada\"\"\"\n",
    "        return self.advanced_generate_answer(query, contexts, model)\n",
    "    \n",
    "    def query_rag(self, question: str, model: str = \"groq\", top_k: int = 5) -> Dict:\n",
    "        \"\"\"Wrapper para compatibilidad - usa query optimizada\"\"\"\n",
    "        return self.query_rag_optimized(question, model, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1084d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando RAG Optimizado...\n",
      "Dispositivo: cuda\n",
      "Cargando BGE-M3...\n",
      "üéØ Cargando Cross-encoder...\n",
      "Cross-encoder cargado\n",
      "üóÑÔ∏è Conectando Qdrant...\n",
      "Colecci√≥n 'advanced_rag_docs' ya existe\n",
      "RAG Optimizado inicializado!\n"
     ]
    }
   ],
   "source": [
    "rag = OptimizedRAGPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd06d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando RAG Optimizado...\n",
      "Dispositivo: cuda\n",
      "Cargando BGE-M3...\n",
      "üéØ Cargando Cross-encoder...\n",
      "Cross-encoder cargado\n",
      "üóÑÔ∏è Conectando Qdrant...\n",
      "Colecci√≥n 'advanced_rag_docs' ya existe\n",
      "RAG Optimizado inicializado!\n",
      "Indexando: C:\\Users\\danie\\Downloads\\prueba\\data\\1-s2.0-S0378517324005799-main.pdf\n",
      "Documento 'my_pdf_document' ya existe - saltando indexaci√≥n\n",
      "PDF C:\\Users\\danie\\Downloads\\prueba\\data\\1-s2.0-S0378517324005799-main.pdf indexado correctamente\n",
      "Query optimizada: ¬øQu√© beneficios ofrece el uso de nanopart√≠culas en...\n",
      "üîç B√∫squeda optimizada: ¬øQu√© beneficios ofrece el uso de nanopart√≠culas en...\n",
      "üîç Construyendo √≠ndice BM25...\n",
      "Recuperados 1513 documentos para BM25\n",
      "√çndice BM25 construido con 1513 documentos\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Pregunta: ¬øQu√© beneficios ofrece el uso de nanopart√≠culas en comparaci√≥n con los medicamentos tradicionales en el tratamiento del c√°ncer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Bas√°ndome en los contextos proporcionados, puedo responder a la pregunta de la siguiente manera:\n",
      "\n",
      "El uso de nanopart√≠culas en el tratamiento del c√°ncer ofrece varios beneficios en comparaci√≥n con los medicamentos tradicionales. Seg√∫n los contextos, estos beneficios incluyen:\n",
      "\n",
      "1. **Mayor eficacia**: Las nanopart√≠culas permiten una entrega m√°s precisa y eficiente de los medicamentos a las c√©lulas objetivo, lo que conduce a una mayor eficacia en el tratamiento del c√°ncer.\n",
      "\n",
      "2. **Reducci√≥n de efectos secundarios**: Las nanopart√≠culas pueden reducir la toxicidad y los efectos secundarios de los medicamentos, ya que solo se dirigen a las c√©lulas objetivo y no a tejidos sanos.\n",
      "\n",
      "3. **Mejora de la adherencia del paciente**: Debido a la reducci√≥n de efectos secundarios, los pacientes son m√°s propensos a adherirse a la terapia, lo que conduce a mejores resultados en el tratamiento.\n",
      "\n",
      "4. **Acceso a mol√©culas hidrof√≥bicas**: Las nanopart√≠culas pueden transportar mol√©culas hidrof√≥bicas que de otra manera no podr√≠an ser utilizadas debido a su baja solubilidad en agua.\n",
      "\n",
      "5. **Controlado release de sustancias activas**: Las nanopart√≠culas pueden controlar la liberaci√≥n de sustancias activas en el lugar y momento adecuados, lo que conduce a una mayor eficacia en el tratamiento.\n",
      "\n",
      "En resumen, el uso de nanopart√≠culas en el tratamiento del c√°ncer ofrece beneficios significativos en comparaci√≥n con los medicamentos tradicionales, incluyendo una mayor eficacia, reducci√≥n de efectos secundarios, mejora de la adherencia del paciente, acceso a mol√©culas hidrof√≥bicas y controlado release de sustancias activas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 1. Inicializar el pipeline RAG\n",
    "rag = OptimizedRAGPipeline(\n",
    "    qdrant_host=\"localhost\", \n",
    "    qdrant_port=6333,\n",
    "    collection_name=\"advanced_rag_docs\",\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Indexar un archivo PDF\n",
    "pdf_path = r'C:\\Users\\danie\\Downloads\\prueba\\data\\1-s2.0-S0378517324005799-main.pdf'\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: El archivo {pdf_path} no existe\")\n",
    "else:\n",
    "    success = rag.index_document(pdf_path, doc_id=\"my_pdf_document\")\n",
    "    if success:\n",
    "        print(f\"PDF {pdf_path} indexado correctamente\")\n",
    "    else:\n",
    "        print(f\"Error al indexar {pdf_path}\")\n",
    "\n",
    "# 3. Consulta optimizada\n",
    "query = \"¬øQu√© beneficios ofrece el uso de nanopart√≠culas en comparaci√≥n con los medicamentos tradicionales en el tratamiento del c√°ncer?\"\n",
    "result = rag.query_rag_optimized(query, model=\"groq\")\n",
    "\n",
    "# 4. Mostrar resultados\n",
    "display(Markdown(f\"\\nPregunta: {result['question']}\"))\n",
    "print(f\"Respuesta: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c00d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando RAG Optimizado...\n",
      "Dispositivo: cuda\n",
      "Cargando BGE-M3...\n",
      "üéØ Cargando Cross-encoder...\n",
      "Cross-encoder cargado\n",
      "üóÑÔ∏è Conectando Qdrant...\n",
      "Colecci√≥n 'advanced_rag_docs' ya existe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Optimizado inicializado!\n",
      "‚úÖ Inicializado AdvancedRAGEvaluationGenerator con 267 ejemplos\n",
      "Indexando documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configurando retriever para PeerQA...\n",
      "Documento 'nlpeer/F1000-22/10-72' ya indexado\n",
      "Documento 'nlpeer/F1000-22/10-170' ya indexado\n",
      "Documento 'nlpeer/F1000-22/10-838' ya indexado\n",
      "Documento 'nlpeer/F1000-22/10-637' ya indexado\n",
      "Documento 'nlpeer/F1000-22/11-404' ya indexado\n",
      "Documento 'nlpeer/F1000-22/10-654' ya indexado\n",
      "Documento 'nlpeer/F1000-22/10-890' ya indexado\n",
      "Documento 'nlpeer/F1000-22/11-222' ya indexado\n",
      "Documento 'nlpeer/F1000-22/11-195' ya indexado\n",
      "Documento 'nlpeer/F1000-22/11-9' ya indexado\n",
      "Documento 'nlpeer/PeerRead-CONLL2016/166' ya indexado\n",
      "Documento 'nlpeer/PeerRead-CONLL2016/142' ya indexado\n",
      "Documento 'nlpeer/PeerRead-CONLL2016/129' ya indexado\n",
      "Documento 'nlpeer/PeerRead-CONLL2016/12' ya indexado\n",
      "Documento 'nlpeer/PeerRead-CONLL2016/13' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1405' ya indexado\n",
      "Documento 'nlpeer/COLING2020/939' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1265' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1570' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1775' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1367' ya indexado\n",
      "Documento 'nlpeer/COLING2020/660' ya indexado\n",
      "Documento 'nlpeer/COLING2020/666' ya indexado\n",
      "Documento 'nlpeer/COLING2020/341' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1315' ya indexado\n",
      "Documento 'nlpeer/COLING2020/679' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1550' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1886' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1289' ya indexado\n",
      "Documento 'nlpeer/COLING2020/1681' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/561' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/193' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/169' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/818' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/619' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/496' ya indexado\n",
      "Documento 'nlpeer/PeerRead-ACL2017/699' ya indexado\n",
      "Documento 'nlpeer/ARR-22/24714d27941075cbad95c02db3ac730df71d355f85f5d247a62c9453ea29004b15cfb386663c82ce55ba17d652253064f4ff70f21dd5c08d8e39986ab22c45ce' ya indexado\n",
      "Documento 'nlpeer/ARR-22/3282bdf968d7f87ae69c4ae7fdd35d4e9a46fa83f56d8bee0296c16dcf23b23cb1e4f20f510c7d01fb7be2309c08dd29cc240b54bdcbc3d66969041b5c84077a' ya indexado\n",
      "Documento 'nlpeer/ARR-22/7e701552b7cfaad6239d3d5dfcd9da1852a03a66907c789fe65d368f1c2845e723adcecd7f7008849eef13c7ca467be73de53747c3376a6bb46d1911ed1b6409' ya indexado\n",
      "Documento 'nlpeer/ARR-22/eb931c7319dc1b08fb3c97fb77b60b7eac9a41d699b58e0139d655f4d1cbc5c61db8d9e9dd42e3515a6cd839518933bd7d033289be73a8111b2170e8b2bf983f' ya indexado\n",
      "Documento 'nlpeer/ARR-22/e446205668a3693734879adf570761b4430fec423bb0696e772f21d237781bb9882cedc47e5172eec7d9f11b2699391177e92d0badacd5e2713d816871fcdc3d' ya indexado\n",
      "Documento 'nlpeer/ARR-22/b6a36ede033d3b89745124150ad5a8837225962c2fe74be65707e75472c5061810f3ce4c771001df53c5109e661207c14952ade0fd5520fb55f9dcba19695fa3' ya indexado\n",
      "Documento 'nlpeer/ARR-22/013b9bf63a6f68fd0c3ecc36f8cbe2ad5bc92ea3bfe5a9f6c15eb056ecc4f858718410182c3765b2dc2695ae29ba08fb5dea5fc495faf2bbb77205bc3f765fcd' ya indexado\n",
      "Documento 'nlpeer/ARR-22/91d9e763eb1470028002e5c97689ada4b74f30be17291a14458f7542c387a9f2a7516e60f4022d887bdbd2165aa8cce6d566db380e5a82af677917c48efc2446' ya indexado\n",
      "Documento 'nlpeer/ARR-22/b94a77b25cea79056553cf9dc3c5b714410344acb0f1cf0f5eb4ff2402b61bfa005313d5e2d103aedaff9b4d1fdeadd9799df73921794d6093799ec80fbb13c1' ya indexado\n",
      "Documento 'nlpeer/ARR-22/27046f6633d116b03e48eb94976921b46a86a976ddd2199ef76c90820a95e0d85406daa5ebc9d7b12359157d20f3bd0025e1cbc2c126bb2506f5e83e93f435b5' ya indexado\n",
      "Documento 'nlpeer/ARR-22/6c1774fe04e0512fceaec933424ffae45c425a9a37ba335d4ec33115b793596036830b117da4efc17b34cb67ec5cc4a23d81c8c09729ad5f5d5f05613ae1eac8' ya indexado\n",
      "Documento 'nlpeer/ARR-22/df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c' ya indexado\n",
      "Documento 'nlpeer/ARR-22/488687633dbb7f9cbac3b0164d08e1cef083aa01ab86fad82e7c5b199bda3c1213b66dfcf3627a183c977d0aee40f46f169cdd51a5efe539ec4fc02981a383f7' ya indexado\n",
      "Documento 'nlpeer/ARR-22/83dc885bef279cbb7a3b5b173494428b6cd043d34527016519250477d274fd1793fa0bddd2bd4bbb3edfa9709ddc85e90825a0554bfa8eceb8cb34d813c06c53' ya indexado\n",
      "Documento 'nlpeer/ARR-22/82520f1d31572d488cfeae02712db642750fda4063e5eab8a771a000f6e17e1397ab18af73e5362e4d660b3dcc78b2260e121fdefc4c1664448e1c30066d5cb9' ya indexado\n",
      "Documento 'nlpeer/ARR-22/0f595b4bd968ef5daacc88c4edb581bf7f683af30b6874ba157c6d2282aabead577041409b949924e4bc5ef54c543d85a10e962be8e70304dea65e1b18441bdb' ya indexado\n",
      "Documento 'nlpeer/ARR-22/0f8bc751a0472564732a976cd152643d92d5393c2e77c8234f386a801883d09ff9540378b3dab5834544a246c22f292d5b1c4d370219943c9a19a1626a8f8781' ya indexado\n",
      "Documento 'nlpeer/ARR-22/1370310ca3e0e0d092a2e8b58f90b0d692331caa6e3ce5ca6dacbef926f2b01ff49c81282f32c554fa66c04979a125b7548c9692c2091bb2673642102d85412d' ya indexado\n",
      "Documento 'nlpeer/ARR-22/826bda43aa717e37d1b584c3c637da2700f29c415a709db4c3188f13f85751d30d29348dfe8f635cda5b6370145e9bbc00465e76921ac9a6abdb0eee6cabdd4b' ya indexado\n",
      "Documento 'nlpeer/ARR-22/eeda0f4ff6c7aef2647481ec15f8ade5a9b32ec32b643e6918a0b8d0ffa1ba44a3f2b9171ba4ba44d5c0faf3199e3bc5f0f740ee31cf48cb966493a96f35ca71' ya indexado\n",
      "Documento 'nlpeer/ARR-22/841d64f4eb3d5e3d6ea89751ea19897bb8424acf856c9dd1574ef7d2803ff7d155df1b9cc201df8b4fc8c1984f6ca8bdfaf94036960348047e8da35b0003ae34' ya indexado\n",
      "Documento 'nlpeer/ARR-22/88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148' ya indexado\n",
      "Documento 'nlpeer/ARR-22/e4742f934fad6dc1f3389981d0d0fc6b8b48638f4c5c0a74b9dc11bdd026489bb2d3e78a6a395e82124e87c7409ff370cdff0edf375958bcf909372da1859815' ya indexado\n",
      "Documento 'nlpeer/ARR-22/3cbf91f8e2f7e62be51109688c84a2b424e6018ee3c976fbe55e5331bc9ab2f4c46491d006087aed3be35a3370c945c100bd718e2c1131de19eac3dbe93326bc' ya indexado\n",
      "Documento 'nlpeer/ARR-22/212dff6380418cd7c050ec757b31e6b5b65bbe922aa30ffbd5950fe7a04ca737b7c7b3d706f1cd1502d7932b61d2b7c079924793e45e437526230c1e9c0626ed' ya indexado\n",
      "Documento 'nlpeer/ARR-22/ed81d66dc55496061fd4c97530be7b966d2e734e8435092b2c0c6764f54ea145e62f67555e480d996aebb93772e0de2877ccaad504e0f2228a1395c68d157eda' ya indexado\n",
      "Documento 'nlpeer/ARR-22/6d1b7e47f044e29d515db7e4762ace8e3f089fe88c2a9de074e8d71f0f78da6169be5756ec9320b5354d2f0ebef3352fedbf5dd82012a7eafde0b0dfbfc311df' ya indexado\n",
      "Documento 'nlpeer/ARR-22/ab82036287f47b63ae70bdae7c1ed5900395bfe4538cc14db1a17281e2b45434b348472df2024151e188fe3890b88a0990bf1a36540eb39ddcc015012567bdd8' ya indexado\n",
      "Documento 'nlpeer/ARR-22/4b7c5fafdc37dc7cb22b9868c3dc5a3df61127f67fe8b48aaebe06624f82096f9a2fd1d81f57f86ecd7bfaae463ae744196e2c0e5f132cbd9ccff1065f620afe' ya indexado\n",
      "Documento 'nlpeer/ARR-22/a4c721144a8df14739ad3a6daa2b625723a1b9e085b0b5b0e64ec74fbca3ded7b0f3ae64d7539ee8465e6f43eee46c1f0c81fbeb96d6c0a53e3b89762b598f46' ya indexado\n",
      "Documento 'nlpeer/ARR-22/db3d0c16f2a8a3baf7fe008d20bfe3fd82dedab22900659a9d8352b2ecce076fa9a82869602cc78d2c3b732f645c25eacb9b0b6fc4d3b6441bfb1825109e85ea' ya indexado\n",
      "Documento 'nlpeer/ARR-22/1fa0c054c69bce9681f7ff64a1173df6c9bfc1ed52e1d5999f09f02d2a9187bde00c7f727bf9b5d7afe299ae00b0c0dd7f0d220f22439cb8ccf7b863bb2749bb' ya indexado\n",
      "Documento 'nlpeer/ARR-22/25eb5aa88234e90e4d4729fea900619c7d42062b6c8c967d02567ea0548fbc60fc4cacfd968a7f10d96f9cd92302173b1db10f7767e3b7e379ff1260c12db74d' ya indexado\n",
      "Documento 'nlpeer/ARR-22/6262c349a3980c6850e252b3a44391b1b1318c26fbf29be55c61c0a32b8b626e5581c98234d1ecb410674c00ebcf3ba2adfdce41c2b2d32211721372a8ed1bc0' ya indexado\n",
      "Documento 'nlpeer/ARR-22/ae39c92fcd34f3ea307de8aae9eade5a405f4035c3260c64416f75a202f27dbc22063341ee73afacd4459fdee1a00d6e84c1317f5271bb087c826599f8989dd4' ya indexado\n",
      "Documento 'nlpeer/ARR-22/43e8458544e18304f6aa2c1aededf974877f088dd2ccd7d53ef71eb59e54567c0572f715ecdcbaf04c39206950c3f5628198e61ebc5ec09fa1ba0a559102f923' ya indexado\n",
      "Documento 'nlpeer/ARR-22/21dd1d5e232c7abf421302a95ef2c9ef44c7bad069da7ca4129a8cf1bfbd6f660afee123144ebcbd8da698aed212e2ca1151a13bca7aaef50ce59d27d23aa83a' ya indexado\n",
      "Documento 'nlpeer/ARR-22/dd0a83852cb35c8d0ac0c40da6b7ebeb385f9c7e4816ef51f43abd6910a9adacc0daf460dfda4e56f37fa542a749961a78ddbbe74af0326f41e5cd38e867b121' ya indexado\n",
      "Documento 'nlpeer/ARR-22/78d7b31f3d19cf16bcc71aa109ca56b923d9082a2919fd8008060641daf4d5ae37d70daea5bb36f4aa83c9c49ad8b44e597aa41960348f3a0abe0abb529f9aea' ya indexado\n",
      "Documento 'nlpeer/ARR-22/3cfcbbad78a71ca6cf4a5b2bbaee2a7ad8a1de295cf9f4103408dfeaf38a0de01b2b8d23cb94e0ef0b5f76a1947815f0335a7077bdea03ea4a29f6a2bff432cc' ya indexado\n",
      "Documento 'nlpeer/ARR-22/026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731' ya indexado\n",
      "Documento 'nlpeer/ARR-22/c53c0e92d95b1e222ebbecee84efd55c426d3f9916ac9f50280eaebe6640d35f9d395744c228f1f352eb4954a72a42b3886d581396fa3202056879d792fb62b7' ya indexado\n",
      "Documento 'nlpeer/ARR-22/12ea2851de6445bf343d642c6ccc8640bf7a6c28b5f20ba123d6d58f02790c7c7f8ab696e6a6b4e45bfad28bd4bc0419bdcf6358693404dd0c14d202b291503b' ya indexado\n",
      "Documento 'nlpeer/ARR-22/4d34c29e099f5d2f50baaf60b1c39792c2a4748b20aaef9eee1cb7c9699ab571bf4822fa0f028af5c3eb936b4738de8068408068c869b1832d2b170d0956fe96' ya indexado\n",
      "Documento 'nlpeer/ARR-22/3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d' ya indexado\n",
      "Documento 'egu/esurf/11-33-2023' ya indexado\n",
      "Documento 'egu/esurf/11-849-2023' ya indexado\n",
      "Documento 'egu/esurf/11-917-2023' ya indexado\n",
      "Documento 'egu/esd/14-915-2023' ya indexado\n",
      "Documento 'egu/esd/14-1261-2023' ya indexado\n",
      "Documento 'egu/esd/14-835-2023' ya indexado\n",
      "Documento 'egu/esd/14-81-2023' ya indexado\n",
      "Documento 'egu/esd/14-185-2023' ya indexado\n",
      "Indexaci√≥n completada: 90 exitosos, 0 fallidos\n",
      "Reconstruyendo √≠ndice BM25...\n",
      "üîç Construyendo √≠ndice BM25...\n",
      "Recuperados 1513 documentos para BM25\n",
      "√çndice BM25 construido con 1513 documentos\n",
      "Indexaci√≥n completada: 90 exitosos, 0 fallidos\n",
      "Generando respuestas con openrouter\n",
      "Procesando desde √≠ndice 0 hasta 10\n",
      "Lotes de 5 con 30s de espera\n",
      "Ya procesados: 0 ejemplos\n",
      "\n",
      "üîÑ Procesando lote 1: √≠ndices 0-4\n",
      "[1/10] Procesando pregunta: How does the proposed method compare to Newton and...\n",
      "Query optimizada: How does the proposed method compare to Newton and...\n",
      "üîç B√∫squeda optimizada: How does the proposed method compare to Newton and...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[2/10] Procesando pregunta: Are the annotators of the test sets native English...\n",
      "Query optimizada: Are the annotators of the test sets native English...\n",
      "üîç B√∫squeda optimizada: Are the annotators of the test sets native English...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[3/10] Procesando pregunta: Does the cross-linguistic analysis include French ...\n",
      "Query optimizada: Does the cross-linguistic analysis include French ...\n",
      "üîç B√∫squeda optimizada: Does the cross-linguistic analysis include French ...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[4/10] Procesando pregunta: Why were classification-based models not used for ...\n",
      "Query optimizada: Why were classification-based models not used for ...\n",
      "üîç B√∫squeda optimizada: Why were classification-based models not used for ...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[5/10] Procesando pregunta: What is the reason for the lower performance on Vi...\n",
      "Query optimizada: What is the reason for the lower performance on Vi...\n",
      "üîç B√∫squeda optimizada: What is the reason for the lower performance on Vi...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "üíæ Progreso guardado: 5 ejemplos completados\n",
      "‚è≥ Esperando 30 segundos...\n",
      "\n",
      "üîÑ Procesando lote 2: √≠ndices 5-9\n",
      "[6/10] Procesando pregunta: Do you mean that a node in the UCCA scheme can be ...\n",
      "Query optimizada: Do you mean that a node in the UCCA scheme can be ...\n",
      "üîç B√∫squeda optimizada: Do you mean that a node in the UCCA scheme can be ...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[7/10] Procesando pregunta: What is the purpose of the average duration report...\n",
      "Query optimizada: What is the purpose of the average duration report...\n",
      "üîç B√∫squeda optimizada: What is the purpose of the average duration report...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[8/10] Procesando pregunta: What is the evidence that the generative model is ...\n",
      "Query optimizada: What is the evidence that the generative model is ...\n",
      "üîç B√∫squeda optimizada: What is the evidence that the generative model is ...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[9/10] Procesando pregunta: Can the proposed algorithm be used to recover real...\n",
      "Query optimizada: Can the proposed algorithm be used to recover real...\n",
      "üîç B√∫squeda optimizada: Can the proposed algorithm be used to recover real...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "[10/10] Procesando pregunta: Can the pairwise distance of the latent variables ...\n",
      "Query optimizada: Can the pairwise distance of the latent variables ...\n",
      "üîç B√∫squeda optimizada: Can the pairwise distance of the latent variables ...\n",
      "Resultados h√≠bridos: 15\n",
      "Cross-encoder reranking aplicado a 15 resultados\n",
      "Despu√©s de re-ranking: 10\n",
      "Contextos expandidos: 0\n",
      "  ‚úÖ Completado!\n",
      "üíæ Progreso guardado: 10 ejemplos completados\n",
      "\n",
      "üéâ ¬°Proceso completado! Total: 10 ejemplos\n",
      "\n",
      "üìä RESUMEN DEL PROGRESO:\n",
      "- Total procesado: 10\n",
      "- Exitosos: 10\n",
      "- Con errores: 0\n",
      "- Modelos usados: {'openrouter': 10}\n",
      "- Dataset original: 267 ejemplos\n",
      "- Progreso: 3.7%\n",
      "        \n",
      "üìä Dataset RAGAS creado: 10 ejemplos\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = OptimizedRAGPipeline()\n",
    "\n",
    "# Crear generador\n",
    "generator = AdvancedRAGEvaluationGenerator(\n",
    "    rag_pipeline=rag_pipeline,\n",
    "    clean_dataset=qa_clean_dataset,\n",
    "    papers_dir=None,  \n",
    "    batch_size=5,\n",
    "    delay=30\n",
    ")\n",
    "\n",
    "# Indexar documentos (redundante, pero asegura que todos los papers est√©n indexados)\n",
    "generator.index_documents()\n",
    "\n",
    "# Generar respuestas (prueba con 30 ejemplos)\n",
    "generator.generate_responses_batch(model_type='openrouter', max_samples=10)\n",
    "\n",
    "# Ver resumen\n",
    "print(generator.get_summary())\n",
    "\n",
    "# Crear dataset RAGAS\n",
    "ragas_dataset = generator.get_ragas_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuevo_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
