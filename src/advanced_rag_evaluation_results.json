[
  {
    "question": "How does the proposed method compare to Newton and standard quasi-Newton methods such as BFGS, L-BFGS, Chord, and Levenberg–Marquardt?",
    "answer": "No tengo suficiente información para responder a esta pregunta. Los contextos proporcionados no mencionan ni comparan el método propuesto con algoritmos de optimización como Newton, BFGS, L-BFGS, Chord o Levenberg–Marquardt. Las comparaciones descritas en los textos se centran en tareas de distilación de conocimiento, representación de entidades, expansión de conjuntos de entidades o enfoques de aprendizaje multitarea, sin referirse a métodos de optimización numérica. Si la pregunta se refiere a un contexto diferente no incluido aquí, sería necesario revisar dicha información adicional.",
    "contexts": [
      "The proposed initialization scheme is very simple: we initialize the student model with the weights from the corresponding layers of the teacher. More precisely, let N s denote the number of layers (including both frozen and task-specific layers) in the student, where N s < 12. We propose to initialize the student from the bottommost N s layers of the teacher. Similar approach has also been used in (Sanh et al. , 2019), where the student is initialized by taking one layer out of two from the teacher. The value of l (τ ) , i.e. the number of task-specific layers in the student model for task τ , determines the final memory and computation overhead for that task. Model Merging In the final step, we merge the single-task student models into one multi-task model (Fig. 1(c)) so that the parameters and computations carried out in the frozen layers can be shared. To achieve this, it suffices to load weights from multiple model checkpoints into one computation graph. Experiments In this section, we compare the performance and efficiency of our model with various baselines on eight GLUE tasks (Wang et al. , 2019b). More details on these tasks can be found in Appendix A. Metrics The performance metrics for GLUE tasks is accuracy except for CoLA and STS-B. We use Matthews correlation for CoLA, and Pearson correlation for STS-B. To measure the parameter and computational efficiency, we introduce the total number of transformer layers that are needed to perform inference for all eight tasks. For the models studied in our experiments, the actual memory usage and the computational overhead are approximately linear with respect to this number. It is named \"overhead\" in the header of Table 2. Baselines The baseline models/methods can be divided into 4 categories: Single-task without KD. There is only one method in this category, i.e. the standard practice of single task full fine-tuning that creates a separate model for each task. Single-task with KD. The methods in this category create a separate model for each task, but a certain knowledge distillation method is applied to compress each task model into a 6-layer one. The KD methods include (Hinton et al. , 2015;Xu et al., 2020b;Sanh et al., 2019;Turc et al., 2019;Sun et al., 2019b;Jiao et al. , 2020;Wang et al., 2020). Multi-task learning. This category includes two versions of MT-DNN (Liu et al.",
      "We report the linking accuracy in Table 6 and leave out models from previous works since they do not report linking metrics of their candidate generators. We specify the inference method used in each case, chosen based on the validation set accuracy of the models. Similar to our cross-encoder results in Table 3, we also report the \"seen\" and \"unseen\" performance on MedMentions. In Table 7 andTable 8, we provide a set of representative examples that demonstrate the improvement in entity linking that our proposed coreferencebased methodology empirically provides on Med-Mentions and ZeShEL, respectively. A third training objective we explore modifies the initial k-NN graph construction by restricting the nodes to the gold entity, the query mention, and a random withincluster mention neighbor of the query, instead of the nearest-neighbor. Baselines We compare to two baselines following previous work: (a) training ψ(•, •) with random negatives (IN-BATCH NEGATIVES) where each gold entity for a mention in a training batch is treated as a negative example for all other mentions in the batch, and (b) training ψ(•, •) with hard negatives (K-NN NEGATIVES) similar to the negative mining in our proposed methods albeit with only mention-entity positive selection. Figure 2: Coreference Distribution. We count the number of mentions in each coreference cluster in MedMentions and ZeShEL. Clusters in ZeShEL are typically very small (at most 3) compared to the MedMentions clusters (up to 1256, 434, and 447 in train, validation, and test, respectively). For each mention m, construct E m by (a) adding edges from m's k-nearest neighbor mentions in M to m, and (b) adding an edge from m's nearest entity to m:E m = (u, m) u ∈ argmink m ′ ∈ M w m ′ ,m ∨ u = argmin e ∈ E w e,m Arborescence-based Training  Objective for Mention & Entity Representations. Shown above is an illustrative example of our proposed training objective for a dual-encoder (Enc M , Enc E ) on real mentions and entities from the MedMentions dataset. Mentions are highlighted in context and entities from UMLS are represented using grey boxes with the name and unique identifier. First, each mention and entity is encoded into a dense vector Dual-Encoder Retriever Results: Recall@k ( † Proposed methods; ‡ Angell et al. (2021); ‡ ‡ Wu et al. (2020)) Dataset Statistics. |M| is the number of mentions.",
      "We then infer the start and end times using the ILP problem in Section 3.1. Although the dataset does not include statistics for \"noon\" (due to the lack of a corresponding greeting), the global objective in the ILP formulation is expected to infer the start and end times for noon based on the surrounding time expressions. Results Figure 4 displays the predicted start and end times for each expression according to each method, in comparison to the gold standard times of each language. For quantitative evaluation, we define minute-level accuracy. We classify each minute of the day to a time expression based on the start and end times, and compute the accuracy compared to the gold standard minute classification. Since the gold standard grounding allows overlap between time expressions, we reward models for predicting any of the gold standard time expressions for a given minute. as the average differences in hours between the predicted and gold standard start (∆Start) and end (∆End) times. There is a general preference for the extractive method, that achieves between 65% and 90% accuracy across languages. The exception is Portuguese, where this method performs worse than the others, and in particular by the LM Start-End method that performs remarkably well. The two LM-based methods perform substantially worse on the other languages. Finally, the results for India are surprisingly not bad despite the mismatch between the native languages of the annotators and the language used by our methods. Table 3: Start and end time for various languages, as predicted by the extractive method, along with the percent of corpus occurrences for each expression. Application To Other Languages We applied our proposed methods to additional unlabelled languages detailed in Table 3. The languages are sorted according to their Wikipedia corpus size. The Table shows the predicted start and end time for each language and each time expression. 9 Without labelled data it is hard to judge the correctness of the predictions, but the predictions of some languages seem more reasonable than others. In particular, we observed that some time expressions appeared in the corpus more frequently than others, causing the model to dedicate most of the 24 hours to such expressions. The percent column in Table 3 show the percent of all corpus occurrences dedicated to each expression.",
      "We now discuss how different characteristics of user-generated text impact the behavior of ESE methods. To understand this, we compare the recall of entities that exhibit one of the target characteristics (multifaceted/non-named/vague) with recall of entities that do not exhibit any of the characteristics. This enables us to analyze the influence of a target characteristic independent of other characteristics. To compute recall, we consider an entity as retrieved if it is ranked in the top-k g predictions. Figure 4 compares the recall of entities across different characteristics. For ease of visualization, we combine entities across the 3 benchmarks. As shown, almost all methods show lower recall of entities that exhibit challenging characteristics than entities without these characteristics, and SOTA methods suffer larger drops than simple methods. This supports our hypothesis that characteristics of user-generated text negatively affect performances, 7 especially for SOTA methods which tend to penalize entities with diverse contexts. Future work may investigate how to overcome these challenges. To provide a qualitative comparison between the behaviors of SOTA methods (e.g., CGExpan) and our proposed baselines (e.g., LM-Base), we show their predictions on two representative concepts in Figure 5. CGExpan and LM-Base have comparable performance on well-formed concepts (e.g., company) in Jobs. However, LM-Base outperforms CGExpan for concepts (e.g., seating arrangement) with entities having characteristics of user-generated text. CGExpan retrieves entities that co-occur frequently with seating arrangement. Takeaway 5 Due to the presence of challenging characteristics in user-generated text, performance of all ESE methods are negatively impacted with SOTA methods exhibiting larger drops. Improvement Opportunities We analyze ensemble methods further since they tend to outperform other ESE methods (Figure 3). It is trivial that ensemble methods perform well when both combined methods are strong. We are more interested in other factors that may impact performance. Specifically, we investigate what influences the effectiveness of a MRR method that combines two ESE methods. An MRR combination is more effective when it outperforms both candidate methods by a larger margin. We define effectiveness of combining methods as: Ef f (m 1 , m 2 ) = S(m 1 + m 2 ) max(S(m 1 ), S(m 2 )) -1 (1) where S(m) means the performance (MAP@k g in our study) of method m, and m 1 + m 2 means the MRR combination of method m 1 , m 2 .",
      "The weights for an auxiliary loss L a,t for a ∈ Aux at training step t are calculated as follows: w a,t = σ ℓa,t µ ℓa,t ℓ a,t = L a,t µ L a,t-1 (3) where ℓ a,t is the loss ratio of loss a ∈ Aux at training step t, σ is the standard deviation over the history of loss ratios and µ ℓ a,t-1 is the mean of the loss ratio ℓ a up to and including step t -1. We also compare CoV to a simple sum of all losses i.e. equal weight for each loss, as shown in Algorithms 1, 2 and 3 (line beginning with L total ). Experimental Setup Datasets Three multilingual datasets are used to compare our methods with their most relevant counterparts. Multilingual Task-Oriented Parsing (MTOP) comprises 15K-22K utterances in each of 6 languages (en, de, fr, es, hi, th) spanning 11 domains (Li et al. , 2021b). The Multilingual Task-Oriented Dialogue (MTOD) consists of 43K English, 8K Spanish and 5K Thai utterances covering 3 domains (Schuster et al. , 2019). The Multilingual ATIS++ (M-ATIS) contains up to 4.5K commands in each of 8 languages (en, es, pt, de, fr, zh, ja, hi, tr) featuring user interactions with a travel information system (Xu et al. , 2020). XLM Our pretrained language model of choice is XLM-RoBERTa (Conneau et al. , 2020). We use the large (550M parameters) model from HuggingFace (Wolf et al., 2019) with a hidden_size = 1,024. Training Setup We use a minimalist setup that features default settings and components to focus the results on the methods rather than hyperparameter tuning or custom architecture design. We implemented all models with PyTorch using fixed hyperparameters between experiments except for MTOD, where due to its size, we trained with fewer epochs and a lower learning rate (both 50% lower2 ). Results Terminology Henceforth, we refer to models trained with labelled data in each language as Target Language, the models trained only on English data as Zero-Shot, our translate-intent method as Translate-Intent (TI), the scores reported by Gritta and Iacobacci (2021) as Previous SOTA, our IO-only implementation of that model as XeroAlign IO (XA IO ), our contrastive alignment method as Contrastive (CTR) and our main method as CrossAligner (CA). Lastly, the simple sum of alignment losses is referred to as 1+1 and the weighted sum from 3.4 as CoV."
    ],
    "ground_truth": "The proposed MST-AM method differs from Newton and standard quasi-Newton methods in several ways. Unlike the Newton method, which requires the computation of the Hessian and the solution of a linear system at each step, a process that can be prohibitive in large problems, the MST-AM method only needs to store two corrected historical iterations. This makes it more efficient for solving high-dimensional and large-scale problems. In comparison to limited-memory quasi-Newton methods, which require many historical iterates or gradients to form the secant equations, the MST-AM method uses short-term recurrences, making it less sensitive to the historical length. Furthermore, the MST-AM method has been shown to surpass BFGS in solving cubic-regularized problems and is comparable to full-memory methods in the forward root-finding process. It also converges faster in the backward process and offers comparable accuracy.",
    "paper_id": "openreview/ICLR-2022-conf/_X90SIKbHa",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:25:54.045813",
    "processed_idx": 0
  },
  {
    "question": "Are the annotators of the test sets native English speakers?",
    "answer": "**ANÁLISIS Y RESPUESTA:**  \nSí, los anotadores de los conjuntos de prueba incluyen hablantes nativos de inglés. Según el **Contexto 1**, de los nueve anotadores que participaron en la evaluación, **seis eran hablantes nativos de inglés**. Todos los anotadores tenían formación previa en anotación de AMR (Representación de Significado Abstracto) y eran estudiantes o profesores universitarios. Además, en el **Contexto 3**, el estudio piloto utilizó tres anotadores que también eran hablantes nativos de inglés y especialistas en AMR. Esto confirma que la selección de anotadores en este trabajo incluyó explícitamente a hablantes nativos, al menos en las fases piloto y principal de la evaluación. No se menciona si todos los anotadores de otros conjuntos de datos (como en el Contexto 5) eran nativos, pero para los conjuntos de prueba descritos en Contexto 1 y 3, la respuesta es afirmativa.",
    "contexts": [
      "However, some of the system output we obtained was generated from the earlier 2015 version of this data (LDC2015E86); these test sets contain the same sentences, with some updates to the AMRs, so we decided to only include AMRs at the intersection of these datasets in our evaluation. Additionally, we chose to exclude AMRs whose root relation was multisentence, which indicates that the portion of text officially segmented as one sentence includes what AMR annotators analyzed as two or more sentences. These were excluded because they are often very long and pilot annotators found they could be very difficult to read and evaluate, and because unlike other AMR relations, multisentence does not represent a semantic relationship between elements of meaning. A total of 335 sentences were excluded from consideration due to differences in their AMRs between the different versions of the data, and 71 for being multi-sentence. Accounting for overlap between the excluded sets, 998 out of 1371 total sentences in the test set were considered eligible for our evaluation. A random sample of 100 of these were used in the survey. Annotation: A total of nine annotators participated in this evaluation, including the three who participated in the pilot. All had prior training in AMR annotation, mostly from taking a semester-long course focused on AMR and other meaning representations. Six of the annotators were native English speakers; all were either current or former PhD students or professors at our university. Each person annotated two different batches of 10 sentences each, except for one annotator who did four batches. The result was that each set of sentences was double-annotated, allowing us to quantify inter-annotator agreement. Additionally, batches were assigned such that each annotator overlapped with at least two other annotators. Analysis Survey Reliability Pilot: The only previous human evaluation of several AMR-to-English generation systems was in the SemEval-2017 task discussed above. Since our survey had several differences from this previous evaluation, it was possible that the methodological differences could lead to substantial differences in judgments on the same data. Thus, before conducting the main survey, we validated our methodology by comparing the results of the pilot survey to that of the SemEval-2017 evaluation. This is the first evaluation of AMR generation to collect separate judgments for fluency and adequacy.",
      "Of the 53 sentences where the participants disagreed with the annotator, 45 were further revised in the third draft. The ratio is 0.849, much higher than the overall ratio of general Rev12 revisions being further revised in Rev23 (161/394 = 0.409) and the ratio of the agreed Rev12 revisions being revised in Rev23 (67/341 = 0.196). In further analysis, a Pearson correlation test is conducted to check the correlation between the number of Rev23 and the number of disagreements for different types of agreement/disagreements, controlling for the number of Rev12. We find a nega-9 Delete revisions were ignored as the deleted sentences are not traceable in Draft3 tive correlation between Rev23 and the number of cases (r=-0.41, p < .03) in which the revisions annotated as Content are verified by the participants; we also find a positive correlation between Rev23 and the number of cases (r=0.36, p <= .05) in which the revisions annotated as Surface are intended to be Content revisions by the participants. Both findings are consistent with H2, suggesting that participants will revise further if they perceive that their intended revisions were not recognized. Testing for H3 We observe that native and L2 speakers exhibit different behaviors. First, we tested the difference in Content23 and Surface23 10 between these speaker groups with ANOVA. We observe significant difference in the number of content (p < .02) and surface (p < .03) revisions made by L2 and native speakers. More specifically, our native speakers make more Content changes while the L2 speakers make more Surface changes.  Second, with ANOVA we found a significant interaction effect of the two factors (Group and users' L2 or native status) (p < .021) on their ratings for the statement \"the system helps me to recognize the weakness of my essay\" with L2 speakers having a stronger Interface A preference.  Third, we observe a significant positive correlation in the native group between the number of content revisions in Rev23 and the ratings of the statement \"the system encourages me to make more revisions than I usually make\" (ρ=.4 and p < .009). This suggests that giving feedback (from either interface) encourages native speakers to make more content revisions.",
      "The reference is also included as a sentence to judge, but is not distinguished from the system outputs. Pilot Evaluation Before collecting the full dataset of human judgments for AMR generation, we completed a smaller pilot experiment to test the validity and practicality of the methodology. This pilot used the data and systems included in the SemEval-2017 shared task (May and Priyadarshi, 2017). A random subset of 25 out of the 1293 sentences in the dataset were used. All were annotated by three annotators, each of whom was a native speaker of English and a linguist with experience with AMR. We tweaked the design of the later survey based on feedback from the pilot annotators. In particular, the surveys were shortened (annotators completed two batches of 10 sentences each, instead of one with 25); more thorough instructions were given, with examples; and wording was changed from \"sentence\" to \"utterance\" to reflect that some are not full sentences in a grammatical sense. Main Evaluation The main evaluation was larger than the pilot, and evaluated more recent systems, most of which are of a markedly higher quality than those in the pilot. We contacted the authors of several recent papers on AMR-to-English generation to obtain their system's output for use in the evaluation, and included all systems for which we obtained usable data in time to begin evaluation: Konstas et al. (2017), Guo et al. (2019), Manning (2019), Ribeiro et al. (2019), and Zhu et al. (2019). These systems are all described in §2.2. The Konstas and Zhu systems both approach AMR generation as a sequence-to-sequence task: Konstas is pretrained on 'silver' data from a jointly-trained AMR parser to mitigate the limitations of the small amount of gold AMR data, while Zhu uses a transformer-based model. Guo and Ribeiro both use graph-tosequence models: Guo uses densely-connected graph convolutional networks to model the graph structure, while Ribeiro uses a dual representation to capture both bottom-up and top-down perspectives on the graph. Finally, Manning is the only non-neural system in this evaluation; it uses handwritten rules to generate possible realizations, augmented with simple statistical modules to choose between candidates. Data: We use the test set from the LDC2017T10 AMR dataset (Knight et al. , 2017).",
      "The Argrewrite Corpus Based on the above design decisions, we have developed a corpus of argumentative essays with three drafts and detailed annotations for sentencealigned revisions between each consecutive pair of drafts. The main corpus has five elements, with the relationships between them shown in Figure 1; Section 3.1 describes the procedure for obtaining them. Section 3.2 briefly describes the revision schema we used and reports the inter-annotator agreement. Additionally, we have collected metadata from the participants who contributed to the corpus (discussed in Section 3.3); these data may be useful for user behavior analysis. Corpus Development Procedure We have recruited 60 participants aged 18 years and older, among whom 40 were English native speakers and 20 were non-native speakers with sufficient English proficiency. 4 The study to collect the corpus is carried out in three 40-60 minute sessions over the duration of two weeks. Draft1 Each participant begins by completing a pre-study questionnaire (Section 3.3) and writing a short essay online. Participants are instructed to keep the essay around 400 words, making a single main point with two supporting examples. They are given the following prompt: \"Suppose you've been asked to contribute a short op-ed piece for The New York Times. Argue whether the proliferation of electronic 4 i.e. , with a TOEFL score higher than 100. communications (e.g., email, text or other social media) enriches or hinders the development of interpersonal relationships. \" Draft2 A few days later, participants are asked to revise their first draft online based on the following feedback: Strengthen the essay by adding one more example or reasoning for the claim; then add a rebuttal to an opposing idea; keep the essay at 400 words. With this feedback we try to push participants to make revisions for later processing by the two interfaces used to create Draft3. Annotated Revisions I (Rev12) The two drafts are semi-manually aligned at the sentence level. 5Then, the purpose of each pair of sentence revision is manually coded by a trained annotator, following the annotation guideline (see Section 3.2). Draft3 Participants write their third draft in a lab environment. This time, they are not given additional instructional feedback. Instead, participants are shown a computer interface that highlights the differences between their first and second drafts. They are asked to revise and create a third draft to improve the general quality of their essay.",
      "We focused on pairs of country and language where most of the country's population speaks that language, and most of the L1 speakers of the language reside in that country. For instance, 78.1% of US residents speak English at home, and 76.9% of L1 English speakers reside in the US. 3 Figure 1 shows that for 3 out of the 4 countries, the majority of workers indicated they were native speakers of the majority language. The exception is India, which has many languages. Hindi is the most spoken language in India (followed by Bengali: 8% and Telugu: 6.7%) and has the larger Wikipedia corpus and a BERT model. Among the workers from India, 16% indicated they were Hindi speakers. While the gold standard annotations are limited to 4 languages, the framework we describe in Section 3 is unsupervised and almost entirely language-agnostic. As we discuss in Section 4.3, we applied the model to additional 23 languages, selected based on the availability of a Wikipedia corpus and an LM for that language.4 Annotation Task Figure 3 displays the HIT. We asked workers to identify their native language, and posed them the following questions regarding each time expression (e.g. noon). 1. If the native language is not English: What is the equivalent word for noon in your native language? We allowed workers to check \"There is no equivalent expression in my language\".  2. What is the range of time you consider as noon? Workers were required to indicate the start and end times. We then allowed workers to add any time expression in their native language that wasn't mentioned in the HIT, as well as free text comments. To ensure the quality of annotations, we required that workers had a 95% approval rate for at least 100 prior HITs. We paid 0.3 USD per HIT. Observations Figure 5 displays the average start and end time for each country and each time expression. Notably, morning is quite consistent across the different countries and noon is the short period around 12 pm. The variation is higher for afternoon and evening. Many workers from Brazil noted that Portuguese uses the same word for evening and night (noite), and that evening turns quickly into night What is your native language? [Select language...] What is the equivalent word for morning in your native language? What is the range of time you consider as morning? -:-to"
    ],
    "ground_truth": "Yes, the annotators of the test sets included native English speakers. In the pilot phase, all three annotators were native English speakers. In the main evaluation, six out of the nine annotators were native English speakers. All annotators were either current or former PhD students or professors at the university and had prior training in AMR annotation.",
    "paper_id": "nlpeer/COLING2020/679",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:26:20.258405",
    "processed_idx": 1
  },
  {
    "question": "Does the cross-linguistic analysis include French and Spanish?",
    "answer": "Sí, el análisis cross-linguístico (interlingüístico) incluye tanto **francés como español**. Esta información está respaldada por los siguientes contextos:\n\n1. **Contexto 1**:  \n   El documento menciona explícitamente que los autores evaluaron el rendimiento de su modelo en tres conjuntos de datos en inglés, uno en francés y otro en español. Además, destacan que lograron una precisión de clasificación cruzada (zero-shot) superior al 80% para ambos idiomas (francés y español) al entrenar el modelo con datos en inglés. Esto confirma que ambos idiomas son parte del análisis cross-lingüístico.\n\n2. **Contexto 3**:  \n   En la sección sobre el escenario *zero-shot cross-lingual*, se especifica que los modelos se entrenaron en el conjunto de datos **Newsela-En** (inglés) y se evaluaron en **Newsela-Es** (español) y **Vikidia-Fr** (francés). Se compara el rendimiento entre estos idiomas, destacando que el modelo de ranking par a par (NPRM) supera al modelo de regresión en ambas evaluaciones. Aquí se detalla cómo se miden las diferencias en el desempeño entre francés y español en contextos cross-lingüísticos.\n\n3. **Contexto 2**:  \n   Aunque no se enfoca en análisis cross-lingüístico, menciona errores en sistemas de alineación multilingüe para idiomas como francés y español, lo que sugiere que ambos idiomas son relevantes en el ámbito de los estudios multilingües. Sin embargo, su relevancia directa para el análisis cross-lingüístico es menor en comparación con los contextos 1 y 3.\n\n**Conclusión**:  \nTanto el francés como el español son incluidos en el análisis cross-lingüístico, especialmente en el contexto de evaluación de modelos de clasificación de legibilidad y alineación multilingüe. Los resultados específicos para estos idiomas se destacan en los contextos 1 y 3, donde se demuestra su viabilidad en escenarios *zero-shot* y se comparan sus desempeños.",
    "contexts": [
      "A Neural Pairwise Ranking Model For Readability Assessment Abstract Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our model by conducting experiments with three English, one French and one Spanish datasets. We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data. Additionally, we also release a new parallel bilingual readability dataset in English and French. To our knowledge, this paper proposes the first neural pairwise ranking model for ARA, and shows the first results of cross-lingual, zeroshot evaluation of ARA with neural models. Introduction Automatic Readability Assessment is the task of assigning a reading level for a given text. It is useful in many applications from selecting age appropriate texts in classrooms (Sheehan et al. , 2014) to assessment of patient education materials (Sare et al. , 2020) and clinical informed consent forms (Perni et al. , 2019). Contemporary NLP approaches treat it primarily as a classification problem which makes it non-transferable to situations where the reading level scale is different from the model. Applying learning to rank methods has been seen as a potential solution to this problem in the past. Ranking texts by readability is also useful in a range of application scenarios, from ranking search results based on readability (Kim et al. , 2012;Fourney et al. , 2018) to controlling the reading level of machine translation output (Agrawal and Carpuat, 2019;Marchisio et al. , 2019). However, exploration of ranking methods has not been a prominent direction for ARA research. Further, recent developments in neural ranking approaches haven't been explored for this task yet, to our knowledge. ARA typically relies on the presence of large amounts of data labeled by reading level. Further, although linguistic features are common in ARA research, it is challenging to calculate them for several languages, due to lack of available software support. Though there is a lot of recent interest in neural network based cross-lingual transfer learning approaches for various NLP tasks, there hasn't been much research in this direction for ARA yet.",
      "Readers interested in language-specific analysis (including many more examples) are encouraged to read Appendix A.1. We focused on errors committed by CrossAligner and XeroAlign IO , which achieved the best individual and combined scores. We sampled 100 random errors from each of the following settings: Hindi, French and German from MTOP, Portuguese, Chinese and Spanish from M-ATIS and Thai from MTOD for a diverse pool of errors. The authors adjudicated with native speakers to categorise mistakes into the following types. Error Types We discovered two main sources of mistakes: A boundary error occurs when the model predicts more or fewer entity words/tokens than given in the gold annotation. A semantics error occurs when the wrong entity class/type is predicted. Models can therefore commit: 1) both errors resulting in Poor Transfer, 2) a boundary error without a semantic error and vice versa giving us a Partial Transfer or 3) neither error (a false negative), which we deemed an Acceptable Transfer. We report individual and average error occurrences as well as transfer type percentages in Table 3. Poor Transfer indicates that the prediction error is too serious and unusable (even misleading) in a real-world personal assistant. This is typically due to both a boundary and a semantics error, however, some mistakes can be serious enough alone to result in poor transfer. For example, a boundary error can cause the retrieved name of a dish, person or a location to be incomplete and therefore invalid. A semantics error that classifies '10 secondes' (French) as 'date_time' instead of 'mu-sic_rewind_time' would elicit the wrong agent response thus is unusable. On average, ∼28% of mistakes fall into the 'Poor Transfer' category. Partial Transfer is defined as either a boundary or a semantics error where neither is considered a serious problem. Such entities could be made usable in a personal assistant application with simple post-processing rules. Around 33 percent of errors were deemed to be partially correct. Often, this was due to including some adjacent punctuation or an article/preposition as part of the entity or a slightly shorter/longer news headline even though a search engine query with that string would have returned the relevant article. Entities such as '24 minat ka' (Hindi) versus '24 minat' (24 minutes) exemplify the fact that a disputed entity boundary is the most frequent source of error in this category.",
      "While the results for N P RM are still somewhat lower in the cross-corpus evaluation than in within corpus evaluation setups, it has to be noted that this evaluation is done without any additional fine-tuning on the target datasets. We did not test for statistical significance in this case as the numbers have large differences between regBERT and N P RM in most cases. This experiment leads us to a conclusion that N P RM can successfully be used to rank documents on a different reading level scale too. Zero Shot, Cross-Lingual Pair-Wise Ranking Zero-shot cross-lingual scenario aims to evaluate whether a model trained on one language can be ef-fectively used to rank texts from another language correctly without explicitly training on the target language. We evaluated NPRM and regBERT models trained with a multilingual BERT (mBERT) model as the base for this task. Both the models were trained on Newsela-En dataset and evaluated on Newsela-Es and Vikidia-Fr datasets. The mBERT7 model is pre-trained on a corpus of multilingual data from 104 languages, including all the three languages in our experiment: English, French and Spanish. We observe that the N P RM with mBERT either performs comparably or outperforms a regression mBERT model on all metrics, for both the datasets. Specifically, the N P RM has a performance increase of 12.3% in RA for Newsela-Es over Vikidia-Fr. Thus, we can conclude that our pairwise ranking approach performs well even in cross-lingual scenarios, and zero-shot, crosslingual transfer can be useful to setup strong baseline models for new languages. We can notice a lower performance on Vikidia-Fr compared to Newsela-ES. Apart from the fact that they are different languages, it can potentially also be due to the fact that Newsela-ES has content from the same domain as Newsela-EN, whereas Vikidia-Fr has more diverse content. It is also possible that the ranking metrics penalize Vikidia-Fr predictions more, as there are only two reading levels. A ranking can still be scored well if most of the ranking order is correct. However, in the case of Vikidia-Fr, an incorrect ranking levels would result in a completely reversed list, which is heavily penalized in SRC and KTCC. Thus, small number of completely incorrectly ranked slugs can result in low SRC and KTCC scores for Vikidia-Fr, but can still result in high SRC and KTCC scores for NewsEla-ES.",
      "This leads to several studies investigating the source of the cross-lingual capability of multilingual models. An early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages (Wu and Dredze, 2019;Pires et al. , 2019), which provides cross-lingual alignment signals to learn useful multilingual representations. However, this hypothesis has been questioned by recent studies (Karthikeyan K and Roth, 2020;Conneau et al. , 2020b) which show that shared word-pieces only play a minor role in the performance. These studies suggest that the model can exploit abstract structures of languages to learn shared multilingual representations. Another line of research suggests that the learning of transferable knowledge happens even in monolingual pretraining. Artetxe et al. (2020) showed that a Transformer encoder pretrained only on L1 exhibits strong cross-lingual transfer performance simply by aligning the L2 embeddings to the encoder. Papadimitriou and Jurafsky (2020) pretrained LSTM encoders with natural languages and non-linguistic data (e.g., code, music, and artificial data) to demonstrate that the encoders achieve reasonable performance in Spanish language modeling. These studies provide additional evidence for the existence of transferable linguistic knowledge learned in the model. Then what is such knowledge? Probing studies (Hupkes and Zuidema, 2018;Conneau et al. , 2018) have revealed that the model captures languageindependent structures such as universal dependency relations (Chi et al. , 2020) and subjecthood (Papadimitriou et al. , 2021). However, the probing methodology does not answer whether such linguistic knowledge contributes to the performance in cross-lingual transfer. In this study, we shed light on this question by studying transfer learning from artificial language with the Test for Inductive Bias via Language Model Transfer (TILT) (Papadimitriou and Jurafsky, 2020). This framework enables us to assess if abstract features generalizable to L2 (natural language) are encoded in L1. Here we explicitly design artificial languages with some structural properties as L1 to investigate their transferability. Studying Language Models With Artificial Language To study the behavior of language models, several studies have employed a specific type of artificial language: artificial variants of natural languages. A typical experimental framework is as follows: (1) create an artificial language that differs from a natural language in one linguistic property, such as word orders (Sinha et al. , 2021b;Dufter and Schütze, 2020;Sinha et al. , 2021a) , scripts (Karthikeyan K and Roth, 2020;Dufter and Schütze, 2020;Conneau et al. , 2020b) , or morphology (Ravfogel et al. , 2019)",
      "For example, there are considerable morphosyntactic variations between Spanish spoken in Spain and Argentina (Bentivoglio and Sedano, 2011), but they are not considered separately in a Spanish PLM (Cañete et al. , 2020). A PLM specific to the Algerian di-alect of Arabic performs better than a multilingual or general Arabic one (Antoun et al. , 2020) in sentiment classification (Abdaoui et al. , 2021). Stylistic variation. Factors such as directness and formality are often associated with different communicative styles across cultures. For example, the level of politeness and thus also how offensive something is perceived depends very much on communicative norms (Gao, 2005;Larina, 2015). Misunderstandings that arise from different communicative styles can occur in any interaction between people of different cultural backgrounds. Thomas (1983) refers to this as pragmatic failure, namely the \"inability to understand 'what is meant by what is said\"', due to how it is being said. Comparative stylistics (Vinay and Darbelnet, 1995) aims to characterise these differences. The same intention (e.g. , to be polite) can lead to different forms in different cultures. For example, an offer of help can be made in an imperative form in Polish, while in English speaking Anglo-Saxon cultures, this form would be considered rude and so an interrogative form would be used instead (Wierzbicka, 2009). Similarly, German native speakers tend to use a high level of directness, which would be considered offensive in English (House and Kasper, 2011). Based on the observation that such differences exist, Ringel et al. (2019) applied distant supervision by inducing labels for English formality and sarcasm detection based on language-in this case German and Japanese, respectively. The expression and perception of emotion also varies across cultures, both in text (Kirmayer et al., 2001;Ryder et al., 2008) and in face-to-face communication (Hareli et al. , 2015). These differences are critical for cross-cultural sentiment analysis (Bautin et al. , 2008) and for text-based recognition of medical conditions such as depression. For example, Loveys et al. (2018) find clear differences in linguistic form across English speakers of different ethnic identities with self-reported depression (e.g., the ratio of positive to negative emotion expression). Not understanding cross-cultural variation could lead to misclassification of movies, or worse, mis-diagnosing people. Common Ground A culture is in part defined by a shared common ground: the shared body of knowledge that can be talked about and that can be assumed as known by others."
    ],
    "ground_truth": "Yes, the cross-linguistic analysis does include French and Spanish. The approach demonstrated in the study achieved a zero-shot cross-lingual ranking accuracy of over 80% for both these languages when trained on English data. The models used for this task were trained on the Newsela-En dataset and evaluated on Newsela-Es and Vikidia-Fr datasets.",
    "paper_id": "nlpeer/ARR-22/ab82036287f47b63ae70bdae7c1ed5900395bfe4538cc14db1a17281e2b45434b348472df2024151e188fe3890b88a0990bf1a36540eb39ddcc015012567bdd8",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:26:39.118889",
    "processed_idx": 2
  },
  {
    "question": "Why were classification-based models not used for the zero shot experiments in Tables 6 and 7?",
    "answer": "La ausencia de modelos basados en clasificación en los experimentos zero-shot de las Tablas 6 y 7 (o Tabla 7 y otros, según los contextos disponibles) se debe probablemente a la naturaleza de las tareas evaluadas y a las estrategias metodológicas adoptadas. Analizando los contextos proporcionados:\n\n1. **Tareas específicas no clasificación**:  \n   En los experimentos mencionados (como en los contextos 1 y 2), las tareas zero-shot incluyen **paráfrasis, análisis de",
    "contexts": [
      "ZERO-SHOT: The second technique fine-tunes large-scale multilingual language models on a English style transfer dataset, and applies it zero-shot on multilingual data during inference. MAD-X: Introduced by Pfeiffer et al. (2020b), this technique is similar to ZERO-SHOT but additionally uses language-specific parameters (\"adapters\") during inference. These language-specific adapters have been originally trained using masked language modeling on the desired language data. Dataset for evaluating classifiers: We conduct our experiments on Hindi formality classification, leveraging our evaluation datasets from Section 5.1. We removed pairs which did not have full agreement across the three annotators and those pairs which had the consensus rating of \"Equal\" formality. This filtering process leaves us with 316 pairs in Hindi (out of 1000). In our experiments, we check whether the classifiers give a higher score to the more formal sentence in the pair. We leverage the multilingual classifiers open-sourced 18 by Krishna et al. (2020). These models have been trained on the English GYAFC formality classification dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al. , 2021b) for formality classification in Italian, French and Brazilian Portuguese. 13 These classifiers were trained on preprocessed data which had trailing punctuation stripped and English sentences lower-cased, encouraging the models to focus on lexical and syntactic choices. As base multilingual language models, we use (1) mBERT-base from Devlin et al. (2019); (2) XLM-RoBERTabase from Conneau et al. (2020). Results: Our results on Hindi are presented in Ta-18 https://github.com/ martiansideofthemoon/ style-transfer-paraphrase/blob/master/ README-multilingual.md ble 7 and other languages in Table 8. Consistent with Pfeiffer et al. (2020b) , we find MAD-X to be a superior zero-shot cross lingual transfer method compared to baselines. We also find XLM-R has better multilingual representations than mBERT. Unfortunately, AdapterHub (Pfeiffer et al. , 2020a) has XLM-R language adapters available only for Hindi & Tamil (among Indic languages). For other languages we use the ZERO-SHOT technique on XLM-R, consistent with the recommendations 13 provided by Krishna et al. (2020) based on their experiments on XFORMAL (Briakou et al., 2021b). Model Accuracy (↑) TRANSLATE TRAIN mBERT 66% ZERO-SHOT mBERT 72% XLM-R 76% MAD-X XLM-R 81% We considered three models for evaluating semantic similarity between the input and output: (1) LaBSE (Feng et al. , 2020)",
      "All models show similar trends on both tasks with improved performances compared to the original settings without document age as input. It is an obvious result considering that the first step for ancient manuscript is assuming the written era. It implies the significance of chronological attribution task in HUE, conveying that chronological attribution task might improve the performance of other HUE tasks. Zero-Shot Experiment Countless number of Hanja documents still remain without any analysis and new documents continue to be unearthed. Therefore, we run zero-shot experiments to verify the effectiveness of our language models on extracting information from the historical documents irrelevant to the training corpora. We use DRRI dataset which is not included in both pretraining and fine-tuning data of our Hanja language models and execute CA and NER. Table 7 shows experimental results with CA and NER on DRRI. All models perform comparably well on the both tasks, regarding that random model will achieve approximately 3.70% performances with 27 classes in CA. Also, all models in CA commonly show high precision which might be due to the monotonous and redundant phrases in the veritable records. It shows similar trends compared to the Table 4, but the gap among models was notably emphasized in the zero-shot settings. This results imply that our CA models might be exploited for the time period prediction of unseen documents in anthology with a reliable level. Our models outperform others on NER achieving absolutely high performances, though entity maps between AJD and DRRI do not match strictly. Interestingly, all models tend to predict location entities better than person entities which is the opposite result compared to the original NER on AJD. It is probably due to the characteristics of each entity, where location entities are all commonly used in nationwide while person entity might differ by situation. Further analysis on person and location entities in the view of time changes is described in Section 6.2. We present that our models trained on the corpora of the Joseon dynasty provide reliable results on unseen records, implying that our model can be exploited for the low-resourced documents. 6 Further Analysis Do Historical Events Affect Language Models? To figure out the effect of historical events on models' prediction, we analyzed the output of language models on CA.",
      "Zero-Shot Paraphrase Ranking We validate our approach and study the effects of various ablations on zero-shot paraphrase ranking. The first half of QUIP training objective is more closely aligned with learning better representations than MLM. The rest of Table 2 shows zero-shot paraphrase ranking results using BERTScore. QUIP improves substantially over RoBERTa on all four datasets, with an average improvement of .076 AUROC. The improvement is greatest on the PAWS datasets; since these datasets cannot be solved by lexical features alone, QUIP representations must be much more contextualized than RoBERTa representations. Training on Unsupervised QA data degrades performance compared to RoBERTa, showing that QUIP does not merely make word representations encode local context in a simple way. Training the bi-encoder directly on the MRQA dataset or without the teacher improves on average over RoBERTa, but QUIP greatly outperforms both baselines. The cross-encoder models also lag behind QUIP at paraphrase ranking, despite their higher QA accuracy; since the cross-encoders are trained to take passagequestion pairs as inputs, their representations of single sentences are not as useful. Thus, we conclude that having real questions, accurate answer supervision, and a bi-encoder student model are all crucial to the success of QUIP. Paraphrase Classification Table 3 shows few-shot paraphrase classification results. As we studied QUIP-related ablations in the previous section, we focus on the comparison between QUIP and baselines based on MLM. First, we use RoBERTa-large embeddings in place of QUIP in our method. Second, we compare with LM-BFF (Gao et al. , 2021) , which pairs RoBERTalarge with MLM-style prompts. We use LM-BFF with manually written prompts and demonstrations, which was their best method on QQP by 2.1 F1 and was 0.3 F1 worse than their best method on MRPC. QUIP used as a frozen encoder is competitive with LM-BFF on QQP and outperforms it by 6.1 F1 on MRPC, 11.2 F1 on PAWS-Wiki, and 12.1 F1 on PAWS-QQP. Fine-tuning QUIP gives additional improvements on three of the four datasets, and outperforms fine-tuning RoBERTa by an average of 6.9 F1. Named Entity Recognition Table 4 shows few-shot NER results on the CoNLL and WNUT datasets. QUIP improves over RoBERTa-large by 11 F1 on CoNLL and 2.9 F1 on WNUT when used with a randomly initialized output layer. We see a further improvement of 4 F1 on CoNLL and 7.4 F1 on WNUT when using question embeddings to initialize the output layer.",
      "as it must perform a separate forward pass for each question (i.e., each entity type in this setting). Zero-Shot Sentiment Analysis Finally, we use QUIP for zero-shot binary sentiment analysis. We reduce sentiment analysis to QA by writing a pair of questions that ask for a reason why an item is good or bad (e.g. , \"Why is this movie [good/bad]?\"). We predict the label whose corresponding question has higher similarity with the QUIP representation of some token in the input. This prompting strategy has the additional benefit of extracting rationales, namely the span that the QUIP model predicts as the answer to the question. While we focus on sentiment analysis, extractive rationales have been used for a wide range of NLP tasks (DeYoung et al. , 2020), suggesting that this method could be applied more broadly. More formally, let x be an input sentence and (q 0 , q 1 ) be a pair of questions (i.e., a prompt). For label y ∈ {0, 1}, we compute a score for y as S(x, y) = max i r(x) ⊤ i f start (q y )+ max i r(x) ⊤ i f end (q y ). (4 ) This formula is a straightforward way to measure the extent to which some span in x looks like the answer to the question q y , based on the model's pre-trained ability to perform QA. We predict whichever y has the higher value of S(x, y) -C y , where C y is a calibration constant that offsets the model's bias towards answering q 0 or q 1 . Our inclusion of C y is inspired by Zhao et al. (2021), who recommend calibrating zero-shot and few-shot models with a baseline derived from content-free inputs to account for biases towards a particular label. To choose C y , we obtain a list W of the ten most frequent English words, all of which convey no sentiment, and define C y as the mean over w ∈ W of S(w, y), i.e. , the score when using w as the input sentence (see Appendix A.4). This method can succeed only if the model produces a lower score for unanswerable questions than answerable ones.",
      "Crossaligner & Co: Zero-Shot Transfer Methods For Task-Oriented Cross-Lingual Natural Language Understanding Abstract Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the lowresource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated. Introduction Natural language understanding (NLU) refers to the ability of a system to 'comprehend' the meaning (semantics) and the structure (syntax) of human language (Wang et al. , 2019) to enable the interaction with a system or device. Cross-lingual natural language understanding (XNLU) alludes to a system that is able to handle multiple languages simultaneously (Artetxe and Schwenk, 2019;Hu et al. , 2020). We focus on task-oriented XNLU that comprises two correlated objectives: i) Intent Classification, which identifies the type of user command, e.g. 'edit_reminder', 'send_message' or 'play_music' and ii) Entity/Slot Recognition, which identifies relevant entities in the utterance including their types such as dates, messages, music tracks, locations, etc. In a modular dialogue system, this information is used by the dialogue manager to decide how to respond to the user (Casanueva et al. , 2017;Gritta et al. , 2021). For neural XNLU systems, the limited availability of annotated data is a significant barrier to scaling dialogue systems to more users (Razumovskaia et al. , 2021). Therefore, we can use cross-lingual methods to zero-shot transfer the knowledge learnt in a high-resource language such as English to the target language of choice (Artetxe et al. , 2020;Siddhant et al., 2020). To this end, we introduce a variety of alignment methods for zero-shot cross-lingual transfer, most notably CrossAligner."
    ],
    "ground_truth": "The authors did not utilize classification-based models for the zero shot experiments in Tables 6 and 7 because, as indicated in the comparison in Table 5, classification did not perform better than regression and ranking. Therefore, for subsequent cross-corpus and cross-lingual zero shot comparisons, the authors decided to focus on regBERT and NPRM models, expecting that performance would slightly degrade in a cross-corpus setting.",
    "paper_id": "nlpeer/ARR-22/ab82036287f47b63ae70bdae7c1ed5900395bfe4538cc14db1a17281e2b45434b348472df2024151e188fe3890b88a0990bf1a36540eb39ddcc015012567bdd8",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:27:20.509306",
    "processed_idx": 3
  },
  {
    "question": "What is the reason for the lower performance on Vikidia-FR compared to Newsela-ES?",
    "answer": "La menor rendimiento en Vikidia-Fr comparado con Newsela-Es se debe a tres factores principales, según los contextos proporcionados:\n\n1. **Diferencias de dominio y contenido**:  \n   - **Newsela-Es** comparte el mismo dominio temático que **Newsela-En** (el conjunto de datos de entrenamiento), ya que ambos son artículos de noticias reescritos en diferentes niveles de lectura. Esto facilita la adaptación del modelo entrenado en Newsela-En a Newsela-Es.  \n   - **Vikidia-Fr**, en cambio, proviene de un dominio completamente diferente: una enciclopedia para niños (Vikidia.org), con contenido más diverso y no centrado en noticias. Esta divergencia en el tipo de texto dificulta la generalización del modelo entrenado en Newsela-En, reduciendo su efectividad.\n\n2. **Estructura de niveles de lectura**:  \n   - **Newsela-Es** tiene **10 niveles de lectura**, mientras que **Vikidia-Fr** solo tiene **2 niveles**. Esto influye en la forma en que se penalizan los errores de clasificación en métricas como **SRC** (Coefficient de concordancia de Kendall) y **KTCC** (Kendall Tau Correlation Coefficient).  \n   - En Vikidia-Fr, un error en el orden de clasificación (por ejemplo, invertir los dos niveles) resulta en una lista completamente invertida, lo que se penaliza de forma más severa en estas métricas. En contraste, en Newsela-Es, con 10 niveles, los errores parciales tienen un impacto menor en las métricas.\n\n3. **Pérdida de compatibilidad lingüística y cultural**:  \n   - Aunque ambas evaluaciones son cross-linguales (modelo entrenado en inglés), **Newsela-Es** (español) está más estrechamente relacionado con el dominio de noticias y estructura de niveles de Newsela-En, lo que facilita la transferencia",
    "contexts": [
      "While the results for N P RM are still somewhat lower in the cross-corpus evaluation than in within corpus evaluation setups, it has to be noted that this evaluation is done without any additional fine-tuning on the target datasets. We did not test for statistical significance in this case as the numbers have large differences between regBERT and N P RM in most cases. This experiment leads us to a conclusion that N P RM can successfully be used to rank documents on a different reading level scale too. Zero Shot, Cross-Lingual Pair-Wise Ranking Zero-shot cross-lingual scenario aims to evaluate whether a model trained on one language can be ef-fectively used to rank texts from another language correctly without explicitly training on the target language. We evaluated NPRM and regBERT models trained with a multilingual BERT (mBERT) model as the base for this task. Both the models were trained on Newsela-En dataset and evaluated on Newsela-Es and Vikidia-Fr datasets. The mBERT7 model is pre-trained on a corpus of multilingual data from 104 languages, including all the three languages in our experiment: English, French and Spanish. We observe that the N P RM with mBERT either performs comparably or outperforms a regression mBERT model on all metrics, for both the datasets. Specifically, the N P RM has a performance increase of 12.3% in RA for Newsela-Es over Vikidia-Fr. Thus, we can conclude that our pairwise ranking approach performs well even in cross-lingual scenarios, and zero-shot, crosslingual transfer can be useful to setup strong baseline models for new languages. We can notice a lower performance on Vikidia-Fr compared to Newsela-ES. Apart from the fact that they are different languages, it can potentially also be due to the fact that Newsela-ES has content from the same domain as Newsela-EN, whereas Vikidia-Fr has more diverse content. It is also possible that the ranking metrics penalize Vikidia-Fr predictions more, as there are only two reading levels. A ranking can still be scored well if most of the ranking order is correct. However, in the case of Vikidia-Fr, an incorrect ranking levels would result in a completely reversed list, which is heavily penalized in SRC and KTCC. Thus, small number of completely incorrectly ranked slugs can result in low SRC and KTCC scores for Vikidia-Fr, but can still result in high SRC and KTCC scores for NewsEla-ES.",
      "More future experiments, with additional languages, would lead us towards a better understanding of what works well across languages and datasets. Ranking Metrics : We reported four ranking metrics in these experiments.  While they all get high numbers in some experimental settings, none of them consistently seem like a better choice than others. We observe that the large majority of the methods score close to 1.0 on NDCG. In comparison, the SRC and KTCC, while generally quite high, appear more susceptible to poor ranking performance. We notice that RA is lower than SRC and KTCC for OSE (Table 6) and NewsEla-Es (Table 6), but SRC and KTCC lag behind RA for Vikidia-Fr (Table 7). We hypothesize that this could be because of the number of reading levels in the datasets. SRC and KTCC seem more forgiving when number of reading levels are more. Clearly, each metric addresses the evaluation of ranking differently, and as the results show, there is no single model that consistently does well across all metrics, in all the evaluations. We hope that this illustrates the value of reporting multiple metrics while benchmarking a new model. Future work in this direction should also focus on the evaluation of the evaluation metrics themselves for this task. Conclusions And Discussion In this paper we proposed a neural pairwise ranking model for ARA (N P RM ). We performed within corpus, cross-corpus and cross-lingual evaluations to benchmark N P RM . Our results in the context of the research questions we started with (Section 1) are discussed below: 1. Is neural, pairwise ranking a better approach than classification or regression for ARA, to achieve cross-corpus compatibility? -While regression, classification, and pairwiseranking models all achieve comparable performance in a within-corpus scenario, pairwise ranking performs better in cross-corpus and cross-lingual evaluation scenarios. 2. Is zero-shot, cross-lingual transfer possible for ARA models through such a ranking approach? -Our experiments show that zeroshot cross-lingual ARA is possible with pairwise ranking. Our proposed model, N P RM , trained with English texts achieved > 80% ranking accuracy on both NewsEla-Es and Vikidia-Fr datasets. Limitations of NPRM: N P RM models the relative reading difficulty level between texts. While this approach has performed well for our generalizability experiments, there is a general lack of interpretability with N P RM .",
      "All the ranking metrics in general seem to have higher scores with NewsEla-En trained models, than OSE models. This could potentially be due to the larger dataset size, as well as the fact that NewsEla-En covers a broader reading level scale. Although the classification models seem to generally perform well on ranking metrics too, it has to be noted that there is no inherent means within classification to distinguish between ties, where the model predicts the same class for two documents of different reading levels. Hence, it is not feasible to continue to use classifiers as rankers. This evaluation is to be seen only means of comparing classification, regression, and ranking with a common set of metrics. Regression Table 3 shows the performance of all the regression models using both regression and ranking metrics. Although there are no other reported results of applying regression models on these datasets to our knowledge, the low MAE/MSE for both datasets indicate that regression models perform well for this problem. Like with classification, we notice that there is no huge difference among the contextual and non-contextual embeddings in terms of the ranking metrics. However, we notice some general differences between classification and regression approaches. In contrast to the classification models, when holding the training data and the regressor constant, models with GloVe embeddings perform worse than models using Word2vec or fasttext in regression specific metrics. When evaluating on ranking metrics, the regression models generally exhibit higher average NDCG, SRC and KTCC than the classification models. Again, like with classification evaluation, the differences across models in terms of the ranking metrics is larger for OSE compared to NewsEla-En. Overall, though, the neural regressor (regBERT) consistently performs better than the OLS regressor in terms of regression metrics, and is either comparable or better than OLS regressor in terms of all the ranking metrics. Pairwise Ranking Table 4 shows the performance of pairwise ranking approaches on both the training datasets. When training on the NewsEla-En dataset, we observe that N P RM outperforms at least one word-embedding + SVMRank combination in the ranking metrics, but only achieves the top score in NDCG when compared with word-embedding SVMRank methods. When training on the OSE dataset, N P RM achieves the top score against the word-embedding + SVMRank combinations, but only NDCG was found to be statistically significant across all models.",
      "Comparisons between N P RM and the word-embedding + SVMRank combinations had p-values < 0.05 for NDCG. For SRR, KTCC and RA, only the difference in scores between N P RM and fasttext + SVMRank were found to be statistically significant. GloVe + SVM-Rank method produces the statistically equivalent scores in SRC, KTCC, and RA as N P RM . Overall, while there is no single approach that ranked as the best uniformly across all the three model settings (Tables 234), BERT based models perform competitively with most of the ranking metrics. Table 5  For Newsela-En, all methods reported a high score of 0.999 for NDCG and regBERT is better with the other metrics. Testing for statistical significance between N P RM , regBERT and BERT classification showed that N P RM is significantly better than BERT classifier (p<0.05) and there is no significant difference between N P RM and reg-BERT. For OSE, NPRM BERT achieves a better performance for all metrics. We did not perform statistical significance testing in this case as the differences are larger. To conclude, when training and testing from the same distribution, regBERT and NPRM BERT per-form better than BERT based classifier in terms of the ranking metrics. Since the performance is generally expected to degrade slightly in a crosscorpus setting compared to a within corpus evaluation, the rest of our experiments will only focus on regBERT and NPRM, and we don't report further experiments with a BERT classifier. Cross-Corpus Pair-Wise Ranking In this experiment, we evaluated the performance of an ARA model trained with one English dataset, on other English datasets. Since NewsEla-en is the larger dataset with more diverse reading levels, we used that for training, and used OSE and Vikidia-En as test sets. Since regression scores can also be used to directly rank predictions, we compared the performance of NPRM with BERT based regression model. NPRM model, trained on Newsela-En, does well with ranking both OSE and Vikidia-En texts by their reading level, and is more robust to variation among the corpora, compared to the reg-BERT model. All measures achieve performance > 0.87 for both the datasets with N P RM . The regBERT performs comparably on Vikidia-En, but does poorly on OSE.",
      "This consists of articles sourced from The Guardian newspaper, rewritten by teachers into three reading levels (beginner, intermediate, advanced) (Vajjala and Lučić, 2018) and has been used as a bench marking dataset for ARA in the recent past. This dataset contains 189 slugs and 3 reading levels, summing to a total of 567 texts (each slug has one text in three versions). NewsEla-Spanish (NewsEla-Es): This is the Spanish subset within the existing NewsEla dataset and contains 1221 texts distributed across 243 slugs and 10 reading levels. Similar to NewsEla-En, each slug does not have all 10 levels in it. Vikidia-En/Fr: Vikidia.org3 is a children's encyclopedia, with content targeting 8-13 year old children, in several European languages. Our dataset contains 24660 texts distributed across 6165 slugs and 2 reading levels, for English (Vikidia-En) and French (Vikidia-Fr) respectively i.e. , each text in the corpus has four versions: en, en-simple, fr and fr-simple, and there are 6165 slugs in total. Azpiazu and Pera (2019)'s experiments used data from this source. However, the data itself is not publicly available. The uniqueness of the current dataset is that these are parallel, document level aligned texts in four versions -en, ensimple, fr, fr-simple. While we did not create paragraph/sentence level alignments on the corpus, we hope that this will be a useful dataset for future English and French research on ARA and Automatic Text Simplification. This is the first such dataset in ARA, and perhaps the first readily available French readability dataset. It can be accessed at: https://zenodo.org/record/6327828 Classification, Regression And Ranking Models Our primary focus in this paper is on the pairwise ranking model. However, we also compared the performance of other classification, regression, and ranking approaches with our pairwise ranking model to establish strong points of comparison. Feature representation: While the use of linguistic features, and more recently, contexual embeddings, have been explored in ARA, noncontextual embeddings were not explored much. Hence, in this paper, we employ three noncontextual embeddings (GloVe (Pennington et al., 2014), Word2vec (Mikolov et al., 2013a), fastText (Bojanowski et al. , 2017)) for training classification/regression/ranking models. Document-level embeddings are obtained by aggregating and averaging word-level embeddings for each token in the text. In addition, we also used a BERT (Devlin et al. , 2018) based classifier."
    ],
    "ground_truth": "The lower performance on Vikidia-FR compared to Newsela-ES could be attributed to several factors. Firstly, the nature of the content in each dataset is different, with Newsela-ES having content from the same domain as Newsela-EN, while Vikidia-FR has more diverse content. Secondly, the ranking metrics might penalize Vikidia-FR predictions more due to the presence of only two reading levels. Incorrect ranking levels in Vikidia-FR would result in a completely reversed list, which is heavily penalized in SRC and KTCC. Therefore, a small number of completely incorrectly ranked slugs can result in low SRC and KTCC scores for Vikidia-FR, but still result in high SRC and KTCC scores for Newsela-ES.",
    "paper_id": "nlpeer/ARR-22/ab82036287f47b63ae70bdae7c1ed5900395bfe4538cc14db1a17281e2b45434b348472df2024151e188fe3890b88a0990bf1a36540eb39ddcc015012567bdd8",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:28:28.938570",
    "processed_idx": 4
  },
  {
    "question": "Do you mean that a node in the UCCA scheme can be either a terminal or a non-terminal, but not both?",
    "answer": "Error generando respuesta: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 1024 tokens, but can only afford 503. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_30C4HaXuG0dBKbuyUyYM7lka6mW'}",
    "contexts": [
      "The use of non-terminal nodes is a simple way to avoid this liability. SDP also differs from UCCA in the type of distinctions it makes, which are more coupled with syntactic considerations, where UCCA aims to capture purely semantic cross-linguistically applicable notions. For instance, the \"poss\" label in the DM target representation is used to annotate syntactic possessive constructions, regardless of whether they correspond to semantic ownership (e.g., \"John's dog\") or other semantic relations, such as marking an argument of a nominal predicate (e.g., \"John's kick\"). UCCA reflects the difference between these constructions. Recent interest in SDP has yielded numerous works on graph parsing (Ribeyre et al. , 2014;Thomson et al., 2014;Almeida and Martins, 2015;Du et al., 2015), including tree approximation (Agić and Koller, 2014;Schluter et al., 2014) and joint syntactic/semantic parsing (Henderson et al., 2013;Swayamdipta et al. , 2016). Abstract Meaning Representation.  Another line of work addresses parsing into AMRs (Flanigan et al. , 2014;Vanderwende et al., 2015;Pust et al. , 2015;Artzi et al. , 2015), which, like UCCA, abstract away from syntactic distinctions and represent meaning directly, using OntoNotes predi-cates (Weischedel et al. , 2013). Events in AMR may also be evoked by non-verbal predicates, including possessive constructions. Unlike in UCCA, the alignment between AMR concepts and the text is not explicitly marked. While sharing much of this work's motivation, not anchoring the representation in the text complicates the parsing task, as it requires the alignment to be automatically (and imprecisely) detected. Indeed, despite considerable technical effort (Flanigan et al. , 2014;Pourdamghani et al. , 2014;Werling et al., 2015), concept identification is only about 80%-90% accurate. Furthermore, anchoring allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (Fernández-González and Martins, 2015;Birch et al. , 2016). Several transition-based AMR parsers have been proposed: CAMR assumes syntactically parsed input, processing dependency trees into AMR (Wang et al. , 2015a(Wang et al. , ,b, 2016;;Goodman et al. , 2016). In contrast, the parsers of Damonte et al. (2017) and Zhou et al. (2016) do not require syntactic pre-processing. Damonte et al. (2017) perform concept identification using a simple heuristic selecting the most frequent graph for each token, and Zhou et al. (2016) perform concept identification and parsing jointly. UCCA parsing does not require separately aligning the input tokens to the graph. TUPA creates non-terminal units as part of the parsing process.",
      "While parsing technology in general, and transition-based parsing in particular, is wellestablished for syntactic parsing, UCCA has several distinct properties that distinguish it from syntactic representations, mostly UCCA's tendency to abstract away from syntactic detail that do not affect argument structure. For instance, consider the following examples where the concept of a scene has a different rationale from the syntactic concept of a clause. First, non-verbal predicates in UCCA are represented like verbal ones, such as when they appear in copula clauses or noun phrases. Indeed, in Figure 1a, \"graduation\" and \"moved\" are considered separate events, despite appearing in the same clause. Second, in the same example, \"John\" is marked as a (remote) Participant in the graduation scene, despite not being overtly marked. Third, consider the possessive construction in Fig- ure 1c. While in UCCA \"trip\" evokes a scene in which \"John and Mary\" is a Participant, a syntactic scheme would analyze this phrase similarly to \"John and Mary's shoes\". These examples demonstrate that a UCCA parser, and more generally semantic parsers, face an additional level of ambiguity compared to their syntactic counterparts (e.g., \"after graduation\" is formally very similar to \"after 2pm\", which does not evoke a scene). Section 6 discusses UCCA in the context of other semantic schemes, such as AMR (Banarescu et al. , 2013). Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6). One such property is reentrancy, namely the sharing of semantic units between predicates. For instance, in Figure 1a, \"John\" is an argument of both \"gradu-ation\" and \"moved\", yielding a DAG rather than a tree. A second property is discontinuity, as in Figure 1b, where \"gave up\" forms a discontinuous semantic unit. Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al. , 2014). Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word. The use of non-terminal nodes is motivated by constructions with no clear head, including coordination structures (e.g., \"John and Mary\" in Figure 1c), some multi-word expressions (e.g., \"The Haves and the Have Nots\"), and prepositional phrases (either the preposition or the head noun can serve as the constituent's head). To our knowledge, no existing parser supports all structural properties required for UCCA parsing. Transition-Based Ucca Parsing We now turn to presenting TUPA.",
      "Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. Transition-based parsers (Nivre, 2003) scan the text from start to end, and create the parse incrementally by applying a transition at each step to the parser's state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V, E, ) of constructed nodes and edges, where V is the set of nodes, E is the set of edges, and : E → L is the label function, L being the set of possible labels. Some states are marked as terminal, meaning that G is the final output. A classifier is used at each step to select the next transition based on features encoding the parser's current state. During training, an oracle creates training instances for the classifier, based on gold-standard annotations. Transition Set. Given a sequence of tokens w 1 , . . . , w n , we predict a UCCA graph G over the sequence. Parsing starts with a single node on the stack (an artificial root node), and the input tokens in the buffer. Figure 2 shows the transition set. In addition to the standard SHIFT and RE-DUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), adding the NODE transition for creating new non-terminal nodes. For every X ∈ L, NODE X creates a new node on the buffer as a par-ent of the first element on the stack, with an Xlabeled edge. LEFT-EDGE X and RIGHT-EDGE X create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed if the child node already has an incoming primary edge. LEFT-REMOTE X and RIGHT-REMOTE X do not have this restriction, and the created edge is additionally marked as remote. We distinguish between these two pairs of transitions to allow the parser to create remote edges without the possibility of producing invalid graphs.",
      "A Transition-Based Directed Acyclic Graph Parser For Ucca Abstract We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures. Introduction Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a(Dixon, ,b, 2012)), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al. , 2015). It has also proven useful for machine translation evaluation (Birch et al. , 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA's applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA's distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al. , 2015;Andor et al., 2016;Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008;Ribeyre et al., 2014;Tokgöz and Eryigit, 2015), constituency parsing (Sagae and Lavie, 2005;Zhang and Clark, 2009;Zhu et al., 2013;Maier, 2015;Maier and Lichte, 2016), AMR parsing (Wang et al. , 2015a(Wang et al.",
      "See Appendix C for the hyperparameter values we found by tuning on the development set. Evaluation. We define a simple measure for comparing UCCA structures G p = (V p , E p , p ) and G g = (V g , E g , g ), the predicted and goldstandard graphs, respectively, over the same sequence of terminals W = {w 1 , . . . , w n }. For an edge e = (u, v) in either graph, u being the parent and v the child, its yield y(e) ⊆ W is the set of terminals in W that are descendants of v. Define the set of mutual edges between G p and G g : We report two variants of this measure: one where we consider only primary edges, and another for remote edges (see Section 2). Performance on remote edges is of pivotal importance in this investigation, which focuses on extending the class of graphs supported by statistical parsers. We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if G p and G g are trees. Punctuation is excluded from the evaluation, but not from the datasets. M (Gp, Gg) = {(e1, e2) ∈ Ep × Eg | y(e1) = y(e2) ∧ p(e1) = g ( Comparison to bilexical graph parsers. As no direct comparison with existing parsers is possible, we compare TUPA to bilexical dependency graph parsers, which support reentrancy and discontinuity but not non-terminal nodes. To facilitate the comparison, we convert our training set into bilexical graphs (see examples in Figure 4), train each of the parsers, and evaluate them by applying them to the test set and then reconstructing UCCA graphs, which are compared with the gold standard. The conversion to bilexical graphs is done by heuristically selecting a head terminal for each non-terminal node, and attaching all terminal descendents to the head terminal. In the inverse conversion, we traverse the bilexical graph in topological order, creating non-terminal parents for all terminals, and attaching them to the previously-created non-terminals corresponding to the bilexical heads. 8  In Section 5 we report the upper bounds on the achievable scores due to the error resulting from the removal of non-terminal nodes. Comparison to tree parsers. For completeness, and as parsing technology is considerably more 8 See Appendix D for a detailed description of the conversion procedures."
    ],
    "ground_truth": "Yes, in the UCCA scheme, a node can be either a terminal or a non-terminal, but it cannot be both. It corresponds to a terminal or to several terminals, based on semantic or cognitive considerations.",
    "paper_id": "nlpeer/PeerRead-ACL2017/193",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:29:11.692863",
    "processed_idx": 5
  },
  {
    "question": "What is the purpose of the average duration reported in Table 1?",
    "answer": "Error generando respuesta: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 1024 tokens, but can only afford 503. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_30C4HaXuG0dBKbuyUyYM7lka6mW'}",
    "contexts": [
      "The Argrewrite Corpus Based on the above design decisions, we have developed a corpus of argumentative essays with three drafts and detailed annotations for sentencealigned revisions between each consecutive pair of drafts. The main corpus has five elements, with the relationships between them shown in Figure 1; Section 3.1 describes the procedure for obtaining them. Section 3.2 briefly describes the revision schema we used and reports the inter-annotator agreement. Additionally, we have collected metadata from the participants who contributed to the corpus (discussed in Section 3.3); these data may be useful for user behavior analysis. Corpus Development Procedure We have recruited 60 participants aged 18 years and older, among whom 40 were English native speakers and 20 were non-native speakers with sufficient English proficiency. 4 The study to collect the corpus is carried out in three 40-60 minute sessions over the duration of two weeks. Draft1 Each participant begins by completing a pre-study questionnaire (Section 3.3) and writing a short essay online. Participants are instructed to keep the essay around 400 words, making a single main point with two supporting examples. They are given the following prompt: \"Suppose you've been asked to contribute a short op-ed piece for The New York Times. Argue whether the proliferation of electronic 4 i.e. , with a TOEFL score higher than 100. communications (e.g., email, text or other social media) enriches or hinders the development of interpersonal relationships. \" Draft2 A few days later, participants are asked to revise their first draft online based on the following feedback: Strengthen the essay by adding one more example or reasoning for the claim; then add a rebuttal to an opposing idea; keep the essay at 400 words. With this feedback we try to push participants to make revisions for later processing by the two interfaces used to create Draft3. Annotated Revisions I (Rev12) The two drafts are semi-manually aligned at the sentence level. 5Then, the purpose of each pair of sentence revision is manually coded by a trained annotator, following the annotation guideline (see Section 3.2). Draft3 Participants write their third draft in a lab environment. This time, they are not given additional instructional feedback. Instead, participants are shown a computer interface that highlights the differences between their first and second drafts. They are asked to revise and create a third draft to improve the general quality of their essay.",
      "Each work contains on average 42K words and 21.5 chapters.  7 Even though the CHAPTERBREAK P G19 split is small, we include it because many LRLMs are pretrained on PG-19; the much larger CHAPTERBREAK AO3 split is outof-distribution for all models that we evaluate. To extract chapters in PG-19, we match for lines beginning with the string \"chapter\", while AO3 stories already have chapter-level metadata. What are the different types of transitions in CHAPTERBREAK and how often do they occur? To get a better sense of our dataset, we perform a fine-grained annotation of 300 randomly-selected chapter transitions from CHAPTERBREAK AO3 . For each transition, we annotate any changes in the following four aspects: events, actors (characters in focus), locations, and continuity.  To annotate continuity, we follow a simplified version of the scheme proposed by Ireland (1986), 8 which considers five categories: continuous (the next chapter occurs within a day of the previous chapter), discontinuous (the next chapter occurs more than a day after the previous chapter), analepsis (the next chapter is a \"flashback\" to an earlier point in the narrative), and parallel (the next chapter reverts to the time of a previous chapter, switching the 6 https://archive.org/download/AO3_ story_dump_continuing 7 More preprocessing details and statistics can be found in Appendix A. 8 To validate our continuity annotations, we also annotate every chapter in Pride and Prejudice and obtain almost the same proportion of continuous transitions (67%) as the number reported by the expert annotation of Ireland (1986)   character or event in focus). 9 The results, shown in Table 1, demonstrate that CHAPTERBREAK covers a diverse array of transitions, including many that require global narrative understanding. Experiments We evaluate three different long-range language models on CHAPTERBREAK and compare their results to those of standard Transformer language models as well as an upper bound directly trained for suffix prediction. Language models: We evaluate three LRLMs pretrained on PG-19: the Local Transformer (Roy et al. , 2021, LT), Routing Transformer (RT) (Roy et al., 2021, RT), and BigBird (Zaheer et al. , 2020). The BigBird model is the decoder part of the released checkpoint fine-tuned with causal LM objective on 14k books of PG-19 for 100k steps. We also evaluate two standard Transformer language models, GPT-2 large (Radford et al. , 2019) and GPT-3 (Brown et al., 2020).",
      "Another model parameter selected for sensitivity studies is the remineralization rate of sunken macroalgal biomass (µ ma , Eq. 16). µ ma is a critical factor impacting the residence time of MOS-captured carbon in the ocean and associated benthic oxygen consumption by remineralization. Macroalgal biomass has been reported to be recalcitrant to microbial degradation; however, the fate of macroalgal biomass in the deep sea is uncertain (Krause-Jensen and Duarte, 2016;Luo et al. , 2019;Sichert et al., 2020). Thus, an extreme and idealized situation with µ ma set to zero is tested in sensitivity simulations (MOS-NoRe, MOS with zero mineralization of sunk macroalgal biomass). This can be thought of as a case where all biomass is permanently buried upon reaching the seafloor. This sensitivity study can also simulate an extreme case of infinitely slow remineralization, which can help in estimating the range of possible fates of remineralized organic matter. Meanwhile, this sensitivity study also represents a different macroalgae farming approach -that of harvesting the biomass to create bioenergy with carbon capture or storage (BECCS) or biochar (e.g. , Kerrison et al. , 2015;Laurens et al., 2020;Roberts et al. , 2015), with the assumption that all harvested biomass was permanently removed from the ocean. While this is a very idealized case, it serves the useful purpose of providing information on how marine biogeochemistry is impacted by the permanent removal of fixed C, N and P. The stoichiometric C : N ratio of macroalgal biomass (MR C:N ) may also influence the CDR capacity of MOS. In the current study, the MR C:N (400 : 20, Table 1) is set as 20, nearly 2 times higher than the phytoplankton stoichiometric biomass C:N ratio in the UVic ESCM (C : N=106 : 16, the Redfield ratio). However, the difference between the macroalgae and phytoplankton stoichiometric C : N ratio may have strong influences on the CDR potential of MOS. For instance, Bach et al. (2021) have indicated that the CDR potential of a floating-macroalgae (Sargassum) belt may be reduced by 7 % to 50 % due to the nutrient reallocation caused by the variation of gaps of C : N between macroalgae and phytoplankton. Thus, sensitivity experiments of macroalgal C : N ratios (MOS_Conti_CN High and MOS_Conti_CN Low ) have been performed to investigate the impacts of MR C:N on the MOS CDR capacity.",
      "Error analysis: Examples 6-10 in Figure 4 highlight failure cases for the model. Example 6 shows a case where the comparison is nonsensical because \"for\" would naturally be followed by a purpose (\"He drove the car for work. \") or a duration (\"She drove the car for hours. \") rather than a concrete object whose size is measurable. Example 7 highlights an underspecified frame. One crowd worker provided the example, \"PERSON stopped the fly with {the jar / a swat-ter},\" where fly < weight {jar, swatter}. However, two crowd workers provided examples like \"PERSON stopped their car with the brake,\" where clearly car > weight brake. This example illustrates complex underlying physics we do not model: a brake-the pedal itself-is used to stop a car, but it does so by applying significant force through a separate system. The next two examples are cases where the model nearly predicts correctly (8, e.g. , \"She lived at the office. \") and is just clearly wrong (9, e.g., \"He snipped off a locket of hair\"). Example 10 demonstrates a case of polysemy where the model picks the wrong side. In the phrase, \"She caught the runner in first,\", it is correct that she > speed runner. However, the sense chosen by the crowd workers is that of, \"She caught the baseball,\" where indeed she < speed baseball. Related Work Several works straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Earlier works in these areas use large amounts of text to try to extract general statements like \"A THING CAN BE READABLE\" (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013;Li et al. , 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al. , 2012;Silberer et al. , 2013)",
      "Good Night At 4 Pm?! Time Expressions In Different Cultures Abstract We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as \"morning\" in English or \"manhã\" in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this method to 27 languages and analyze the similarities across languages in the grounding of time expressions. Introduction Natural language understanding requires the ability to map language such as color descriptions (McMahan and Stone, 2015), spatial instructions (Chen et al. , 2019), and gradable adjectives (Shivade et al. , 2016) to real-world physical properties. This paper focuses on temporal grounding, particularly mapping time expressions such as \"morning\" and \"evening\" to hours in the day. Temporal commonsense reasoning has been gaining traction lately (Zhou et al. , 2019;Qin et al., 2021), and this important capability can benefit various temporal tasks such as event ordering and duration prediction. One of the challenges in grounding time expressions to standard times is that such expressions may be interpreted with some variation by different people. Reiter and Sripada (2002) found that human-written weather forecasts exhibited significant individual differences between forecasters in the interpretation of time expressions. One factor for this variation is cultural differences. Vilares and Gómez-Rodríguez (2018) analyzed the time of day in which people from 53 countries posted time-specific greetings such as \"good morning\" and \"good evening\" on Twitter. They showed variation in greeting times across languages and cultures, which they connected to known facts and published statistics about cultural differences, such as differences in average wake and sleep times. We propose to re-frame the research question posed by Vilares and Gómez-Rodríguez (2018) as a task of time expression grounding: given a time expression, the goal is to map it to a range of hours during the day. For example, what is the range of hours an Italian speaker refers to when saying pomeriggio (afternoon)? Such a grounding model can provide cultural context to machine translation systems (de Medeiros Caseli et al. , 2010), language learning apps (Teske, 2017), and user-centered dialogue systems (Miehle et al. , 2016). We collected gold standard interpretations from four countries, which indeed exhibited some variation. We then proposed 3 language-agnostic methods based on either a corpus or a language model (LM)."
    ],
    "ground_truth": "The purpose of the average duration reported in Table 1 is to demonstrate that the proposed human evaluation method is time-effective.",
    "paper_id": "nlpeer/ARR-22/0f595b4bd968ef5daacc88c4edb581bf7f683af30b6874ba157c6d2282aabead577041409b949924e4bc5ef54c543d85a10e962be8e70304dea65e1b18441bdb",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:29:24.076931",
    "processed_idx": 6
  },
  {
    "question": "What is the evidence that the generative model is successful in synthesizing new molecules?",
    "answer": "Error generando respuesta: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 1024 tokens, but can only afford 503. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_30C4HaXuG0dBKbuyUyYM7lka6mW'}",
    "contexts": [
      ", 2008), although this typically assumes access to a pool or stream of unlabelled data for which the learning algorithm can query labels (Settles, 2009). In our setting, no unlabelled questions are provided, necessitating the use of a generative model to suggest questions instead. Moreover, our annotators are free to edit Figure 2: The Annotation Interface used for data collection. This example shows a question generated using a generative assistant trained on the AdversarialQA data and selected an adversarial sampler, which successfully allowed the annotator to beat the QA model in the loop. and browse generated questions, whereas annotators in active learning typically only provide labels and have no choice in what to label. Some of our sampling and filtering strategies based on entropy are inspired by uncertainty sampling, a standard active learning algorithm (Lewis and Gale, 1994). Experimental Setup Our study focuses on the effects of incorporating generative annotation assistants, and understanding their interactions with annotators and discriminative models-in-the-loop in a DADC context for QA. We provide crowdworkers with a short passage from Wikipedia and ask them to write five questions and highlight the span in the passage that best answers the question for each (see Figure 2). We pay workers equally across experiment modes to avoid creating an incentive imbalance and pay out an additional bonus for each question that successfully beats the discriminative QA model i.e. , for each question that the model fails to answer correctly.  Finally, we validate all collected examples using a separate worker pool that also undergoes rigorous onboarding and validation. We ask three of these additional workers to report on the validity of each annotated example. Selected Passages We select passages from KILT (Petroni et al. , 2021) to allow for the possibility of future investigation into cross-domain and task transfer. We restrict KILT passages to those with between 100 and 600 tokens that are used by at least 5 of the KILT tasks. Furthermore, we filter out any passages with any 8-gram overlap (after normalisation) to the SQuAD1.1 training or development sets, seeking to ensure that all passages used in our study are novel and previously unseen by the discriminative QA models in the loop. This leaves a total of 10,109 passages from 421 Wikipedia pages. We retain and supply all passage-relevant KILT metadata (such as IDs and provenances) with our collected datasets to facilitate future work.",
      "Model-in-the-Loop The discriminative QA model in the loop is ELECTRA Large (Clark et al. , 2020) trained on SQuAD1.1 and AdversarialQA, and enhanced using SynQA to improve adversarial robustness as investigated by Bartolo et al. (2021). 1This model represents the best-performing model on the Dynabench (Kiela et al. , 2021) leaderboard at the time of conducting this study, obtaining a word-overlap F 1 score of 94.5% on the SQuAD1.1 dev set, and represents the state-of-the-art on AdversarialQA achieving 77.6% on the D BiDAF subset, 71.5% on D BERT , and 63.2% on D RoBERTa . Generator-in-the-Loop For our generative models, we use the fairseq (Ott et al. , 2019) implemen-tation of BART Large (Lewis et al. , 2020), and finetune the decoder to generate questions conditioned on the passage and the answer highlighted by the annotator. To provide annotators with a diverse set of questions, we decode using nucleus sampling with top p = 0.75, as decoding using standard beam search results in questions which are more similar to each other and therefore likely to be less useful as question prompts to annotators. To speed up inference and model-annotator interaction, we preemptively identify answer candidates for each passage and generate questions to build up a large cache from which we serve questions during annotation. Once there are no questions remaining in the cache for a particular answer, or if the annotator selects an answer that is not in the cache, we fall back to querying the generative model in realtime. In this work, we investigate generative assistants trained on three different sources of questions: SQuAD1.1, AdversarialQA, and the combination of both SQuAD and AdversarialQA. Question Sampling We investigate three different selection strategies for presenting the generated questions as prompts to annotators: i) generator likelihood samples candidates in the order prescribed by the generative model's associated likelihood values; ii) adversarial sampling selects generated questions in order of the least word-overlap F 1 scores when queried against the discriminative QA model; and iii) uncertainty sampling is inspired by active learning and selects generated questions in order of the least span selection confidence when queried against the QA model. The latter two provide an interesting trade-off for exploration as we would expect the quality of the generated questions to be worse than if sampled based on likelihood.",
      ", 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al. , 2021) and being sensitive to the choice of model that was used in the loop (Bowman and Dahl, 2021;Phang et al. , 2021). This work builds on previ-ous work in adversarial data collection methods for QA (Bartolo et al. , 2020) , and work investigating the use of question generation models to create synthetic adversarial data to improve QA model robustness (Bartolo et al. , 2021). Generative Model Annotation Support A long line of prior work has trained generative models for question answering (Du et al. , 2017;Du and Cardie, 2018;Zhao et al., 2018;Lewis and Fan, 2019;Alberti et al., 2019;Puri et al., 2020;Yang et al., 2020;Bartolo et al. , 2021;Lewis et al. , 2021). In many cases, these approaches filter out questions that an external QA model gets wrong, in order to ensure correctness of the generated questions; our filtering strategies instead focus on generated questions that QA models get wrong as we hypothesise that these would serve as more useful initial prompts to human annotators. Generative models have also been used to aid experts with writing contrast sets (Wu et al. , 2021;Ross et al. , 2021), but to the best of our knowledge, this is the first work to investigate the use of generative annotation assistants for crowdworkers directly in the annotation loop for NLP. Recent work on supporting crowdworkers for textual entailment in a non-adversarial setting shows no improvements on downstream transfer performance over baseline, albeit with reductions in previously observed issues with annotation artefacts (Bowman et al. , 2020). Subsequent work highlights the need for further data collection efforts focusing on improving writing-based annotation processes (Vania et al. , 2021), which we aim to investigate in this work. Separately, Ettinger et al. (2017) provide breakers with the ability to minimally edit original data to identify the boundaries of system capabilities, while Potts et al. (2021) analyse the use of prompts to assist crowdworkers in beating a model in the loop for sentiment analysis. In both cases, prompts are sourced from existing datasets and are not generated on the fly. Active Learning And Weak Supervision Active learning approaches have been used to accelerate annotation (Tsuruoka et al.",
      "Models In The Loop: Aiding Crowdworkers With Generative Annotation Assistants Abstract In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection. Introduction Natural language processing has become increasingly reliant on large datasets obtained using crowd sourcing. However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017;Schwartz et al. , 2017;Gururangan et al. , 2018;Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al., 2016;Weissenborn et al., 2017;Yogatama et al. , 2019;McCoy et al., 2019). Dynamic Adversarial Data Collection (DADC) aims to address these issues Example interaction between an annotator and the models in the loop. The annotator selects an answer from the passage, for which the Generative Annotation Assistant (GAA) prompts a question. The annotator can then freely modify the question and/or answer, or generate another prompt. In the adversarial data collection setting, a model-in-the-loop provides predictions with the aim of encouraging annotators to find model-fooling examples. In the answer prompting setting, an answer suggestion is prompted by the assistive model instead of being selected by the annotator. by introducing state-of-the-art models into the data collection loop and asking human annotators to produce examples that these models find challenging (Kiela et al. , 2021). The intuition behind this approach is that it leads human annotators to better explore the space of possible examples.",
      "Annotator Interaction With Gaas While we provide annotators with instructions explaining how they can use the GAAs to aid their annotation, they are free to query the generative models as many times as they like, if at all, during annotation. We are interested to see how the three main factors affecting interaction with the GAAs that we explore -training data, sampling strategy, and answer prompting -affect the ways in which annotators interact or use the GAAs. Results, shown in Table 5, indicate that annotators query the GAA less frequently when being shown simpler prompts i.e. those obtained using a GAA trained on non-adversarially sourced examples, or selected using likelihood sampling which tends to provide higher quality and less complex generated texts. We also find that annotators query the GAA more frequently when an answer prompt is also provided. We believe that this can be attributed to the fact that the answer and question prompt setting is more similar to a validation work- flow, allowing annotators to generate prompts until a satisfactory one is found. Discussion And Conclusion In this work, we introduce Generative Annotation Assistants (GAAs) and investigate their potential to aid crowdworkers with creating more effective training data more efficiently. We perform a thorough analysis of how GAAs can be used for improving QA dataset annotation in different settings, including different generative model training data, sampling strategies, and whether to also provide annotators with answer suggestions. We find that GAAs are beneficial in both the standard and adversarial data collection settings. In the standard data collection setting, and under the assumption of no access to adversarially-collected data, GAAs with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent downstream QA performance. We find that while GAAs are effective for improving standard data collection, we still do not approach the performance obtained when using adversarial data collection. For adversarial data collection, we demonstrate improved effectiveness of the annotation process over the non-GAA baseline, although this comes at a cost of reduced annotation efficiency. We show that also aiding annotators with answer prompts boosts data collection efficiency even beyond that of standard data collection, while retaining overall downstream performance."
    ],
    "ground_truth": "The evidence that the generative model is successful in synthesizing new molecules lies in the high Validity score of 88.18% achieved by the G-SphereNet method, which is significantly higher than the 39.77% achieved by E-NFs. This high performance indicates that the model effectively captures the dependency between atoms and learns the underlying chemical rules of molecular geometries. Furthermore, the G-SphereNet method has lower MMD distances for four types of chemical bonds compared to G-SchNet, demonstrating its ability to accurately model the 3D structural distribution of molecular geometries.",
    "paper_id": "openreview/ICLR-2022-conf/C03Ajc-NS5W",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:29:35.647613",
    "processed_idx": 7
  },
  {
    "question": "Can the proposed algorithm be used to recover real neural networks?",
    "answer": "Error generando respuesta: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 1024 tokens, but can only afford 503. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_30C4HaXuG0dBKbuyUyYM7lka6mW'}",
    "contexts": [
      "It has also been used to evaluate recent work on uncertainty propagation methods for neural networks (Hernández-Lobato and Adams, 2015). Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under-and over-assessment are usually different depending on the specific scenario. An engineering example is given by Zellner (1986) in the context of dam construction, where an underestimate of peak water level is much more serious than an overestimate. Such real-world applications guided many developments in this field: we believe that translation and other language processing scenarios which rely on NLP technologies can heavily benefit from these advancements. Conclusions This work explored new probabilistic models for machine translation QE that allow better uncertainty estimates. We proposed the use of NLPD, which can capture information on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings. Uncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al. , 2013). Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work (Nguyen and O'Connor, 2015). This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance. Our main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions.",
      "For example, the phrases following \"we propose/apply/use\" could be important in the text. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features. To effectively capture both the semantic and syntactic features, we use recurrent neural networks (RNN) (Cho et al. , 2014;Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.e., semantic understanding). Furthermore, we incorporate a copying mechanism (Gu et al. , 2016) to allow our model to find important parts based on positional information. Thus, our model can generate keyphrases based on an understanding of the text, regardless of the presence or absence of keyphrases in the text; at the same time, it does not lose important in-text information. The contribution of this paper is three-fold.  First, we propose to apply an RNN-based generative model to keyphrase prediction, as well as incorporate a copying mechanism in RNN, which enables the model to successfully predict phrases that rarely occur. Second, this is the first work that concerns the problem of absent keyphrase prediction for scientific publications, and our model recalls up to 20% of absent keyphrases. Third, we conducted a comprehensive comparison against six important baselines on a broad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods. In the remainder of this paper, we first review the related work in Section 2. Then, we elaborate upon the proposed model in Section 3. After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6. Section 7 concludes the paper. Related Work Automatic Keyphrase Extraction A keyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and the process of extracting keyphrases can typically be broken down into two steps. The first step is to generate a list of phrase can-didates with heuristic methods. As these candidates are prepared for further filtering, a considerable number of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The primary ways of extracting candidates include retaining word sequences that match certain part-of-speech tag patterns (e.g., nouns, adjectives) (Liu et al.",
      "One major distinction in different classification algorithms can be drawn in the consideration of auxiliary information. Most contributions take a purely feature-oriented approach, while others try to incorporate information revolving around spreading Fake News and the respective dissemination process. The feature-based approach focuses on extracting relevant linguistic peculiarities associated with Fake News. Typical examples are characters n-grams, words or a measure of the readability and syntax of an article body (Pérez-Rosas et al. , 2018, p.5). Other authors included the average word length, counts of exclamation marks or the sentiment of an article (Khan et al. , 2019, p.6). After finding appropriate features, traditional machine learning algorithms as well as (deep) neural networks are proposed to classify the text instance given the extracted features. Other feature-based approaches directly rely on deep learning, using neural networks for learning good representations of the text input by a stack of hidden layers which is then fed in a last classification layer. This approach is used by Yang et al. (2018) who simultaneously train a CNN on text and image data to classify fake entities and by Dong et al. (2019) who implement a two-step approach of using supervised and unsupervised learning with a CNN as well. But despite the fact that feature-based approaches are fairly popular within the FND research, Shu et al. (2017) argue that they are not sufficient. Approaches that use auxiliary information e.g. revolve around modeling the dissemination process of Fake News by incorporating spatio-temporal information about users who like, share or publish (potential) Fake News (Ruchansky et al. , 2017;Ren and Zhang, 2020). The FNC-1 organizers use a feature-based approach by proposing a baseline model that extracts various features from the headlines and article bodies. The approach of specifically modeling the relationship between a headline and a respective article body was also exploited by Yoon et al. (2019) in the context of clickbait detection while the FNC-1 takes this approach as a pre-step to FND. Material And Methods Model Architectures Representation learning can be seen as one of the crucial success factors of large pretrained models such as BERT or XLNet that yield outstanding performances on a variety of NLP tasks. With the introduction of BERT (Devlin et al. , 2019) -short for Bidirectional Encoder Representations from Transformersthe possibility of simultaneously learning left and right word context was introduced.",
      "Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a bias transformation to encode the mapping between 'clean' tags and the kinds of tags produced from projection. We propose a new neural network model for sequence tagging in a low-resource language, suitable for training with both a tiny gold standard annotated corpus, as well as distant supervision using cross-lingual tag projection. Our model uses a bidirectional Long Short-Term Memory (BiL-STM), which produces two types of output: gold tags generated directly from the hidden states of a neural network, and uncertain projected tags generated after applying a further linear transformation. This transformation, encodes the mapping between the projected tags from the high-resource language, and the gold tags in the target low-resource language, and learns when and how much to trust the projected data. For example, for languages without determiners, the model can learn to map projected determiner tags to nouns, or if verbs are often poorly aligned, the model can learn to effectively ignore the projected verb tag, by associating all tags with verbs. Our model is trained jointly on gold and distant projected annotations, and can be trained end-to-end with backpropagation. Our approach captures the relations among tokens, noisy projected POS tags and ground truth POS tags. Our work differs in the use of projection, in that we explicitly model the transformation between tagsets as part of a more expressive deep learning neural network. We make three main contributions. First, we study the noise of projected data in word alignments and describe it with an additional transformation layer in the model. Second, we integrate the model into a deep neural network and jointly train the model on both annotated and projected data to make the model learn from better supervision. Finally, evaluating on eight simulated and two real-world low-resource languages, experimental results demonstrate that our approach uniformly equals or exceeds existing methods on simulated languages, and achieves 86.7% accuracy for Malagasy and 82.6% on Kinyarwanda, exceeding the state-of-the-art results of Duong et al. (2014). Related Work For most natural language processing tasks, the conventional approach to developing a system is to use supervised learning algorithms trained on a set of annotated data. However, this approach is inappropriate for low-resource languages due to the lack of annotated data.",
      "Learning Confidence For Transformer-Based Neural Machine Translation Abstract Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success. A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings. However, this task remains a severe challenge for neural machine translation (NMT), where probabilities from softmax distribution fail to describe when the model is probably mistaken. To address this problem, we propose an unsupervised confidence estimate learning jointly with the training of the NMT model. We explain confidence as how many hints the NMT model needs to make a correct prediction, and more hints indicate low confidence. Specifically, the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty. Then, we approximate their level of confidence by counting the number of hints the model uses. We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence/word-level quality estimation tasks. Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios: (1) discovering noisy samples and (2) detecting out-of-domain data. We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate, which outperforms standard label smoothing 1 . Introduction Confidence estimation has become increasingly critical with the widespread deployment of deep neural networks in practice (Amodei et al. , 2016). It aims to measure the model's confidence in the prediction, showing when it probably fails. A calibrated confidence estimate can accurately identify * Work done while the author was an intern at Tencent. † Corresponding author. 1 https://github.com/yulu-dada/Learned-conf-NMT failure, further measuring the potential risk induced by noisy samples and out-of-distribution data prevalent in real scenarios (Nguyen and O'Connor, 2015;Snoek et al. , 2019). Unfortunately, neural machine translation (NMT) is reported to yield poor-calibrated confidence estimate (Kumar and Sarawagi, 2019;Wang et al. , 2020) , which is common in the application of modern neural networks (Guo et al., 2017). It implies that the probability a model assigns to a prediction is not reflective of its correctness. Even worse, the model often fails silently by providing high-probability predictions while being woefully mistaken (Hendrycks and Gimpel, 2017). We take Figure 1 as an example."
    ],
    "ground_truth": "The proposed algorithm can indeed be used to recover real neural networks, specifically two- and three-layer networks. However, it's important to note that these types of networks are not commonly used in practice. Additionally, the algorithm's effectiveness may be limited by practical constraints such as access to the black-box network and potential numerical inaccuracies. Thus, while the algorithm's ability to recover these networks is theoretically proven, its practical application may be limited.",
    "paper_id": "openreview/ICLR-2023-conf/-CoNloheTs",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:29:47.309111",
    "processed_idx": 8
  },
  {
    "question": "Can the pairwise distance of the latent variables be compared to the distance when fitting a proper phylogenetic tree?",
    "answer": "Error generando respuesta: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 1024 tokens, but can only afford 503. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_30C4HaXuG0dBKbuyUyYM7lka6mW'}",
    "contexts": [
      "5 We believe its utility is worth further examination, however, in our preliminary experiments, the phylogenetic weighting demonstrates better agreement with the existing universals from the literature than the unweighted average. As Bickel (2011) and Dryer (2018) point out, geography is also an important factor, i.e. neighboring languages are more likely to have common properties than distant languages. However, both works agree that measuring geographic distance of languages brings its own complications and therefore it is not taken up in this paper. Experiments The goal of our experiments is to demonstrate the usefulness of the proposed framework for empirical exploration of the language-typological space beyond simple implications and Boolean variables. The framework is to serve as a methodological tool and should ultimately be complemented with a theoretically motivated agenda for exploring systematic correlations among underexplored properties of grammatical systems. To establish how the framework can be used we proceed in three steps: First, we re-evaluate some famous universals of Greenberg (1963) and discuss the interpretations of varying truth values and standard deviations (Sec. 3.1). In the second step, we demonstrate a scenario for testing unexplored candidates for typological universals (which can consist of arbitrary formulae from propositional logic) against the full spectrum of treebanks, taking into variability with respect to each property in the real-valued logic. We work with a simple procedure of generating candidate formulae: taking over the shape of potentially relevant logical combinations from established universals and replacing the properties they include with new ones. A framework is useful for empirical exploration if its diagnostics are robust: a statement capturing a systematic relationship should generalize from one sufficiently large sample of languages to another. We test this by splitting the set of treebanks in an estimation set and a held-out set (Sec. 3.2). Finally, we enumerate implications of two variables from a selected set of single-link properties and discuss the top-scoring formulae, also in comparison to the findings from previous work on binary properties (Sec. 3.3). Evaluation Of Greenberg'S Universals The framework can be used to evaluate universals proposed in typological literature. The most important example are the 45 universals by Greenberg (1963).",
      "If more than two conditions are uploaded, normalization and initial differential expression analysis will be done over the entire data set. However, it is often useful to perform pairwise comparison of two conditions or timepoints. For pairwise comparisons of two conditions or time-points of larger datasets, a Filter menu is provided. Data resulting from pairwise comparison can be visualized with MA and Volcano plots. All plots are interactive and the user can obtain detailed information about a gene hovering over the dots of the plots. All details on normalization and differential expression analysis can be found in the user manual of RNfuzzyApp. We wanted to provide a simple way of clustering gene expression data from a limited number of samples, e.g. from a short time-series. To this end, RNfuzzyApp offers a heatmap, generated by heatmaply. Figure 4a shows the heatmap of a timecourse of 3 time points from the Tabula muris senis project, 15 where the gene expression levels of the replicates per time point are clustered using hierarchical clustering. The user can choose the distance matrix and agglomeration method, as well as the FDR cut-off of genes to include in the clustering. Clustering is done using hclust and cutree to generate the coloured dendrogram on the heatmap. The genes contained in the different clusters indicated by the colours in the dendrogram are downloadable as a table for further analysis. Enrichment analysis and orthology assignment. For enrichment analysis of Gene Ontology (GO-) terms, 16 pathways (Wikipathways, 17 Reactome 18 and KEGG 19 ), Human Protein Atlas, 20 CORUM data on protein complexes, 21 and TRANSFAC, 22 the gprofiler2 package is included in RNfuzzyApp. Results are displayed as an image of overall enrichment, as well as a results table . The table together with a bar plot of the enriched term names sorted according to p-value are downloadable by the user (Figure 4b, bar plot of enrichments generated from downloaded table). Gprofiler2 is also used to find orthologs in another species of a userprovided list of genes. To this end, the user simply needs to upload a list of genes, and select the original and the target species. Fuzzy clustering of time-series expression data is a highly useful technique for analysing temporal data. The Mfuzz package from R was developed for soft clustering of temporal gene expression data.",
      "is not ideal. A usual way to deal with this problem is model the logarithm of the response variables, since this transformation maps strictly positive values to the real line. However, there is no reason to believe this is the best possible mapping: a better idea would be to learn it from the data. Warped GPs (Snelson et al. , 2004) are an extension of GPs that allows the learning of arbitrary mappings. It does that by placing a monotonic warping function over the observations and modelling the warped values inside a standard GP. The posterior distribution is obtained by applying a change of variables: p(y * |x * ) = f (y * ) 2πσ 2 * exp f (y * ) -µ * 2σ * , where µ * and σ * are the mean and standard deviation of the latent (warped) response variable and f and f are the warping function and its derivative. Point predictions from this model depend on the loss function to be minimised. For absolute error, the median is the optimal value while for squared error it is the mean of the posterior. In standard GPs, since the posterior is Gaussian the median and mean coincide but this in general is not the case for a Warped GP posterior. The median can be easily obtained by applying the inverse warping function to the latent median: y med * = f -1 (µ * ). While the inverse of the warping function is usually not available in closed form, we can use its gradient to have a numerical estimate. The mean is obtained by integrating y * over the latent density: E[y * ] = f -1 (z)N z (µ * , σ 2 * )dz, where z is the latent variable. This can be easily approximated using Gauss-Hermite quadrature since it is a one dimensional integral over a Gaussian density. The warping function should be flexible enough to allow the learning of complex mappings, but it needs to be monotonic. Snelson et al.",
      "To achieve the conversion of vocal tone, we introduce a mapping function M to convert the latent variables from q ϕ (z a |x a , c a ) to q ϕ (z p |x p , c p ). Concretely, we sample a latent variable of amateur vocal tone z a from q ϕ (z a |x a , c a ), and map z a to M(z a ). Then, M can be optimized by minimizing the negative log-likelihood of M(z a ): L map1 (M) = -log q ϕ (M(z a )|x p , c p ). Define ĉp as the mix of 1) the content vectors from the amateur recording aligned by SADTW, 2) vocal timbre embedding encoded by timbre encoder, and 3) template pitch7 embeddings encoded by pitch encoder. To make sure the converted latent variable could work well together with ĉp to generate a high-quality audio sample (with the correct pitch and improved vocal tone), we send M(z a ) to the CVAE decoder to generate x, and propose an additional loss: L map2 (M) = ∥x -x p ∥ 1 + λ(D(x) -1) 2 , where D has been optimized by Eq. ( 1); λ is a hyper-parameter. Training And Inference There are two training stages for NSVB: in the first training stage, we optimize CVAE by minimizing the following loss function L(ϕ, θ) = -ELBO(ϕ, θ) + λL adv (ϕ, θ), and optimize the discriminator (D) by minimizing Eq. ( 1). Note that, the first stage is the reconstruction process of mel-spectrograms, where any unpaired, unlabeled singing data beyond Pop-BuTFy could be leveraged to facilitate the learning of the latent representations. In the second training stage, we optimize M on the parallel dataset Pop-BuTFy by minimizing the following loss function L(M) = L map1 (M) + L map2 (M). ϕ, θ, and D are not optimized in this stage. In inference, the encoder of CVAE encodes x a with the condition c a to predict z a .  Secondly, we map z a to M(z a ), and run SADTW to align the amateur recordings with the template pitch curve. The template pitch curve can be derived from a reference recording with good intonation or a pitch predictor with the input of music notes.",
      "Extensions Our neural transformer-based sequence tagging model can be very effective at proposing potentially good gaps, but the task becomes more challenging when we expect the output to meet additional requirements such as no repetitions, no gap interdependence, a minimum distance between gaps and a varied selection of lexico-grammatical items. We address these issues using two complementary strategies: a manipulation of the loss function and a post-processing module. Loss Manipulation In order to spread gaps evenly throughout the text, we modify the token-level loss function of our tagging model by imposing a higher penalty on tokens that are in close proximity to a gap. Let g be the position of a gap in the sequence, then for each token in position i in the proximity of g, i.e. |g -i| < D, the loss function l i ′ for the token in position i is defined as: l i ′ = l i * W |g -i| (1) where W represents the penalty and D is the maximum distance scope for penalisation. 3 Equation 1 thus gives more weight to tokens closer to gaps, which results in higher penalisation of their cost functions whenever they are misclassified. Post-Processing We also employ a post-processing strategy where we replace the gaps that are repeated in the text with better options. We optimise the choice of these alternative gaps by considering the distance between them and the resulting distribution of gaps with different part-of-speech (PoS) tags. Our post-processing step can be seen as a reranking function. The gap candidates that are originally ranked based on the model's confidence scores change their ranking to match other desirable requirements of a well-structured cloze test. If the selected n-best gaps include repetitions, our post-processing algorithm randomly chooses one of them at a time and attempts to replace it with a better alternative. An alternative gap is deemed better if 1) its answer is not a repetition of another gapped word, 2) its distance to other selected gaps meets the minimum required distance or is higher than the pairwise distances of the originally selected gaps, and 3) it improves the PoS distribution of the gapped words. The PoS distribution of each new selection of gaps is compared to the average gapped PoS distribution of the cloze tests in the training data using Kullback-Leibler (KL) divergence."
    ],
    "ground_truth": "Yes, the pairwise distance of the latent variables can be compared to the distance when fitting a proper phylogenetic tree. This comparison was conducted through a quantitative analysis, which involved projecting the dataset in a non-linear manifold against the branch lengths, or phylogenetic distances. The Euclidean distances were then computed between the latent representations of the leaves and the corresponding branch lengths in the phylogenetic tree. However, due to the challenge of comparing different data types, the conclusions drawn from this section should be interpreted with caution. More conclusive results can be found in Figures 2 and 7.",
    "paper_id": "openreview/ICLR-2022-conf/FZoZ7a31GCW",
    "model_used": "openrouter",
    "timestamp": "2025-07-24T20:30:01.553860",
    "processed_idx": 9
  }
]