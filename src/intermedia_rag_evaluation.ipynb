{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8bf724",
   "metadata": {},
   "source": [
    "## 1. Carga y Procesamiento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dd63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from collections import defaultdict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23020a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "def unstructured_extraction(pdf_path:str):\n",
    "    elements = partition(filename=pdf_path)\n",
    "    return elements\n",
    "\n",
    "pdf_path = r\"C:/Users/danie/Downloads/PaperMind/backend/data/raw/n28a12.pdf\"\n",
    "elements = unstructured_extraction(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd15c1d",
   "metadata": {},
   "source": [
    "## 2. Chunking con Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df5673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = defaultdict(list)\n",
    "for el in elements:\n",
    "    if el.text.strip():\n",
    "        metadata = el.metadata.to_dict()\n",
    "        page = metadata.get('page_number')\n",
    "        pages[page].append({\n",
    "            'Texto': el.text.strip(),\n",
    "            'Nombre del documento': metadata.get('filename'),\n",
    "            'Idioma': metadata.get('languages', ['unknown'])[0],\n",
    "        })\n",
    "all_chunks = []\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "for page_number, elementos in pages.items():\n",
    "    full_text = \" \" .join(el['Texto'] for el in elementos)\n",
    "    base_metadata = {\n",
    "        'N√∫mero de pagina': page_number,\n",
    "        'Nombre del documento': elementos[0]['Nombre del documento'],\n",
    "        'Idioma': elementos[0]['Idioma'],\n",
    "    }\n",
    "    doc = Document(page_content=full_text, metadata=base_metadata)\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7923a",
   "metadata": {},
   "source": [
    "## 3. Vector Store y Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c56dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "vector_store = Chroma.from_documents(all_chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dca2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(all_chunks, k=5)\n",
    "chroma_retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092202",
   "metadata": {},
   "source": [
    "## 4. Inicializaci√≥n de Clientes y Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3574b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "open_router_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_KEY\")\n",
    ")\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "llama_model_name = \"llama-3.3-70b-versatile\"\n",
    "qwen_model_name = \"qwen/qwen3-32b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ded186",
   "metadata": {},
   "source": [
    "## 5. Generaci√≥n de Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe12739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(client, model_name, user_query, context_str, client_type):\n",
    "    system_prompt = \"Eres un asistente que responde preguntas bas√°ndote √∫nicamente en el contexto proporcionado. Si la respuesta no est√° en el contexto, di que no tienes suficiente informaci√≥n.\"\n",
    "    full_prompt = f\"Contexto:\\n{context_str}\\n\\nPregunta: {user_query}\"\n",
    "\n",
    "    try:\n",
    "        if client_type == 'gemini':\n",
    "            response = client.generate_content(full_prompt)\n",
    "            return response.text\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ]\n",
    "            if client_type == 'groq':\n",
    "                chat_completion = client.chat.completions.create(messages=messages, model=model_name)\n",
    "            elif client_type == 'openrouter':\n",
    "                chat_completion = client.chat.completions.create(messages=messages, model=model_name)\n",
    "            return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error al generar respuesta con {model_name}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2823a9",
   "metadata": {},
   "source": [
    "## 6. Ejecuci√≥n y Comparaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab85fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Gemini:**\n",
       "Este documento trata sobre el desarrollo de un corpus (un conjunto de textos) de ensayos acad√©micos revisados.\n",
       "\n",
       "**Puntos clave:**\n",
       "\n",
       "*   Los participantes escribieron ensayos que fueron revisados en m√∫ltiples borradores (Draft 1, Draft 2 y Draft 3).\n",
       "*   Se utiliz√≥ una interfaz de computadora para resaltar las diferencias entre los borradores iniciales y las revisiones.\n",
       "*   El corpus contiene 180 ensayos en total.\n",
       "*   Se alinearon las oraciones entre los borradores y se etiquetaron los prop√≥sitos de las revisiones.\n",
       "\n",
       "Adem√°s, el documento hace referencia a otros estudios y datos relacionados con temas como la inversi√≥n extranjera directa (FDI), el desarrollo econ√≥mico y las emisiones contaminantes, mencionando a autores como Chase-Dunn, Tanzi, Ohlin, Bekun, Agosin & Machado entre otros y entidades como las Naciones Unidas y la FAO.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Llama (Groq):**\n",
       "El documento parece ser una recopilaci√≥n de diferentes secciones y referencias de art√≠culos y publicaciones acad√©micas. A continuaci√≥n, te presento un resumen de los temas y puntos clave que se mencionan:\n",
       "\n",
       "1. **Desarrollo de un corpus**: Se describe un procedimiento para mejorar la calidad de ensayos a trav√©s de revisiones y creaci√≥n de borradores. Se menciona que se alignmentaron oraciones a lo largo de los borradores y se etiquetaron los prop√≥sitos de revisi√≥n.\n",
       "2. **Estad√≠sticas descriptivas**: Se proporcionan estad√≠sticas sobre un conjunto de datos que incluye 180 ensayos, con un promedio de 400 palabras en los dos primeros borradores y 500 palabras en el tercer borrador.\n",
       "3. **Bibliograf√≠a y referencias**: Se incluyen referencias a varios art√≠culos y publicaciones acad√©micas sobre temas como la econom√≠a, el desarrollo sostenible, la globalizaci√≥n y la pol√≠tica econ√≥mica.\n",
       "4. **Modelos econ√≥micos**: Se menciona el modelo de Agosin y Machado (2005) para el estudio de la relaci√≥n entre la inversi√≥n extranjera directa (IED) y la inversi√≥n dom√©stica.\n",
       "5. **Pol√≠tica y recomendaciones**: Se incluye una secci√≥n sobre recomendaciones de pol√≠tica y se menciona que los datos subyacentes est√°n disponibles para garantizar la reproducibilidad de los resultados.\n",
       "6. **Desarrollo y energ√≠a**: Se abordan temas relacionados con el desarrollo sostenible, la energ√≠a renovable y la mitigaci√≥n de emisiones en pa√≠ses como la India.\n",
       "\n",
       "En general, el documento parece ser una recopilaci√≥n de diferentes temas y referencias relacionadas con la econom√≠a, el desarrollo sostenible y la pol√≠tica econ√≥mica, con un enfoque en la inversi√≥n extranjera directa y el desarrollo de modelos econ√≥micos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Qwen (OpenRouter):**\n",
       "El documento presenta informaci√≥n sobre **dos l√≠neas tem√°ticas principales**:\n",
       "\n",
       "1. **Desarrollo de un corpus de ensayos acad√©micos**:  \n",
       "   - Se describe un proceso de revisi√≥n iterativo en el que participantes mejoran sus ensayos a trav√©s de tres borradores (Draft1, Draft2, Draft3).  \n",
       "   - Se emplea una interfaz computacional para resaltar diferencias entre borradores y alinear frases revisadas, etiquetando los prop√≥sitos de las revisiones.  \n",
       "   - Estad√≠sticas descriptivas: 180 ensayos en total (120 con 400 palabras en promedio para los primeros dos borradores y 60 con 500 palabras para el tercero).\n",
       "\n",
       "2. **An√°lisis econ√≥mico y sostenibilidad**:  \n",
       "   - Se citan estudios sobre temas como inversi√≥n extranjera directa (FDI), desarrollo econ√≥mico, cambio clim√°tico y reforma fiscal, con enfoque en pa√≠ses en desarrollo y √Åfrica subsahariana.  \n",
       "   - Se mencionan modelos te√≥ricos (ej.: Agosin & Machado, 2005) para analizar relaci√≥n entre FDI y estimaci√≥n de inversi√≥n, usando datos de la FAOSTAT y clasificaciones de las Naciones Unidas.  \n",
       "   - Regiones y autores relevantes: India, √Åfrica subsahariana, y referencias a trabajos de C. K. Chase-Dunn, V. Tanzi, y F. Bekun.  \n",
       "\n",
       "**Fuente de datos**: Se utilizan datos de la FAO y bases de la ONU (2020) para apoyar an√°lisis y conclusiones.  \n",
       "**Aspectos destacados**: Uso de subt√≠tulos como \"Conclusiones\" en secciones de pol√≠ticas, y verificaci√≥n de la reproducibilidad de resultados mediante evidencia respaldada por datos.  \n",
       "\n",
       "*No se puede identificar la respuesta directa a la solicitud de resumen en la pontencia debido a que la consulta no se dirige espec√≠ficamente a uno de los contextos proporcionados. El resumen sintetiza los temas principales del documento fragmentado.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "question = \"Realizame un resumen del documento\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "context_str = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "response_gemini = generate_response(gemini_model, 'gemini-2.0-flash', question, context_str, 'gemini')\n",
    "display(Markdown(f\"**Respuesta de Gemini:**\\n{response_gemini}\"))\n",
    "\n",
    "response_llama = generate_response(groq_client, llama_model_name, question, context_str, 'groq')\n",
    "display(Markdown(f\"**Respuesta de Llama (Groq):**\\n{response_llama}\"))\n",
    "\n",
    "response_qwen = generate_response(open_router_client, qwen_model_name, question, context_str, 'openrouter')\n",
    "display(Markdown(f\"**Respuesta de Qwen (OpenRouter):**\\n{response_qwen}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a93e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n",
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "peer_qa = load_dataset(\n",
    "    \"UKPLab/PeerQA\",\n",
    "    \"qa\",\n",
    ")\n",
    "\n",
    "peer_papers = load_dataset(\n",
    "    \"UKPLab/PeerQA\",\n",
    "    \"papers\",\n",
    "    split='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af0ab4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n",
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset de preguntas + respuestas\n",
    "peerqa_ds = load_dataset(\"UKPLab/PeerQA\", \"qa\", split=\"test\")\n",
    "\n",
    "# Dataset de papers (documentos fuente)\n",
    "papers_ds = load_dataset(\"UKPLab/PeerQA\", \"papers\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c3fb2",
   "metadata": {},
   "source": [
    "## 6. Preparaci√≥n de los datos para la evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d621bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 579 ejemplos...\n",
      "Procesando ejemplo 0/579\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth v√°lido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "\n",
      "N√∫mero final de ejemplos v√°lidos: 267\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset\n",
    "qa = load_dataset(\"UKPLab/PeerQA\", \"qa\")[\"test\"]\n",
    "\n",
    "# Funci√≥n mejorada para validar el ground truth\n",
    "def is_valid_text(val):\n",
    "    # Verificar si es None\n",
    "    if val is None:\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es NaN float\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es pandas NaN\n",
    "    if pd.isna(val):\n",
    "        return False\n",
    "    \n",
    "    # Convertir a string y verificar\n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "    # Verificar si es string vac√≠o o variaciones de \"nan\"\n",
    "    if val_str == \"\" or val_str in [\"nan\", \"none\", \"null\", \"<na>\", \"n/a\"]:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Extraer ground truth con prioridad y validar\n",
    "def get_ground_truth(example):\n",
    "    for key in [\"answer_free_form_augmented\", \"answer_free_form\"]:\n",
    "        val = example.get(key)\n",
    "        if is_valid_text(val):\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "# Funci√≥n para validar contextos\n",
    "def is_valid_context(context):\n",
    "    if not context:\n",
    "        return False\n",
    "    if isinstance(context, list):\n",
    "        # Verificar que la lista no est√© vac√≠a y tenga elementos v√°lidos\n",
    "        return len(context) > 0 and all(is_valid_text(item) for item in context)\n",
    "    return is_valid_text(context)\n",
    "\n",
    "# Construir dataset filtrado con m√°s debugging\n",
    "examples = []\n",
    "total_examples = len(qa)\n",
    "filtered_stats = {\n",
    "    \"not_answerable\": 0,\n",
    "    \"invalid_paper_id\": 0,\n",
    "    \"no_ground_truth\": 0,\n",
    "    \"invalid_context\": 0,\n",
    "    \"valid\": 0\n",
    "}\n",
    "\n",
    "print(f\"Procesando {total_examples} ejemplos...\")\n",
    "\n",
    "for i, ex in enumerate(qa):\n",
    "    # Debug cada 1000 ejemplos\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Procesando ejemplo {i}/{total_examples}\")\n",
    "    \n",
    "    # Verificar si es answerable\n",
    "    if not ex[\"answerable\"]:\n",
    "        filtered_stats[\"not_answerable\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Verificar paper_id\n",
    "    if not is_valid_text(ex[\"paper_id\"]):\n",
    "        filtered_stats[\"invalid_paper_id\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Obtener ground truth\n",
    "    ground_truth = get_ground_truth(ex)\n",
    "    if not ground_truth:\n",
    "        filtered_stats[\"no_ground_truth\"] += 1\n",
    "        # Debug: imprimir algunos casos problem√°ticos\n",
    "        if filtered_stats[\"no_ground_truth\"] <= 5:\n",
    "            print(f\"Ejemplo sin ground_truth v√°lido:\")\n",
    "            print(f\"  answer_free_form_augmented: {repr(ex.get('answer_free_form_augmented'))}\")\n",
    "            print(f\"  answer_free_form: {repr(ex.get('answer_free_form'))}\")\n",
    "        continue\n",
    "    \n",
    "    # Verificar contexto\n",
    "    context = ex[\"answer_evidence_sent\"] or ex[\"raw_answer_evidence\"]\n",
    "    if not is_valid_context(context):\n",
    "        filtered_stats[\"invalid_context\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Si llegamos aqu√≠, el ejemplo es v√°lido\n",
    "    filtered_stats[\"valid\"] += 1\n",
    "    examples.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"contexts\": context,\n",
    "        \"paper_id\": ex[\"paper_id\"]\n",
    "    })\n",
    "\n",
    "# Crear dataset limpio\n",
    "qa_clean_dataset = Dataset.from_list(examples)\n",
    "\n",
    "print(f\"\\nN√∫mero final de ejemplos v√°lidos: {len(qa_clean_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea4054",
   "metadata": {},
   "source": [
    "## 7. Clase orquestadora de la evaluaci√≥n por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edaa1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class RAGEvaluationGenerator:\n",
    "    \"\"\"\n",
    "    Clase para generar respuestas y dataset RAGAS usando RAGPipeline.\n",
    "    Maneja indexaci√≥n de documentos y procesamiento en lotes.\n",
    "    \"\"\"\n",
    "    def __init__(self, rag_pipeline, clean_dataset, batch_size=3, delay=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rag_pipeline: Instancia de RAGPipeline.\n",
    "            clean_dataset: Dataset limpio (qa_clean_dataset).\n",
    "            batch_size: Tama√±o del lote para procesamiento.\n",
    "            delay: Segundos de espera entre lotes.\n",
    "        \"\"\"\n",
    "        self.rag = rag_pipeline\n",
    "        self.dataset = clean_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.delay = delay\n",
    "        self.results_file = \"rag_evaluation_results.json\"\n",
    "        self.results = self.load_previous_results()\n",
    "        print(f\"‚úÖ Inicializado RAGEvaluationGenerator con {len(clean_dataset)} ejemplos\")\n",
    "\n",
    "    def load_previous_results(self):\n",
    "        \"\"\"Cargar resultados previos para reanudar.\"\"\"\n",
    "        if os.path.exists(self.results_file):\n",
    "            with open(self.results_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return []\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Guardar resultados incrementalmente.\"\"\"\n",
    "        with open(self.results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def setup_retriever(self):\n",
    "        \"\"\"Configurar el retriever PeerQA (equivalente a index_documents).\"\"\"\n",
    "        print(\"Configurando retriever PeerQA...\")\n",
    "        try:\n",
    "            # Tu m√©todo ya no retorna nada, solo configura self.rag.retriever\n",
    "            self.rag.setup_peerqa_retriever()\n",
    "            print(\"‚úÖ Retriever configurado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error configurando retriever: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def generate_responses_batch(self, model_type='groq', start_idx=None, max_samples=30):\n",
    "        \"\"\"\n",
    "        Generar respuestas en lotes.\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'groq', 'gemini' o 'openrouter'.\n",
    "            start_idx: √çndice inicial (None para continuar desde resultados previos).\n",
    "            max_samples: M√°ximo n√∫mero de muestras a procesar.\n",
    "        \"\"\"\n",
    "        # Verificar que el retriever est√© configurado\n",
    "        if not self.rag.retriever:\n",
    "            print(\"‚ö†Ô∏è Retriever no configurado. Configurando autom√°ticamente...\")\n",
    "            if not self.setup_retriever():\n",
    "                raise ValueError(\"No se pudo configurar el retriever\")\n",
    "\n",
    "        total_samples = len(self.dataset)\n",
    "        if start_idx is None:\n",
    "            start_idx = len(self.results)\n",
    "\n",
    "        end_idx = min(start_idx + max_samples, total_samples) if max_samples else total_samples\n",
    "\n",
    "        print(f\"\\nüöÄ Generando respuestas con modelo: {model_type}\")\n",
    "        print(f\"üìã Procesando desde √≠ndice {start_idx} hasta {end_idx}\")\n",
    "        print(f\"üì¶ Lotes de {self.batch_size} con {self.delay}s de espera\")\n",
    "        print(f\"üìä Ya procesados: {len(self.results)} ejemplos\")\n",
    "\n",
    "        for i in range(start_idx, end_idx, self.batch_size):\n",
    "            batch_end = min(i + self.batch_size, end_idx)\n",
    "            current_batch = range(i, batch_end)\n",
    "\n",
    "            print(f\"\\nüîÑ Procesando lote {i//self.batch_size + 1}: √≠ndices {i}-{batch_end-1}\")\n",
    "\n",
    "            for idx in current_batch:\n",
    "                try:\n",
    "                    example = self.dataset[idx]\n",
    "                    question = example['question']\n",
    "                    print(f\"[{idx+1}/{end_idx}] Procesando: {question[:50]}...\")\n",
    "\n",
    "                    # Generar respuesta con RAGPipeline\n",
    "                    rag_result = self.rag.query_rag(\n",
    "                        question=question, \n",
    "                        model_type=model_type,\n",
    "                        max_context_length=3000\n",
    "                    )\n",
    "\n",
    "                    # Preparar resultado para RAGAS\n",
    "                    result = {\n",
    "                        'question': question,\n",
    "                        'answer': rag_result['answer'],\n",
    "                        'contexts': rag_result['contexts'],  # Ya viene limitado a top 5\n",
    "                        'ground_truth': example['ground_truth'],\n",
    "                        'paper_id': example.get('paper_id', 'unknown'),\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.datetime.now().isoformat(),\n",
    "                        'processed_idx': idx,\n",
    "                        'context_used_length': len(rag_result['context_used']),\n",
    "                        'retrieved_docs_count': len(rag_result['retrieved_docs'])\n",
    "                    }\n",
    "                    self.results.append(result)\n",
    "                    print(f\"  ‚úÖ Completado! (Contexto: {result['context_used_length']} chars)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error en √≠ndice {idx}: {str(e)}\")\n",
    "                    error_result = {\n",
    "                        'question': self.dataset[idx]['question'] if idx < len(self.dataset) else \"Error loading\",\n",
    "                        'answer': f\"ERROR: {str(e)}\",\n",
    "                        'contexts': [],\n",
    "                        'ground_truth': self.dataset[idx].get('ground_truth', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'paper_id': self.dataset[idx].get('paper_id', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.datetime.now().isoformat(),\n",
    "                        'processed_idx': idx,\n",
    "                        'error': True\n",
    "                    }\n",
    "                    self.results.append(error_result)\n",
    "\n",
    "            # Guardar progreso despu√©s de cada lote\n",
    "            self.save_results()\n",
    "            print(f\"üíæ Progreso guardado: {len(self.results)} ejemplos completados\")\n",
    "\n",
    "            # Esperar entre lotes (excepto en el √∫ltimo)\n",
    "            if batch_end < end_idx:\n",
    "                print(f\"‚è≥ Esperando {self.delay} segundos antes del siguiente lote...\")\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        print(f\"\\nüéâ ¬°Proceso completado! Total: {len(self.results)} ejemplos procesados\")\n",
    "        return self.results\n",
    "\n",
    "    def get_ragas_dataset(self, filter_errors=True):\n",
    "        \"\"\"\n",
    "        Crear dataset para RAGAS.\n",
    "        \n",
    "        Args:\n",
    "            filter_errors: Excluir ejemplos con errores.\n",
    "        \"\"\"\n",
    "        clean_results = [r for r in self.results if not r.get('error', False)] if filter_errors else self.results\n",
    "\n",
    "        if not clean_results:\n",
    "            print(\"‚ùå No hay resultados v√°lidos para crear dataset\")\n",
    "            return None\n",
    "\n",
    "        ragas_data = {\n",
    "            'question': [r['question'] for r in clean_results],\n",
    "            'answer': [r['answer'] for r in clean_results],\n",
    "            'contexts': [r['contexts'] for r in clean_results],\n",
    "            'ground_truth': [r['ground_truth'] for r in clean_results]\n",
    "        }\n",
    "\n",
    "        dataset = Dataset.from_dict(ragas_data)\n",
    "        print(f\"üìä Dataset RAGAS creado exitosamente: {len(dataset)} ejemplos\")\n",
    "        return dataset\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Resumen del progreso actual.\"\"\"\n",
    "        if not self.results:\n",
    "            return \"üì≠ No hay resultados a√∫n\"\n",
    "\n",
    "        total = len(self.results)\n",
    "        errors = sum(1 for r in self.results if r.get('error', False))\n",
    "        successful = total - errors\n",
    "        \n",
    "        # Contar modelos usados\n",
    "        models_used = {}\n",
    "        for r in self.results:\n",
    "            model = r.get('model_used', 'unknown')\n",
    "            models_used[model] = models_used.get(model, 0) + 1\n",
    "\n",
    "        # Estad√≠sticas adicionales\n",
    "        avg_context_length = 0\n",
    "        if successful > 0:\n",
    "            context_lengths = [r.get('context_used_length', 0) for r in self.results if not r.get('error', False)]\n",
    "            avg_context_length = sum(context_lengths) / len(context_lengths) if context_lengths else 0\n",
    "\n",
    "        return f\"\"\"\n",
    "üìä RESUMEN DEL PROGRESO:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "üìà Total procesado: {total}\n",
    "‚úÖ Exitosos: {successful}\n",
    "‚ùå Con errores: {errors}\n",
    "ü§ñ Modelos usados: {dict(models_used)}\n",
    "üìö Dataset original: {len(self.dataset)} ejemplos\n",
    "üìä Progreso: {(total/len(self.dataset)*100):.1f}%\n",
    "üéØ Contexto promedio: {avg_context_length:.0f} caracteres\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "        \"\"\"\n",
    "\n",
    "    def get_failed_examples(self):\n",
    "        \"\"\"Obtener ejemplos que fallaron para debugging.\"\"\"\n",
    "        failed = [r for r in self.results if r.get('error', False)]\n",
    "        if not failed:\n",
    "            return \"‚úÖ No hay ejemplos fallidos\"\n",
    "        \n",
    "        return f\"‚ùå {len(failed)} ejemplos fallidos:\\n\" + \"\\n\".join([\n",
    "            f\"  - √çndice {r['processed_idx']}: {r['answer'][:100]}...\"\n",
    "            for r in failed[:5]  # Mostrar solo los primeros 5\n",
    "        ])\n",
    "\n",
    "    def export_results(self, filename=None):\n",
    "        \"\"\"Exportar resultados a un archivo espec√≠fico.\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"rag_evaluation_export_{timestamp}.json\"\n",
    "        \n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'total_examples': len(self.results),\n",
    "                'successful': len([r for r in self.results if not r.get('error', False)]),\n",
    "                'failed': len([r for r in self.results if r.get('error', False)]),\n",
    "                'export_timestamp': datetime.datetime.now().isoformat(),\n",
    "                'dataset_size': len(self.dataset)\n",
    "            },\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"üì§ Resultados exportados a: {filename}\")\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690425a",
   "metadata": {},
   "source": [
    "## 8. Clase de la RAG intermedia preparada y adpatada para la evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb31fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from unstructured.partition.auto import partition\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, chroma_host=\"localhost\", chroma_port=8000):\n",
    "        load_dotenv('../.env')\n",
    "        self.groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        self.gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.open_router_client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=os.getenv(\"OPENROUTE_API_KEY\")\n",
    "        )\n",
    "        self.llama_model_name = \"llama-3.3-70b-versatile\"\n",
    "        self.qwen_model_name = \"qwen/qwen2-72b-instruct\"\n",
    "        self.retriever = None\n",
    "        \n",
    "        # Configuraci√≥n de Chroma Client para Docker\n",
    "        try:\n",
    "            self.chroma_client = chromadb.HttpClient(\n",
    "                host=chroma_host,\n",
    "                port=chroma_port,\n",
    "                settings=Settings(allow_reset=True)\n",
    "            )\n",
    "            # Test de conectividad\n",
    "            self.chroma_client.heartbeat()\n",
    "            print(f\"‚úÖ Conectado a Chroma en {chroma_host}:{chroma_port}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a Chroma: {e}\")\n",
    "            print(f\"üí° Aseg√∫rate de que Chroma est√© corriendo en {chroma_host}:{chroma_port}\")\n",
    "            print(\"   Comando: docker run -d --name chroma-rag -p 8000:8000 -v chroma_rag_data:/chroma/chroma chromadb/chroma:latest\")\n",
    "            raise\n",
    "        \n",
    "        self.collection_name = \"peerqa_papers\"\n",
    "\n",
    "    def setup_peerqa_retriever(self):\n",
    "        \"\"\"Configura el retriever espec√≠ficamente para PeerQA usando Chroma Docker\"\"\"\n",
    "        chunks_path = \"../data/peerqa_chunks.pkl\"\n",
    "        \n",
    "        # Verificar si ya existe la colecci√≥n en Chroma\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            collection_count = collection.count()\n",
    "            print(f\"Colecci√≥n '{self.collection_name}' ya existe con {collection_count} documentos\")\n",
    "            \n",
    "            # Si existe la colecci√≥n y tenemos chunks guardados, cargar directamente\n",
    "            if collection_count > 0 and os.path.exists(chunks_path):\n",
    "                print(\"Cargando chunks desde archivo...\")\n",
    "                with open(chunks_path, \"rb\") as f:\n",
    "                    chunks = pickle.load(f)\n",
    "                \n",
    "                self.retriever = self.crear_retriever_desde_chunks(chunks)\n",
    "                print(\"Retriever cargado correctamente desde datos existentes!\")\n",
    "                return self.retriever\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Colecci√≥n no existe o error: {e}\")\n",
    "            print(\"Creando nueva colecci√≥n...\")\n",
    "        \n",
    "        # Procesar dataset si no existe\n",
    "        print(\"Procesando dataset PeerQA...\")\n",
    "        dataset = load_dataset(\"UKPLab/PeerQA\", \"papers\")\n",
    "        papers = dataset['test']\n",
    "        print(\"Procesando papers a chunks...\")\n",
    "        chunks = self.procesar_dataset_peerqa_a_chunks_optimizado(papers)\n",
    "\n",
    "        # Guardar chunks en disco\n",
    "        print(f\"Guardando {len(chunks)} chunks en disco...\")\n",
    "        os.makedirs(os.path.dirname(chunks_path), exist_ok=True)\n",
    "        with open(chunks_path, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        print(\"Creando retriever...\")\n",
    "        self.retriever = self.crear_retriever_desde_chunks(chunks)\n",
    "        print(\"Retriever configurado correctamente!\")\n",
    "        return self.retriever\n",
    "\n",
    "    def crear_retriever_desde_chunks(self, chunks):\n",
    "        \"\"\"Crear retriever usando Chroma Docker\"\"\"\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Usando device: {device}\")\n",
    "\n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        # Crear vector store con Chroma Docker\n",
    "        print(\"Configurando vector store con Chroma Docker...\")\n",
    "        vector_store = Chroma(\n",
    "            client=self.chroma_client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=embeddings_model\n",
    "        )\n",
    "        \n",
    "        # Verificar si la colecci√≥n ya tiene documentos\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            if collection.count() == 0:\n",
    "                print(\"Agregando documentos a la colecci√≥n...\")\n",
    "                vector_store.add_documents(chunks)\n",
    "                print(f\"Agregados {len(chunks)} documentos a Chroma\")\n",
    "            else:\n",
    "                print(f\"La colecci√≥n ya tiene {collection.count()} documentos\")\n",
    "        except:\n",
    "            print(\"Creando colecci√≥n y agregando documentos...\")\n",
    "            vector_store.add_documents(chunks)\n",
    "            print(f\"Agregados {len(chunks)} documentos a Chroma\")\n",
    "\n",
    "        # Crear BM25 retriever\n",
    "        print(\"Creando BM25 retriever...\")\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks, k=7)\n",
    "        \n",
    "        # Crear vector retriever\n",
    "        print(\"Creando vector retriever...\")\n",
    "        chroma_retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 7}\n",
    "        )\n",
    "\n",
    "        # Crear ensemble retriever\n",
    "        print(\"Creando ensemble retriever...\")\n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[chroma_retriever, bm25_retriever],\n",
    "            weights=[0.7, 0.3]\n",
    "        )\n",
    "\n",
    "        return retriever\n",
    "\n",
    "    def procesar_dataset_peerqa_a_chunks_optimizado(self, papers_dataset):\n",
    "        \"\"\"Versi√≥n optimizada para PeerQA con chunks m√°s grandes y mejor metadata\"\"\"\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=512,  \n",
    "            chunk_overlap=150,  \n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  \n",
    "        )\n",
    "        \n",
    "        all_chunks = []\n",
    "        papers_by_id = {}\n",
    "        \n",
    "        # Agrupar por paper_id\n",
    "        for row in papers_dataset:\n",
    "            paper_id = row[\"paper_id\"]\n",
    "            if paper_id not in papers_by_id:\n",
    "                papers_by_id[paper_id] = []\n",
    "            papers_by_id[paper_id].append(row)\n",
    "        \n",
    "        for paper_id, sections in papers_by_id.items():\n",
    "            # Construir texto completo del paper\n",
    "            full_paper_text = \"\"\n",
    "            section_info = []\n",
    "            \n",
    "            for section in sections:\n",
    "                content = section[\"content\"].strip()\n",
    "                if content:\n",
    "                    section_type = section.get(\"type\", \"text\")\n",
    "                    last_heading = section.get(\"last_heading\", \"\")\n",
    "                    \n",
    "                    if section_type in [\"title\", \"heading\"] or last_heading:\n",
    "                        content = f\"[{section_type.upper()}] {content}\"\n",
    "                    \n",
    "                    full_paper_text += f\"{content}\\n\\n\"\n",
    "                    section_info.append({\n",
    "                        \"type\": section_type,\n",
    "                        \"heading\": last_heading,\n",
    "                        \"idx\": section.get(\"idx\", 0)\n",
    "                    })\n",
    "            \n",
    "            # Crear chunks para este paper\n",
    "            if full_paper_text.strip():\n",
    "                doc = Document(\n",
    "                    page_content=full_paper_text.strip(),\n",
    "                    metadata={\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"source\": \"peerqa\",\n",
    "                        \"sections_count\": len(sections)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                chunks = splitter.split_documents([doc])\n",
    "                \n",
    "                # Agregar metadata espec√≠fico a cada chunk\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk.metadata.update({\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"chunk_id\": f\"{paper_id}_{i}\",\n",
    "                        \"source\": \"peerqa\",\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    })\n",
    "                \n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"Procesados {len(papers_by_id)} papers √∫nicos en {len(all_chunks)} chunks\")\n",
    "        return all_chunks\n",
    "\n",
    "    def query_rag(self, question, model_type='groq', max_context_length=3000):\n",
    "        \"\"\"Funci√≥n principal para queries con l√≠mite de contexto\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Retriever no configurado. Ejecuta setup_peerqa_retriever() primero.\")\n",
    "        \n",
    "        # Recuperar documentos relevantes\n",
    "        retrieved_docs = self.retriever.invoke(question)\n",
    "        \n",
    "        # Construir contexto respetando l√≠mite\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        \n",
    "        for doc in retrieved_docs:\n",
    "            doc_text = doc.page_content\n",
    "            if total_length + len(doc_text) <= max_context_length:\n",
    "                context_parts.append(f\"Paper: {doc.metadata.get('paper_id', 'unknown')}\\n{doc_text}\")\n",
    "                total_length += len(doc_text)\n",
    "            else:\n",
    "                # Truncar si es necesario\n",
    "                remaining = max_context_length - total_length\n",
    "                if remaining > 100:  \n",
    "                    truncated = doc_text[:remaining] + \"...\"\n",
    "                    context_parts.append(f\"Paper: {doc.metadata.get('paper_id', 'unknown')}\\n{truncated}\")\n",
    "                break\n",
    "        \n",
    "        context_str = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "        \n",
    "        if model_type == 'groq':\n",
    "            response = self.generate_response(\n",
    "                self.groq_client, self.llama_model_name, question, context_str, 'groq'\n",
    "            )\n",
    "        elif model_type == 'gemini':\n",
    "            response = self.generate_response(\n",
    "                self.gemini_model, None, question, context_str, 'gemini'\n",
    "            )\n",
    "        elif model_type == 'openrouter':\n",
    "            response = self.generate_response(\n",
    "                self.open_router_client, self.qwen_model_name, question, context_str, 'openrouter'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"model_type debe ser 'groq', 'gemini' o 'openrouter'\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': response,\n",
    "            'contexts': [doc.page_content for doc in retrieved_docs[:5]],  # Solo top 5 para RAGAS\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context_str\n",
    "        }\n",
    "\n",
    "    def generate_response(self, client, model_name, user_query, context_str, client_type):\n",
    "        \"\"\"Prompt mejorado para evaluaci√≥n acad√©mica\"\"\"\n",
    "        system_prompt = \"\"\"Eres un asistente de investigaci√≥n experto en machine learning y procesamiento de lenguaje natural. \n",
    "\n",
    "INSTRUCCIONES:\n",
    "- Responde bas√°ndote √∫nicamente en el contexto proporcionado de los papers acad√©micos\n",
    "- Si tienes informaci√≥n suficiente, proporciona una respuesta completa y t√©cnicamente precisa\n",
    "- Cita conceptos, m√©todos o resultados espec√≠ficos cuando sea relevante\n",
    "- Si la informaci√≥n es parcial, responde con lo disponible y especifica qu√© no puedes determinar\n",
    "- Solo di \"La informaci√≥n proporcionada no es suficiente\" si el contexto es completamente irrelevante\n",
    "- Usa terminolog√≠a t√©cnica apropiada\n",
    "- S√© conciso pero completo\"\"\"\n",
    "\n",
    "        full_prompt = f\"Contexto de papers acad√©micos:\\n{context_str}\\n\\nPregunta: {user_query}\\n\\nRespuesta basada en el contexto:\"\n",
    "        \n",
    "        try:\n",
    "            if client_type == 'gemini':\n",
    "                response = client.generate_content(full_prompt)\n",
    "                return response.text\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": full_prompt}\n",
    "                ]\n",
    "                if client_type == 'groq':\n",
    "                    chat_completion = client.chat.completions.create(\n",
    "                        messages=messages, \n",
    "                        model=model_name,\n",
    "                        temperature=0.1,  \n",
    "                        max_tokens=1000\n",
    "                    )\n",
    "                elif client_type == 'openrouter':\n",
    "                    chat_completion = client.chat.completions.create(\n",
    "                        messages=messages, \n",
    "                        model=model_name,\n",
    "                        temperature=0.1,\n",
    "                        max_tokens=1000\n",
    "                    )\n",
    "                return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error al generar respuesta: {str(e)}\"\n",
    "\n",
    "    def reset_collection(self):\n",
    "        \"\"\"M√©todo para resetear la colecci√≥n si es necesario\"\"\"\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(name=self.collection_name)\n",
    "            print(f\"Colecci√≥n '{self.collection_name}' eliminada\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al eliminar colecci√≥n: {e}\")\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"M√©todo para obtener informaci√≥n sobre la colecci√≥n\"\"\"\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            count = collection.count()\n",
    "            print(f\"Colecci√≥n '{self.collection_name}' tiene {count} documentos\")\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            print(f\"Error al obtener informaci√≥n de la colecci√≥n: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def procesar_pdf_a_chunks(self, pdf_path):\n",
    "        \"\"\"M√©todo original para PDFs (mantenido para compatibilidad)\"\"\"\n",
    "        elements = partition(filename=pdf_path)\n",
    "        pages = defaultdict(list)\n",
    "        for el in elements:\n",
    "            if el.text.strip():\n",
    "                metadata = el.metadata.to_dict()\n",
    "                page = metadata.get('page_number')\n",
    "                pages[page].append({\n",
    "                    'Texto': el.text.strip(),\n",
    "                    'Nombre del documento': metadata.get('filename'),\n",
    "                    'Idioma': metadata.get('languages', ['unknown'])[0],\n",
    "                })\n",
    "        all_chunks = []\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        for page_number, elementos in pages.items():\n",
    "            full_text = \" \".join(el['Texto'] for el in elementos)\n",
    "            base_metadata = {\n",
    "                'N√∫mero de pagina': page_number,\n",
    "                'Nombre del documento': elementos[0]['Nombre del documento'],\n",
    "                'Idioma': elementos[0]['Idioma'],\n",
    "            }\n",
    "            doc = Document(page_content=full_text, metadata=base_metadata)\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            all_chunks.extend(chunks)\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1719c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a Chroma en localhost:8000\n",
      "Colecci√≥n 'peerqa_papers' ya existe con 12844 documentos\n",
      "Procesando dataset PeerQA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando papers a chunks...\n",
      "Procesados 90 papers √∫nicos en 9732 chunks\n",
      "Guardando 9732 chunks en disco...\n",
      "Total chunks: 9732\n",
      "Creando retriever...\n",
      "Usando device: cuda\n",
      "Configurando vector store con Chroma Docker...\n",
      "La colecci√≥n ya tiene 12844 documentos\n",
      "Creando BM25 retriever...\n",
      "Creando vector retriever...\n",
      "Creando ensemble retriever...\n",
      "Retriever configurado correctamente!\n",
      "‚úÖ Inicializado RAGEvaluationGenerator con 267 ejemplos\n",
      "\n",
      "üöÄ Generando respuestas con modelo: gemini\n",
      "üìã Procesando desde √≠ndice 8 hasta 12\n",
      "üì¶ Lotes de 3 con 30s de espera\n",
      "üìä Ya procesados: 8 ejemplos\n",
      "\n",
      "üîÑ Procesando lote 3: √≠ndices 8-10\n",
      "[9/12] Procesando: Can the proposed algorithm be used to recover real...\n",
      "  ‚úÖ Completado! (Contexto: 3551 chars)\n",
      "[10/12] Procesando: Can the pairwise distance of the latent variables ...\n",
      "  ‚úÖ Completado! (Contexto: 3456 chars)\n",
      "[11/12] Procesando: Can the parameters of the BLOSUM matrix be estimat...\n",
      "  ‚úÖ Completado! (Contexto: 3181 chars)\n",
      "üíæ Progreso guardado: 11 ejemplos completados\n",
      "‚è≥ Esperando 30 segundos antes del siguiente lote...\n",
      "\n",
      "üîÑ Procesando lote 4: √≠ndices 11-11\n",
      "[12/12] Procesando: Does the same preprocessing step of converting num...\n",
      "  ‚úÖ Completado! (Contexto: 3322 chars)\n",
      "üíæ Progreso guardado: 12 ejemplos completados\n",
      "\n",
      "üéâ ¬°Proceso completado! Total: 12 ejemplos procesados\n",
      "\n",
      "üìä RESUMEN DEL PROGRESO:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üìà Total procesado: 12\n",
      "‚úÖ Exitosos: 12\n",
      "‚ùå Con errores: 0\n",
      "ü§ñ Modelos usados: {'gemini': 12}\n",
      "üìö Dataset original: 267 ejemplos\n",
      "üìä Progreso: 4.5%\n",
      "üéØ Contexto promedio: 3470 caracteres\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "        \n",
      "üìä Dataset RAGAS creado exitosamente: 12 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# 1. Inicializar tu RAGPipeline\n",
    "rag = RAGPipeline()  # Sin par√°metros Chroma ya que usa archivos locales\n",
    "\n",
    "# Setup (solo la primera vez, despu√©s ya est√° persistido)\n",
    "rag.setup_peerqa_retriever()\n",
    "\n",
    "# 2. Crear el generador (mismo c√≥digo que ten√≠as)\n",
    "generator = RAGEvaluationGenerator(\n",
    "    rag_pipeline=rag,\n",
    "    clean_dataset=qa_clean_dataset,  # Tu dataset filtrado\n",
    "    batch_size=3,    # Solo 3 por lote (m√°s conservador)\n",
    "    delay=30         # 30 segundos entre lotes (no 90)\n",
    ")\n",
    "\n",
    "# 3. Procesar de a poquito (empezar con 4 ejemplos de prueba)\n",
    "generator.generate_responses_batch(\n",
    "    model_type='gemini',\n",
    "    max_samples=4  # Solo 4 para probar\n",
    ")\n",
    "\n",
    "# 4. Ver progreso\n",
    "print(generator.get_summary())\n",
    "\n",
    "# 5. Cuando est√© listo, crear dataset para RAGAS\n",
    "ragas_dataset = generator.get_ragas_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuevo_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
