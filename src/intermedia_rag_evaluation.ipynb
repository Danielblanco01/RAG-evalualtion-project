{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8bf724",
   "metadata": {},
   "source": [
    "## 1. Carga y Procesamiento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dd63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from collections import defaultdict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23020a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "def unstructured_extraction(pdf_path:str):\n",
    "    elements = partition(filename=pdf_path)\n",
    "    return elements\n",
    "\n",
    "pdf_path = r\"C:/Users/danie/Downloads/PaperMind/backend/data/raw/n28a12.pdf\"\n",
    "elements = unstructured_extraction(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd15c1d",
   "metadata": {},
   "source": [
    "## 2. Chunking con Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df5673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = defaultdict(list)\n",
    "for el in elements:\n",
    "    if el.text.strip():\n",
    "        metadata = el.metadata.to_dict()\n",
    "        page = metadata.get('page_number')\n",
    "        pages[page].append({\n",
    "            'Texto': el.text.strip(),\n",
    "            'Nombre del documento': metadata.get('filename'),\n",
    "            'Idioma': metadata.get('languages', ['unknown'])[0],\n",
    "        })\n",
    "all_chunks = []\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "for page_number, elementos in pages.items():\n",
    "    full_text = \" \" .join(el['Texto'] for el in elementos)\n",
    "    base_metadata = {\n",
    "        'Número de pagina': page_number,\n",
    "        'Nombre del documento': elementos[0]['Nombre del documento'],\n",
    "        'Idioma': elementos[0]['Idioma'],\n",
    "    }\n",
    "    doc = Document(page_content=full_text, metadata=base_metadata)\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7923a",
   "metadata": {},
   "source": [
    "## 3. Vector Store y Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c56dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "vector_store = Chroma.from_documents(all_chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dca2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(all_chunks, k=5)\n",
    "chroma_retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092202",
   "metadata": {},
   "source": [
    "## 4. Inicialización de Clientes y Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3574b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "open_router_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_KEY\")\n",
    ")\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "llama_model_name = \"llama-3.3-70b-versatile\"\n",
    "qwen_model_name = \"qwen/qwen3-32b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ded186",
   "metadata": {},
   "source": [
    "## 5. Generación de Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe12739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(client, model_name, user_query, context_str, client_type):\n",
    "    system_prompt = \"Eres un asistente que responde preguntas basándote únicamente en el contexto proporcionado. Si la respuesta no está en el contexto, di que no tienes suficiente información.\"\n",
    "    full_prompt = f\"Contexto:\\n{context_str}\\n\\nPregunta: {user_query}\"\n",
    "\n",
    "    try:\n",
    "        if client_type == 'gemini':\n",
    "            response = client.generate_content(full_prompt)\n",
    "            return response.text\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ]\n",
    "            if client_type == 'groq':\n",
    "                chat_completion = client.chat.completions.create(messages=messages, model=model_name)\n",
    "            elif client_type == 'openrouter':\n",
    "                chat_completion = client.chat.completions.create(messages=messages, model=model_name)\n",
    "            return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error al generar respuesta con {model_name}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2823a9",
   "metadata": {},
   "source": [
    "## 6. Ejecución y Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab85fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Gemini:**\n",
       "Este documento trata sobre el desarrollo de un corpus (un conjunto de textos) de ensayos académicos revisados.\n",
       "\n",
       "**Puntos clave:**\n",
       "\n",
       "*   Los participantes escribieron ensayos que fueron revisados en múltiples borradores (Draft 1, Draft 2 y Draft 3).\n",
       "*   Se utilizó una interfaz de computadora para resaltar las diferencias entre los borradores iniciales y las revisiones.\n",
       "*   El corpus contiene 180 ensayos en total.\n",
       "*   Se alinearon las oraciones entre los borradores y se etiquetaron los propósitos de las revisiones.\n",
       "\n",
       "Además, el documento hace referencia a otros estudios y datos relacionados con temas como la inversión extranjera directa (FDI), el desarrollo económico y las emisiones contaminantes, mencionando a autores como Chase-Dunn, Tanzi, Ohlin, Bekun, Agosin & Machado entre otros y entidades como las Naciones Unidas y la FAO.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Llama (Groq):**\n",
       "El documento parece ser una recopilación de diferentes secciones y referencias de artículos y publicaciones académicas. A continuación, te presento un resumen de los temas y puntos clave que se mencionan:\n",
       "\n",
       "1. **Desarrollo de un corpus**: Se describe un procedimiento para mejorar la calidad de ensayos a través de revisiones y creación de borradores. Se menciona que se alignmentaron oraciones a lo largo de los borradores y se etiquetaron los propósitos de revisión.\n",
       "2. **Estadísticas descriptivas**: Se proporcionan estadísticas sobre un conjunto de datos que incluye 180 ensayos, con un promedio de 400 palabras en los dos primeros borradores y 500 palabras en el tercer borrador.\n",
       "3. **Bibliografía y referencias**: Se incluyen referencias a varios artículos y publicaciones académicas sobre temas como la economía, el desarrollo sostenible, la globalización y la política económica.\n",
       "4. **Modelos económicos**: Se menciona el modelo de Agosin y Machado (2005) para el estudio de la relación entre la inversión extranjera directa (IED) y la inversión doméstica.\n",
       "5. **Política y recomendaciones**: Se incluye una sección sobre recomendaciones de política y se menciona que los datos subyacentes están disponibles para garantizar la reproducibilidad de los resultados.\n",
       "6. **Desarrollo y energía**: Se abordan temas relacionados con el desarrollo sostenible, la energía renovable y la mitigación de emisiones en países como la India.\n",
       "\n",
       "En general, el documento parece ser una recopilación de diferentes temas y referencias relacionadas con la economía, el desarrollo sostenible y la política económica, con un enfoque en la inversión extranjera directa y el desarrollo de modelos económicos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Respuesta de Qwen (OpenRouter):**\n",
       "El documento presenta información sobre **dos líneas temáticas principales**:\n",
       "\n",
       "1. **Desarrollo de un corpus de ensayos académicos**:  \n",
       "   - Se describe un proceso de revisión iterativo en el que participantes mejoran sus ensayos a través de tres borradores (Draft1, Draft2, Draft3).  \n",
       "   - Se emplea una interfaz computacional para resaltar diferencias entre borradores y alinear frases revisadas, etiquetando los propósitos de las revisiones.  \n",
       "   - Estadísticas descriptivas: 180 ensayos en total (120 con 400 palabras en promedio para los primeros dos borradores y 60 con 500 palabras para el tercero).\n",
       "\n",
       "2. **Análisis económico y sostenibilidad**:  \n",
       "   - Se citan estudios sobre temas como inversión extranjera directa (FDI), desarrollo económico, cambio climático y reforma fiscal, con enfoque en países en desarrollo y África subsahariana.  \n",
       "   - Se mencionan modelos teóricos (ej.: Agosin & Machado, 2005) para analizar relación entre FDI y estimación de inversión, usando datos de la FAOSTAT y clasificaciones de las Naciones Unidas.  \n",
       "   - Regiones y autores relevantes: India, África subsahariana, y referencias a trabajos de C. K. Chase-Dunn, V. Tanzi, y F. Bekun.  \n",
       "\n",
       "**Fuente de datos**: Se utilizan datos de la FAO y bases de la ONU (2020) para apoyar análisis y conclusiones.  \n",
       "**Aspectos destacados**: Uso de subtítulos como \"Conclusiones\" en secciones de políticas, y verificación de la reproducibilidad de resultados mediante evidencia respaldada por datos.  \n",
       "\n",
       "*No se puede identificar la respuesta directa a la solicitud de resumen en la pontencia debido a que la consulta no se dirige específicamente a uno de los contextos proporcionados. El resumen sintetiza los temas principales del documento fragmentado.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "question = \"Realizame un resumen del documento\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "context_str = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "response_gemini = generate_response(gemini_model, 'gemini-2.0-flash', question, context_str, 'gemini')\n",
    "display(Markdown(f\"**Respuesta de Gemini:**\\n{response_gemini}\"))\n",
    "\n",
    "response_llama = generate_response(groq_client, llama_model_name, question, context_str, 'groq')\n",
    "display(Markdown(f\"**Respuesta de Llama (Groq):**\\n{response_llama}\"))\n",
    "\n",
    "response_qwen = generate_response(open_router_client, qwen_model_name, question, context_str, 'openrouter')\n",
    "display(Markdown(f\"**Respuesta de Qwen (OpenRouter):**\\n{response_qwen}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a93e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n",
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "peer_qa = load_dataset(\n",
    "    \"UKPLab/PeerQA\",\n",
    "    \"qa\",\n",
    ")\n",
    "\n",
    "peer_papers = load_dataset(\n",
    "    \"UKPLab/PeerQA\",\n",
    "    \"papers\",\n",
    "    split='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af0ab4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n",
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset de preguntas + respuestas\n",
    "peerqa_ds = load_dataset(\"UKPLab/PeerQA\", \"qa\", split=\"test\")\n",
    "\n",
    "# Dataset de papers (documentos fuente)\n",
    "papers_ds = load_dataset(\"UKPLab/PeerQA\", \"papers\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c3fb2",
   "metadata": {},
   "source": [
    "## 6. Preparación de los datos para la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d621bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'qa' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\qa\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Sun Jul 20 11:03:10 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 579 ejemplos...\n",
      "Procesando ejemplo 0/579\n",
      "Ejemplo sin ground_truth válido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth válido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth válido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth válido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "Ejemplo sin ground_truth válido:\n",
      "  answer_free_form_augmented: 'nan'\n",
      "  answer_free_form: None\n",
      "\n",
      "Número final de ejemplos válidos: 267\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset\n",
    "qa = load_dataset(\"UKPLab/PeerQA\", \"qa\")[\"test\"]\n",
    "\n",
    "# Función mejorada para validar el ground truth\n",
    "def is_valid_text(val):\n",
    "    # Verificar si es None\n",
    "    if val is None:\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es NaN float\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return False\n",
    "    \n",
    "    # Verificar si es pandas NaN\n",
    "    if pd.isna(val):\n",
    "        return False\n",
    "    \n",
    "    # Convertir a string y verificar\n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "    # Verificar si es string vacío o variaciones de \"nan\"\n",
    "    if val_str == \"\" or val_str in [\"nan\", \"none\", \"null\", \"<na>\", \"n/a\"]:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Extraer ground truth con prioridad y validar\n",
    "def get_ground_truth(example):\n",
    "    for key in [\"answer_free_form_augmented\", \"answer_free_form\"]:\n",
    "        val = example.get(key)\n",
    "        if is_valid_text(val):\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "# Función para validar contextos\n",
    "def is_valid_context(context):\n",
    "    if not context:\n",
    "        return False\n",
    "    if isinstance(context, list):\n",
    "        # Verificar que la lista no esté vacía y tenga elementos válidos\n",
    "        return len(context) > 0 and all(is_valid_text(item) for item in context)\n",
    "    return is_valid_text(context)\n",
    "\n",
    "# Construir dataset filtrado con más debugging\n",
    "examples = []\n",
    "total_examples = len(qa)\n",
    "filtered_stats = {\n",
    "    \"not_answerable\": 0,\n",
    "    \"invalid_paper_id\": 0,\n",
    "    \"no_ground_truth\": 0,\n",
    "    \"invalid_context\": 0,\n",
    "    \"valid\": 0\n",
    "}\n",
    "\n",
    "print(f\"Procesando {total_examples} ejemplos...\")\n",
    "\n",
    "for i, ex in enumerate(qa):\n",
    "    # Debug cada 1000 ejemplos\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Procesando ejemplo {i}/{total_examples}\")\n",
    "    \n",
    "    # Verificar si es answerable\n",
    "    if not ex[\"answerable\"]:\n",
    "        filtered_stats[\"not_answerable\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Verificar paper_id\n",
    "    if not is_valid_text(ex[\"paper_id\"]):\n",
    "        filtered_stats[\"invalid_paper_id\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Obtener ground truth\n",
    "    ground_truth = get_ground_truth(ex)\n",
    "    if not ground_truth:\n",
    "        filtered_stats[\"no_ground_truth\"] += 1\n",
    "        # Debug: imprimir algunos casos problemáticos\n",
    "        if filtered_stats[\"no_ground_truth\"] <= 5:\n",
    "            print(f\"Ejemplo sin ground_truth válido:\")\n",
    "            print(f\"  answer_free_form_augmented: {repr(ex.get('answer_free_form_augmented'))}\")\n",
    "            print(f\"  answer_free_form: {repr(ex.get('answer_free_form'))}\")\n",
    "        continue\n",
    "    \n",
    "    # Verificar contexto\n",
    "    context = ex[\"answer_evidence_sent\"] or ex[\"raw_answer_evidence\"]\n",
    "    if not is_valid_context(context):\n",
    "        filtered_stats[\"invalid_context\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # Si llegamos aquí, el ejemplo es válido\n",
    "    filtered_stats[\"valid\"] += 1\n",
    "    examples.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"contexts\": context,\n",
    "        \"paper_id\": ex[\"paper_id\"]\n",
    "    })\n",
    "\n",
    "# Crear dataset limpio\n",
    "qa_clean_dataset = Dataset.from_list(examples)\n",
    "\n",
    "print(f\"\\nNúmero final de ejemplos válidos: {len(qa_clean_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea4054",
   "metadata": {},
   "source": [
    "## 7. Clase orquestadora de la evaluación por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edaa1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class RAGEvaluationGenerator:\n",
    "    \"\"\"\n",
    "    Clase para generar respuestas y dataset RAGAS usando RAGPipeline.\n",
    "    Maneja indexación de documentos y procesamiento en lotes.\n",
    "    \"\"\"\n",
    "    def __init__(self, rag_pipeline, clean_dataset, batch_size=3, delay=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rag_pipeline: Instancia de RAGPipeline.\n",
    "            clean_dataset: Dataset limpio (qa_clean_dataset).\n",
    "            batch_size: Tamaño del lote para procesamiento.\n",
    "            delay: Segundos de espera entre lotes.\n",
    "        \"\"\"\n",
    "        self.rag = rag_pipeline\n",
    "        self.dataset = clean_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.delay = delay\n",
    "        self.results_file = \"rag_evaluation_results.json\"\n",
    "        self.results = self.load_previous_results()\n",
    "        print(f\"✅ Inicializado RAGEvaluationGenerator con {len(clean_dataset)} ejemplos\")\n",
    "\n",
    "    def load_previous_results(self):\n",
    "        \"\"\"Cargar resultados previos para reanudar.\"\"\"\n",
    "        if os.path.exists(self.results_file):\n",
    "            with open(self.results_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return []\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Guardar resultados incrementalmente.\"\"\"\n",
    "        with open(self.results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def setup_retriever(self):\n",
    "        \"\"\"Configurar el retriever PeerQA (equivalente a index_documents).\"\"\"\n",
    "        print(\"Configurando retriever PeerQA...\")\n",
    "        try:\n",
    "            # Tu método ya no retorna nada, solo configura self.rag.retriever\n",
    "            self.rag.setup_peerqa_retriever()\n",
    "            print(\"✅ Retriever configurado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error configurando retriever: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def generate_responses_batch(self, model_type='groq', start_idx=None, max_samples=30):\n",
    "        \"\"\"\n",
    "        Generar respuestas en lotes.\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'groq', 'gemini' o 'openrouter'.\n",
    "            start_idx: Índice inicial (None para continuar desde resultados previos).\n",
    "            max_samples: Máximo número de muestras a procesar.\n",
    "        \"\"\"\n",
    "        # Verificar que el retriever esté configurado\n",
    "        if not self.rag.retriever:\n",
    "            print(\"⚠️ Retriever no configurado. Configurando automáticamente...\")\n",
    "            if not self.setup_retriever():\n",
    "                raise ValueError(\"No se pudo configurar el retriever\")\n",
    "\n",
    "        total_samples = len(self.dataset)\n",
    "        if start_idx is None:\n",
    "            start_idx = len(self.results)\n",
    "\n",
    "        end_idx = min(start_idx + max_samples, total_samples) if max_samples else total_samples\n",
    "\n",
    "        print(f\"\\n🚀 Generando respuestas con modelo: {model_type}\")\n",
    "        print(f\"📋 Procesando desde índice {start_idx} hasta {end_idx}\")\n",
    "        print(f\"📦 Lotes de {self.batch_size} con {self.delay}s de espera\")\n",
    "        print(f\"📊 Ya procesados: {len(self.results)} ejemplos\")\n",
    "\n",
    "        for i in range(start_idx, end_idx, self.batch_size):\n",
    "            batch_end = min(i + self.batch_size, end_idx)\n",
    "            current_batch = range(i, batch_end)\n",
    "\n",
    "            print(f\"\\n🔄 Procesando lote {i//self.batch_size + 1}: índices {i}-{batch_end-1}\")\n",
    "\n",
    "            for idx in current_batch:\n",
    "                try:\n",
    "                    example = self.dataset[idx]\n",
    "                    question = example['question']\n",
    "                    print(f\"[{idx+1}/{end_idx}] Procesando: {question[:50]}...\")\n",
    "\n",
    "                    # Generar respuesta con RAGPipeline\n",
    "                    rag_result = self.rag.query_rag(\n",
    "                        question=question, \n",
    "                        model_type=model_type,\n",
    "                        max_context_length=3000\n",
    "                    )\n",
    "\n",
    "                    # Preparar resultado para RAGAS\n",
    "                    result = {\n",
    "                        'question': question,\n",
    "                        'answer': rag_result['answer'],\n",
    "                        'contexts': rag_result['contexts'],  # Ya viene limitado a top 5\n",
    "                        'ground_truth': example['ground_truth'],\n",
    "                        'paper_id': example.get('paper_id', 'unknown'),\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.datetime.now().isoformat(),\n",
    "                        'processed_idx': idx,\n",
    "                        'context_used_length': len(rag_result['context_used']),\n",
    "                        'retrieved_docs_count': len(rag_result['retrieved_docs'])\n",
    "                    }\n",
    "                    self.results.append(result)\n",
    "                    print(f\"  ✅ Completado! (Contexto: {result['context_used_length']} chars)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error en índice {idx}: {str(e)}\")\n",
    "                    error_result = {\n",
    "                        'question': self.dataset[idx]['question'] if idx < len(self.dataset) else \"Error loading\",\n",
    "                        'answer': f\"ERROR: {str(e)}\",\n",
    "                        'contexts': [],\n",
    "                        'ground_truth': self.dataset[idx].get('ground_truth', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'paper_id': self.dataset[idx].get('paper_id', 'N/A') if idx < len(self.dataset) else \"N/A\",\n",
    "                        'model_used': model_type,\n",
    "                        'timestamp': datetime.datetime.now().isoformat(),\n",
    "                        'processed_idx': idx,\n",
    "                        'error': True\n",
    "                    }\n",
    "                    self.results.append(error_result)\n",
    "\n",
    "            # Guardar progreso después de cada lote\n",
    "            self.save_results()\n",
    "            print(f\"💾 Progreso guardado: {len(self.results)} ejemplos completados\")\n",
    "\n",
    "            # Esperar entre lotes (excepto en el último)\n",
    "            if batch_end < end_idx:\n",
    "                print(f\"⏳ Esperando {self.delay} segundos antes del siguiente lote...\")\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        print(f\"\\n🎉 ¡Proceso completado! Total: {len(self.results)} ejemplos procesados\")\n",
    "        return self.results\n",
    "\n",
    "    def get_ragas_dataset(self, filter_errors=True):\n",
    "        \"\"\"\n",
    "        Crear dataset para RAGAS.\n",
    "        \n",
    "        Args:\n",
    "            filter_errors: Excluir ejemplos con errores.\n",
    "        \"\"\"\n",
    "        clean_results = [r for r in self.results if not r.get('error', False)] if filter_errors else self.results\n",
    "\n",
    "        if not clean_results:\n",
    "            print(\"❌ No hay resultados válidos para crear dataset\")\n",
    "            return None\n",
    "\n",
    "        ragas_data = {\n",
    "            'question': [r['question'] for r in clean_results],\n",
    "            'answer': [r['answer'] for r in clean_results],\n",
    "            'contexts': [r['contexts'] for r in clean_results],\n",
    "            'ground_truth': [r['ground_truth'] for r in clean_results]\n",
    "        }\n",
    "\n",
    "        dataset = Dataset.from_dict(ragas_data)\n",
    "        print(f\"📊 Dataset RAGAS creado exitosamente: {len(dataset)} ejemplos\")\n",
    "        return dataset\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Resumen del progreso actual.\"\"\"\n",
    "        if not self.results:\n",
    "            return \"📭 No hay resultados aún\"\n",
    "\n",
    "        total = len(self.results)\n",
    "        errors = sum(1 for r in self.results if r.get('error', False))\n",
    "        successful = total - errors\n",
    "        \n",
    "        # Contar modelos usados\n",
    "        models_used = {}\n",
    "        for r in self.results:\n",
    "            model = r.get('model_used', 'unknown')\n",
    "            models_used[model] = models_used.get(model, 0) + 1\n",
    "\n",
    "        # Estadísticas adicionales\n",
    "        avg_context_length = 0\n",
    "        if successful > 0:\n",
    "            context_lengths = [r.get('context_used_length', 0) for r in self.results if not r.get('error', False)]\n",
    "            avg_context_length = sum(context_lengths) / len(context_lengths) if context_lengths else 0\n",
    "\n",
    "        return f\"\"\"\n",
    "📊 RESUMEN DEL PROGRESO:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 Total procesado: {total}\n",
    "✅ Exitosos: {successful}\n",
    "❌ Con errores: {errors}\n",
    "🤖 Modelos usados: {dict(models_used)}\n",
    "📚 Dataset original: {len(self.dataset)} ejemplos\n",
    "📊 Progreso: {(total/len(self.dataset)*100):.1f}%\n",
    "🎯 Contexto promedio: {avg_context_length:.0f} caracteres\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "        \"\"\"\n",
    "\n",
    "    def get_failed_examples(self):\n",
    "        \"\"\"Obtener ejemplos que fallaron para debugging.\"\"\"\n",
    "        failed = [r for r in self.results if r.get('error', False)]\n",
    "        if not failed:\n",
    "            return \"✅ No hay ejemplos fallidos\"\n",
    "        \n",
    "        return f\"❌ {len(failed)} ejemplos fallidos:\\n\" + \"\\n\".join([\n",
    "            f\"  - Índice {r['processed_idx']}: {r['answer'][:100]}...\"\n",
    "            for r in failed[:5]  # Mostrar solo los primeros 5\n",
    "        ])\n",
    "\n",
    "    def export_results(self, filename=None):\n",
    "        \"\"\"Exportar resultados a un archivo específico.\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"rag_evaluation_export_{timestamp}.json\"\n",
    "        \n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'total_examples': len(self.results),\n",
    "                'successful': len([r for r in self.results if not r.get('error', False)]),\n",
    "                'failed': len([r for r in self.results if r.get('error', False)]),\n",
    "                'export_timestamp': datetime.datetime.now().isoformat(),\n",
    "                'dataset_size': len(self.dataset)\n",
    "            },\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"📤 Resultados exportados a: {filename}\")\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690425a",
   "metadata": {},
   "source": [
    "## 8. Clase de la RAG intermedia preparada y adpatada para la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb31fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from unstructured.partition.auto import partition\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, chroma_host=\"localhost\", chroma_port=8000):\n",
    "        load_dotenv('../.env')\n",
    "        self.groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        self.gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.open_router_client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=os.getenv(\"OPENROUTE_API_KEY\")\n",
    "        )\n",
    "        self.llama_model_name = \"llama-3.3-70b-versatile\"\n",
    "        self.qwen_model_name = \"qwen/qwen2-72b-instruct\"\n",
    "        self.retriever = None\n",
    "        \n",
    "        # Configuración de Chroma Client para Docker\n",
    "        try:\n",
    "            self.chroma_client = chromadb.HttpClient(\n",
    "                host=chroma_host,\n",
    "                port=chroma_port,\n",
    "                settings=Settings(allow_reset=True)\n",
    "            )\n",
    "            # Test de conectividad\n",
    "            self.chroma_client.heartbeat()\n",
    "            print(f\"✅ Conectado a Chroma en {chroma_host}:{chroma_port}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error conectando a Chroma: {e}\")\n",
    "            print(f\"💡 Asegúrate de que Chroma esté corriendo en {chroma_host}:{chroma_port}\")\n",
    "            print(\"   Comando: docker run -d --name chroma-rag -p 8000:8000 -v chroma_rag_data:/chroma/chroma chromadb/chroma:latest\")\n",
    "            raise\n",
    "        \n",
    "        self.collection_name = \"peerqa_papers\"\n",
    "\n",
    "    def setup_peerqa_retriever(self):\n",
    "        \"\"\"Configura el retriever específicamente para PeerQA usando Chroma Docker\"\"\"\n",
    "        chunks_path = \"../data/peerqa_chunks.pkl\"\n",
    "        \n",
    "        # Verificar si ya existe la colección en Chroma\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            collection_count = collection.count()\n",
    "            print(f\"Colección '{self.collection_name}' ya existe con {collection_count} documentos\")\n",
    "            \n",
    "            # Si existe la colección y tenemos chunks guardados, cargar directamente\n",
    "            if collection_count > 0 and os.path.exists(chunks_path):\n",
    "                print(\"Cargando chunks desde archivo...\")\n",
    "                with open(chunks_path, \"rb\") as f:\n",
    "                    chunks = pickle.load(f)\n",
    "                \n",
    "                self.retriever = self.crear_retriever_desde_chunks(chunks)\n",
    "                print(\"Retriever cargado correctamente desde datos existentes!\")\n",
    "                return self.retriever\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Colección no existe o error: {e}\")\n",
    "            print(\"Creando nueva colección...\")\n",
    "        \n",
    "        # Procesar dataset si no existe\n",
    "        print(\"Procesando dataset PeerQA...\")\n",
    "        dataset = load_dataset(\"UKPLab/PeerQA\", \"papers\")\n",
    "        papers = dataset['test']\n",
    "        print(\"Procesando papers a chunks...\")\n",
    "        chunks = self.procesar_dataset_peerqa_a_chunks_optimizado(papers)\n",
    "\n",
    "        # Guardar chunks en disco\n",
    "        print(f\"Guardando {len(chunks)} chunks en disco...\")\n",
    "        os.makedirs(os.path.dirname(chunks_path), exist_ok=True)\n",
    "        with open(chunks_path, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        print(\"Creando retriever...\")\n",
    "        self.retriever = self.crear_retriever_desde_chunks(chunks)\n",
    "        print(\"Retriever configurado correctamente!\")\n",
    "        return self.retriever\n",
    "\n",
    "    def crear_retriever_desde_chunks(self, chunks):\n",
    "        \"\"\"Crear retriever usando Chroma Docker\"\"\"\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Usando device: {device}\")\n",
    "\n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        # Crear vector store con Chroma Docker\n",
    "        print(\"Configurando vector store con Chroma Docker...\")\n",
    "        vector_store = Chroma(\n",
    "            client=self.chroma_client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=embeddings_model\n",
    "        )\n",
    "        \n",
    "        # Verificar si la colección ya tiene documentos\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            if collection.count() == 0:\n",
    "                print(\"Agregando documentos a la colección...\")\n",
    "                vector_store.add_documents(chunks)\n",
    "                print(f\"Agregados {len(chunks)} documentos a Chroma\")\n",
    "            else:\n",
    "                print(f\"La colección ya tiene {collection.count()} documentos\")\n",
    "        except:\n",
    "            print(\"Creando colección y agregando documentos...\")\n",
    "            vector_store.add_documents(chunks)\n",
    "            print(f\"Agregados {len(chunks)} documentos a Chroma\")\n",
    "\n",
    "        # Crear BM25 retriever\n",
    "        print(\"Creando BM25 retriever...\")\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks, k=7)\n",
    "        \n",
    "        # Crear vector retriever\n",
    "        print(\"Creando vector retriever...\")\n",
    "        chroma_retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 7}\n",
    "        )\n",
    "\n",
    "        # Crear ensemble retriever\n",
    "        print(\"Creando ensemble retriever...\")\n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[chroma_retriever, bm25_retriever],\n",
    "            weights=[0.7, 0.3]\n",
    "        )\n",
    "\n",
    "        return retriever\n",
    "\n",
    "    def procesar_dataset_peerqa_a_chunks_optimizado(self, papers_dataset):\n",
    "        \"\"\"Versión optimizada para PeerQA con chunks más grandes y mejor metadata\"\"\"\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=512,  \n",
    "            chunk_overlap=150,  \n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  \n",
    "        )\n",
    "        \n",
    "        all_chunks = []\n",
    "        papers_by_id = {}\n",
    "        \n",
    "        # Agrupar por paper_id\n",
    "        for row in papers_dataset:\n",
    "            paper_id = row[\"paper_id\"]\n",
    "            if paper_id not in papers_by_id:\n",
    "                papers_by_id[paper_id] = []\n",
    "            papers_by_id[paper_id].append(row)\n",
    "        \n",
    "        for paper_id, sections in papers_by_id.items():\n",
    "            # Construir texto completo del paper\n",
    "            full_paper_text = \"\"\n",
    "            section_info = []\n",
    "            \n",
    "            for section in sections:\n",
    "                content = section[\"content\"].strip()\n",
    "                if content:\n",
    "                    section_type = section.get(\"type\", \"text\")\n",
    "                    last_heading = section.get(\"last_heading\", \"\")\n",
    "                    \n",
    "                    if section_type in [\"title\", \"heading\"] or last_heading:\n",
    "                        content = f\"[{section_type.upper()}] {content}\"\n",
    "                    \n",
    "                    full_paper_text += f\"{content}\\n\\n\"\n",
    "                    section_info.append({\n",
    "                        \"type\": section_type,\n",
    "                        \"heading\": last_heading,\n",
    "                        \"idx\": section.get(\"idx\", 0)\n",
    "                    })\n",
    "            \n",
    "            # Crear chunks para este paper\n",
    "            if full_paper_text.strip():\n",
    "                doc = Document(\n",
    "                    page_content=full_paper_text.strip(),\n",
    "                    metadata={\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"source\": \"peerqa\",\n",
    "                        \"sections_count\": len(sections)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                chunks = splitter.split_documents([doc])\n",
    "                \n",
    "                # Agregar metadata específico a cada chunk\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk.metadata.update({\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"chunk_id\": f\"{paper_id}_{i}\",\n",
    "                        \"source\": \"peerqa\",\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    })\n",
    "                \n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"Procesados {len(papers_by_id)} papers únicos en {len(all_chunks)} chunks\")\n",
    "        return all_chunks\n",
    "\n",
    "    def query_rag(self, question, model_type='groq', max_context_length=3000):\n",
    "        \"\"\"Función principal para queries con límite de contexto\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Retriever no configurado. Ejecuta setup_peerqa_retriever() primero.\")\n",
    "        \n",
    "        # Recuperar documentos relevantes\n",
    "        retrieved_docs = self.retriever.invoke(question)\n",
    "        \n",
    "        # Construir contexto respetando límite\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        \n",
    "        for doc in retrieved_docs:\n",
    "            doc_text = doc.page_content\n",
    "            if total_length + len(doc_text) <= max_context_length:\n",
    "                context_parts.append(f\"Paper: {doc.metadata.get('paper_id', 'unknown')}\\n{doc_text}\")\n",
    "                total_length += len(doc_text)\n",
    "            else:\n",
    "                # Truncar si es necesario\n",
    "                remaining = max_context_length - total_length\n",
    "                if remaining > 100:  \n",
    "                    truncated = doc_text[:remaining] + \"...\"\n",
    "                    context_parts.append(f\"Paper: {doc.metadata.get('paper_id', 'unknown')}\\n{truncated}\")\n",
    "                break\n",
    "        \n",
    "        context_str = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "        \n",
    "        if model_type == 'groq':\n",
    "            response = self.generate_response(\n",
    "                self.groq_client, self.llama_model_name, question, context_str, 'groq'\n",
    "            )\n",
    "        elif model_type == 'gemini':\n",
    "            response = self.generate_response(\n",
    "                self.gemini_model, None, question, context_str, 'gemini'\n",
    "            )\n",
    "        elif model_type == 'openrouter':\n",
    "            response = self.generate_response(\n",
    "                self.open_router_client, self.qwen_model_name, question, context_str, 'openrouter'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"model_type debe ser 'groq', 'gemini' o 'openrouter'\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': response,\n",
    "            'contexts': [doc.page_content for doc in retrieved_docs[:5]],  # Solo top 5 para RAGAS\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context_str\n",
    "        }\n",
    "\n",
    "    def generate_response(self, client, model_name, user_query, context_str, client_type):\n",
    "        \"\"\"Prompt mejorado para evaluación académica\"\"\"\n",
    "        system_prompt = \"\"\"Eres un asistente de investigación experto en machine learning y procesamiento de lenguaje natural. \n",
    "\n",
    "INSTRUCCIONES:\n",
    "- Responde basándote únicamente en el contexto proporcionado de los papers académicos\n",
    "- Si tienes información suficiente, proporciona una respuesta completa y técnicamente precisa\n",
    "- Cita conceptos, métodos o resultados específicos cuando sea relevante\n",
    "- Si la información es parcial, responde con lo disponible y especifica qué no puedes determinar\n",
    "- Solo di \"La información proporcionada no es suficiente\" si el contexto es completamente irrelevante\n",
    "- Usa terminología técnica apropiada\n",
    "- Sé conciso pero completo\"\"\"\n",
    "\n",
    "        full_prompt = f\"Contexto de papers académicos:\\n{context_str}\\n\\nPregunta: {user_query}\\n\\nRespuesta basada en el contexto:\"\n",
    "        \n",
    "        try:\n",
    "            if client_type == 'gemini':\n",
    "                response = client.generate_content(full_prompt)\n",
    "                return response.text\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": full_prompt}\n",
    "                ]\n",
    "                if client_type == 'groq':\n",
    "                    chat_completion = client.chat.completions.create(\n",
    "                        messages=messages, \n",
    "                        model=model_name,\n",
    "                        temperature=0.1,  \n",
    "                        max_tokens=1000\n",
    "                    )\n",
    "                elif client_type == 'openrouter':\n",
    "                    chat_completion = client.chat.completions.create(\n",
    "                        messages=messages, \n",
    "                        model=model_name,\n",
    "                        temperature=0.1,\n",
    "                        max_tokens=1000\n",
    "                    )\n",
    "                return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error al generar respuesta: {str(e)}\"\n",
    "\n",
    "    def reset_collection(self):\n",
    "        \"\"\"Método para resetear la colección si es necesario\"\"\"\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(name=self.collection_name)\n",
    "            print(f\"Colección '{self.collection_name}' eliminada\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al eliminar colección: {e}\")\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Método para obtener información sobre la colección\"\"\"\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(name=self.collection_name)\n",
    "            count = collection.count()\n",
    "            print(f\"Colección '{self.collection_name}' tiene {count} documentos\")\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            print(f\"Error al obtener información de la colección: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def procesar_pdf_a_chunks(self, pdf_path):\n",
    "        \"\"\"Método original para PDFs (mantenido para compatibilidad)\"\"\"\n",
    "        elements = partition(filename=pdf_path)\n",
    "        pages = defaultdict(list)\n",
    "        for el in elements:\n",
    "            if el.text.strip():\n",
    "                metadata = el.metadata.to_dict()\n",
    "                page = metadata.get('page_number')\n",
    "                pages[page].append({\n",
    "                    'Texto': el.text.strip(),\n",
    "                    'Nombre del documento': metadata.get('filename'),\n",
    "                    'Idioma': metadata.get('languages', ['unknown'])[0],\n",
    "                })\n",
    "        all_chunks = []\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        for page_number, elementos in pages.items():\n",
    "            full_text = \" \".join(el['Texto'] for el in elementos)\n",
    "            base_metadata = {\n",
    "                'Número de pagina': page_number,\n",
    "                'Nombre del documento': elementos[0]['Nombre del documento'],\n",
    "                'Idioma': elementos[0]['Idioma'],\n",
    "            }\n",
    "            doc = Document(page_content=full_text, metadata=base_metadata)\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            all_chunks.extend(chunks)\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1719c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since UKPLab/PeerQA couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado a Chroma en localhost:8000\n",
      "Colección 'peerqa_papers' ya existe con 12844 documentos\n",
      "Procesando dataset PeerQA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the latest cached dataset configuration 'papers' at C:\\Users\\danie\\.cache\\huggingface\\datasets\\UKPLab___peer_qa\\papers\\1.0.0\\2b4c608f2e508bafc5d874167212597ed9d3351a9f107dfa83f628a4bdfbc337 (last modified on Tue Jul 15 20:49:57 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando papers a chunks...\n",
      "Procesados 90 papers únicos en 9732 chunks\n",
      "Guardando 9732 chunks en disco...\n",
      "Total chunks: 9732\n",
      "Creando retriever...\n",
      "Usando device: cuda\n",
      "Configurando vector store con Chroma Docker...\n",
      "La colección ya tiene 12844 documentos\n",
      "Creando BM25 retriever...\n",
      "Creando vector retriever...\n",
      "Creando ensemble retriever...\n",
      "Retriever configurado correctamente!\n",
      "✅ Inicializado RAGEvaluationGenerator con 267 ejemplos\n",
      "\n",
      "🚀 Generando respuestas con modelo: gemini\n",
      "📋 Procesando desde índice 8 hasta 12\n",
      "📦 Lotes de 3 con 30s de espera\n",
      "📊 Ya procesados: 8 ejemplos\n",
      "\n",
      "🔄 Procesando lote 3: índices 8-10\n",
      "[9/12] Procesando: Can the proposed algorithm be used to recover real...\n",
      "  ✅ Completado! (Contexto: 3551 chars)\n",
      "[10/12] Procesando: Can the pairwise distance of the latent variables ...\n",
      "  ✅ Completado! (Contexto: 3456 chars)\n",
      "[11/12] Procesando: Can the parameters of the BLOSUM matrix be estimat...\n",
      "  ✅ Completado! (Contexto: 3181 chars)\n",
      "💾 Progreso guardado: 11 ejemplos completados\n",
      "⏳ Esperando 30 segundos antes del siguiente lote...\n",
      "\n",
      "🔄 Procesando lote 4: índices 11-11\n",
      "[12/12] Procesando: Does the same preprocessing step of converting num...\n",
      "  ✅ Completado! (Contexto: 3322 chars)\n",
      "💾 Progreso guardado: 12 ejemplos completados\n",
      "\n",
      "🎉 ¡Proceso completado! Total: 12 ejemplos procesados\n",
      "\n",
      "📊 RESUMEN DEL PROGRESO:\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "📈 Total procesado: 12\n",
      "✅ Exitosos: 12\n",
      "❌ Con errores: 0\n",
      "🤖 Modelos usados: {'gemini': 12}\n",
      "📚 Dataset original: 267 ejemplos\n",
      "📊 Progreso: 4.5%\n",
      "🎯 Contexto promedio: 3470 caracteres\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "        \n",
      "📊 Dataset RAGAS creado exitosamente: 12 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# 1. Inicializar tu RAGPipeline\n",
    "rag = RAGPipeline()  # Sin parámetros Chroma ya que usa archivos locales\n",
    "\n",
    "# Setup (solo la primera vez, después ya está persistido)\n",
    "rag.setup_peerqa_retriever()\n",
    "\n",
    "# 2. Crear el generador (mismo código que tenías)\n",
    "generator = RAGEvaluationGenerator(\n",
    "    rag_pipeline=rag,\n",
    "    clean_dataset=qa_clean_dataset,  # Tu dataset filtrado\n",
    "    batch_size=3,    # Solo 3 por lote (más conservador)\n",
    "    delay=30         # 30 segundos entre lotes (no 90)\n",
    ")\n",
    "\n",
    "# 3. Procesar de a poquito (empezar con 4 ejemplos de prueba)\n",
    "generator.generate_responses_batch(\n",
    "    model_type='gemini',\n",
    "    max_samples=4  # Solo 4 para probar\n",
    ")\n",
    "\n",
    "# 4. Ver progreso\n",
    "print(generator.get_summary())\n",
    "\n",
    "# 5. Cuando esté listo, crear dataset para RAGAS\n",
    "ragas_dataset = generator.get_ragas_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuevo_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
